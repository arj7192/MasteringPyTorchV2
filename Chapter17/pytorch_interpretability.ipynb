{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch==1.12\n",
      "  Using cached torch-1.12.0-cp39-none-macosx_10_9_x86_64.whl (133.6 MB)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torch==1.12) (4.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\n",
      "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\n",
      "torchdata 0.4.1 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchvision==0.13\n",
      "  Using cached torchvision-0.13.0-cp39-cp39-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchvision==0.13) (1.22.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchvision==0.13) (4.3.0)\n",
      "Requirement already satisfied: requests in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchvision==0.13) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchvision==0.13) (9.2.0)\n",
      "Requirement already satisfied: torch==1.12.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchvision==0.13) (1.12.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchvision==0.13) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchvision==0.13) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchvision==0.13) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchvision==0.13) (3.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torchvision 0.13.1\n",
      "    Uninstalling torchvision-0.13.1:\n",
      "      Successfully uninstalled torchvision-0.13.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed torchvision-0.13.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk==3.7 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from nltk==3.7) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from nltk==3.7) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from nltk==3.7) (2022.7.25)\n",
      "Requirement already satisfied: click in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from nltk==3.7) (8.1.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchtext==0.13.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: requests in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchtext==0.13.1) (2.28.1)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchtext==0.13.1) (1.22.4)\n",
      "Requirement already satisfied: tqdm in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchtext==0.13.1) (4.64.0)\n",
      "Collecting torch==1.12.1\n",
      "  Using cached torch-1.12.1-cp39-none-macosx_10_9_x86_64.whl (133.8 MB)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torch==1.12.1->torchtext==0.13.1) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (1.26.11)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torch 1.12.0\n",
      "    Uninstalling torch-1.12.0:\n",
      "      Successfully uninstalled torch-1.12.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0 requires torch==1.12.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: networkx==2.8.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (2.8.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchviz==0.0.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchviz==0.0.2) (1.12.1)\n",
      "Requirement already satisfied: graphviz in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchviz==0.0.2) (0.20.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torch->torchviz==0.0.2) (4.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for gym: [Errno 2] No such file or directory: '/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages/gym-0.19.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib==3.5.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (1.22.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (4.34.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (9.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib==3.5.2) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-image==0.19.3 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.19.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (1.8.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (2.8.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (21.3)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (2.21.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (1.3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (2022.8.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (9.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from packaging>=20.0->scikit-image==0.19.3) (3.0.9)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gym==0.25.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from gym==0.25.1) (1.22.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from gym==0.25.1) (4.12.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from gym==0.25.1) (0.0.7)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from gym==0.25.1) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym==0.25.1) (3.8.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: atari-py==0.2.9 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.2.9)\n",
      "Requirement already satisfied: six in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from atari-py==0.2.9) (1.16.0)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from atari-py==0.2.9) (1.22.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: opencv-python==4.6.0.66 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from opencv-python==4.6.0.66) (1.22.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting autoPyTorch==0.0.2\n",
      "  Using cached autoPyTorch-0.0.2-py3-none-any.whl (312 kB)\n",
      "Requirement already satisfied: ConfigSpace in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.24.2)\n",
      "Requirement already satisfied: torchvision in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.13.0)\n",
      "Requirement already satisfied: pynisher in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.6.4)\n",
      "Requirement already satisfied: statsmodels in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.13.2)\n",
      "Requirement already satisfied: imblearn in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.0)\n",
      "Requirement already satisfied: netifaces in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.11.0)\n",
      "Requirement already satisfied: fasteners in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.17.3)\n",
      "Requirement already satisfied: scipy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (1.8.1)\n",
      "Requirement already satisfied: openml in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.12.2)\n",
      "Requirement already satisfied: Cython in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.29.32)\n",
      "Requirement already satisfied: hpbandster in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.7.4)\n",
      "Requirement already satisfied: tensorboard-logger in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (0.1.0)\n",
      "Requirement already satisfied: setuptools in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (59.8.0)\n",
      "Requirement already satisfied: pandas in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (1.4.3)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (1.22.4)\n",
      "Requirement already satisfied: torch in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from autoPyTorch==0.0.2) (1.12.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-learn>=0.20.0->autoPyTorch==0.0.2) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-learn>=0.20.0->autoPyTorch==0.0.2) (1.1.0)\n",
      "Requirement already satisfied: pyparsing in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from ConfigSpace->autoPyTorch==0.0.2) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from ConfigSpace->autoPyTorch==0.0.2) (4.3.0)\n",
      "Requirement already satisfied: Pyro4 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from hpbandster->autoPyTorch==0.0.2) (4.82)\n",
      "Requirement already satisfied: serpent in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from hpbandster->autoPyTorch==0.0.2) (1.41)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from imblearn->autoPyTorch==0.0.2) (0.9.1)\n",
      "Requirement already satisfied: liac-arff>=2.4.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from openml->autoPyTorch==0.0.2) (2.5.0)\n",
      "Requirement already satisfied: pyarrow in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from openml->autoPyTorch==0.0.2) (9.0.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from openml->autoPyTorch==0.0.2) (2.8.2)\n",
      "Requirement already satisfied: xmltodict in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from openml->autoPyTorch==0.0.2) (0.13.0)\n",
      "Requirement already satisfied: requests in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from openml->autoPyTorch==0.0.2) (2.28.1)\n",
      "Requirement already satisfied: minio in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from openml->autoPyTorch==0.0.2) (7.1.11)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from pandas->autoPyTorch==0.0.2) (2022.1)\n",
      "Requirement already satisfied: psutil in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from pynisher->autoPyTorch==0.0.2) (5.9.1)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from statsmodels->autoPyTorch==0.0.2) (0.5.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from statsmodels->autoPyTorch==0.0.2) (21.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from tensorboard-logger->autoPyTorch==0.0.2) (9.2.0)\n",
      "Requirement already satisfied: six in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from tensorboard-logger->autoPyTorch==0.0.2) (1.16.0)\n",
      "Requirement already satisfied: protobuf in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from tensorboard-logger->autoPyTorch==0.0.2) (3.19.4)\n",
      "Collecting scikit-learn>=0.20.0\n",
      "  Using cached scikit_learn-1.1.2-cp39-cp39-macosx_10_9_x86_64.whl (8.7 MB)\n",
      "Requirement already satisfied: urllib3 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from minio->openml->autoPyTorch==0.0.2) (1.26.11)\n",
      "Requirement already satisfied: certifi in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from minio->openml->autoPyTorch==0.0.2) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->openml->autoPyTorch==0.0.2) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->openml->autoPyTorch==0.0.2) (3.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: scikit-learn, autoPyTorch\n",
      "  Attempting uninstall: scikit-learn\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "  Attempting uninstall: autoPyTorch\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: autoPyTorch 0.2.1\n",
      "    Uninstalling autoPyTorch-0.2.1:\n",
      "      Successfully uninstalled autoPyTorch-0.2.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed autoPyTorch-0.0.2 scikit-learn-1.1.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchviz==0.0.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: graphviz in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchviz==0.0.2) (0.20.1)\n",
      "Requirement already satisfied: torch in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchviz==0.0.2) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torch->torchviz==0.0.2) (4.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting configspace==0.5.0\n",
      "  Using cached ConfigSpace-0.5.0-cp39-cp39-macosx_10_9_x86_64.whl (966 kB)\n",
      "Requirement already satisfied: scipy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from configspace==0.5.0) (1.8.1)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from configspace==0.5.0) (1.22.4)\n",
      "Requirement already satisfied: cython in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from configspace==0.5.0) (0.29.32)\n",
      "Requirement already satisfied: pyparsing in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from configspace==0.5.0) (3.0.9)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: configspace\n",
      "  Attempting uninstall: configspace\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: ConfigSpace 0.6.0\n",
      "    Uninstalling ConfigSpace-0.6.0:\n",
      "      Successfully uninstalled ConfigSpace-0.6.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed configspace-0.5.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/shukon/HpBandSter.git\n",
      "  Cloning https://github.com/shukon/HpBandSter.git to /private/var/folders/gs/mjlw0j210yz02z4yrv9gshdm0000gq/T/pip-req-build-8n563e6j\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/shukon/HpBandSter.git /private/var/folders/gs/mjlw0j210yz02z4yrv9gshdm0000gq/T/pip-req-build-8n563e6j\n",
      "  Resolved https://github.com/shukon/HpBandSter.git to commit 89eb5337a4841077d00b238f75b8e2ad9c37476f\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Pyro4 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from hpbandster==0.7.4) (4.82)\n",
      "Requirement already satisfied: serpent in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from hpbandster==0.7.4) (1.41)\n",
      "Requirement already satisfied: ConfigSpace in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from hpbandster==0.7.4) (0.5.0)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from hpbandster==0.7.4) (1.22.4)\n",
      "Requirement already satisfied: statsmodels in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from hpbandster==0.7.4) (0.13.2)\n",
      "Requirement already satisfied: scipy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from hpbandster==0.7.4) (1.8.1)\n",
      "Requirement already satisfied: netifaces in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from hpbandster==0.7.4) (0.11.0)\n",
      "Requirement already satisfied: cython in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from ConfigSpace->hpbandster==0.7.4) (0.29.32)\n",
      "Requirement already satisfied: pyparsing in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from ConfigSpace->hpbandster==0.7.4) (3.0.9)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from statsmodels->hpbandster==0.7.4) (1.4.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from statsmodels->hpbandster==0.7.4) (0.5.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from statsmodels->hpbandster==0.7.4) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from pandas>=0.25->statsmodels->hpbandster==0.7.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from pandas>=0.25->statsmodels->hpbandster==0.7.4) (2022.1)\n",
      "Requirement already satisfied: six in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from patsy>=0.5.2->statsmodels->hpbandster==0.7.4) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: captum==0.5.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from captum==0.5.0) (1.22.4)\n",
      "Requirement already satisfied: matplotlib in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from captum==0.5.0) (3.5.2)\n",
      "Requirement already satisfied: torch>=1.6 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from captum==0.5.0) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torch>=1.6->captum==0.5.0) (4.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib->captum==0.5.0) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib->captum==0.5.0) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib->captum==0.5.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib->captum==0.5.0) (4.34.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib->captum==0.5.0) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib->captum==0.5.0) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib->captum==0.5.0) (2.8.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->captum==0.5.0) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.12\n",
    "!pip install torchvision==0.13\n",
    "!pip install nltk==3.7\n",
    "!pip install torchtext==0.13.1\n",
    "!pip install networkx==2.8.5\n",
    "!pip install torchviz==0.0.2\n",
    "!pip install matplotlib==3.5.2\n",
    "!pip install scikit-image==0.19.3\n",
    "!pip install gym==0.25.1\n",
    "!pip install atari-py==0.2.9\n",
    "!pip install opencv-python==4.6.0.66\n",
    "!pip install autoPyTorch==0.0.2\n",
    "!pip install torchviz==0.0.2\n",
    "!pip install configspace==0.5.0\n",
    "!pip install git+https://github.com/shukon/HpBandSter.git\n",
    "!pip install captum==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bc0e5740294735904dbf83e0ce40bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367857a49ba94803955e98d6a30749e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8947e4e7ae44d1b7086e11dcf50e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae2b5304f4a4bcc828e60ac173a26cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.333742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.504298\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.336827\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.684411\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.434219\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.608343\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.770463\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.431585\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.372163\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.409993\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.579152\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.550262\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.527237\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.099100\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.480612\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.444318\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.243250\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.404903\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.373952\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.062566\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.248978\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.170154\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.198263\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.160716\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.354085\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.058246\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.114943\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.166820\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.421265\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.150150\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.221313\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.053462\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.227447\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.194833\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.346394\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.269492\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.327498\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.189451\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.071351\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.105581\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.191753\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.304682\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.307259\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.205596\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.388677\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.157545\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.062400\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.072262\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.152597\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.115311\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.060675\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.138316\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.236591\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.209230\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.389900\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.135537\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.305453\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.101006\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.387825\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.087863\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.050547\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.136968\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.161405\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.116955\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.252918\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.089545\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.101473\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.217464\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.176696\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.011146\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.162355\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.468988\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.306770\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.025528\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.206167\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.133217\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.077363\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.251368\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.260464\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.038845\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.069026\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.198948\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.020880\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.113637\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.026265\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.179046\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.095820\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.081377\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.066137\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.105601\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.226562\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.029085\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.108234\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.259311\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.199958\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.192944\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.346146\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.132118\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.048927\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.164862\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.102333\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.210938\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.060971\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.079564\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.117801\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.473926\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.106239\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.074868\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.097399\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.183769\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.008209\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.127666\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.068722\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.301627\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.558476\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.040753\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.123419\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.125256\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.132798\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.035179\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.088902\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.050071\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.053317\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.011192\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.159517\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.326905\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.005937\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.411617\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.570390\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.152211\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.088355\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.029821\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.095718\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.017024\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.109110\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.034757\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.273123\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.143421\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.095004\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.237784\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.102713\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.156955\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.011227\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.122379\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.125398\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.081500\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.297314\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.066637\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.015701\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.140693\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.067499\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.017821\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.299386\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.461141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.063890\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.157850\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.050877\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.029393\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.156317\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.256441\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.013070\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.082736\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.052154\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.149190\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.040126\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.019928\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.017528\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.337962\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.068636\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.156115\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.072559\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.019566\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.428155\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.117205\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.014432\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.062214\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.126929\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.263368\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.026794\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.588975\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.166414\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.073647\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.318702\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.012623\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.091141\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.034730\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.158065\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.005788\n",
      "\n",
      "Test dataset: Overall Loss: 0.0559, Overall Accuracy: 9815/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.069370\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.059703\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.117000\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.011116\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.174225\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.016626\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.004864\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.058761\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.030764\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.009196\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.003040\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.246245\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.057502\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.003083\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.016155\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.069402\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.038480\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.011187\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.067020\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.292856\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.051527\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.022826\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.015957\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.025008\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.018219\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.047203\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.078102\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.004211\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.210560\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.018830\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.146098\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.020092\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.079232\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.006101\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.028404\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.014766\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.141674\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.299027\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.025585\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.130003\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.032668\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.125669\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.165778\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.038193\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.025593\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.026341\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.009354\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.349725\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.029236\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.071091\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.461211\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.019507\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.028332\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.220331\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.045459\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.045673\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.019535\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.019058\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.051245\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.037608\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.089849\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.040997\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.096717\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.036050\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.137247\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.020770\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.148152\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.054991\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.005666\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.007598\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.115861\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.014224\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.060427\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.045002\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.183449\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.333729\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.019034\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.005693\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.030252\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.032936\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.040658\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.014352\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.056199\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.061151\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.081959\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.029525\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.003109\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.001638\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.075450\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.014664\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.104869\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.011978\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.026509\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.025245\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.006974\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.312401\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.025254\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.042918\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.031479\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.013502\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.198653\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.085860\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.391842\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.079760\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.002461\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.008268\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.015342\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.032862\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.037607\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.002062\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.066516\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.004127\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.059770\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.193144\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.088746\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.019431\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.082988\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.005974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.054520\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.023639\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.050625\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.049475\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.016467\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.098247\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.025398\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.056281\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.010923\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.157137\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.073890\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.247721\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.289163\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.042822\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.092192\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.071724\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.279847\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.006946\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.027910\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.108334\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.001884\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.013285\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.002416\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.529453\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.021631\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.006473\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.025129\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.014461\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.344067\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.030757\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.193906\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.125684\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.033457\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.051274\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.017482\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.002541\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.004013\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.016308\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.011303\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.041656\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.006241\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.039359\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.195933\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.066045\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.134749\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.138555\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.017028\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.034311\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.012560\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.374398\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.144456\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.005871\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.013278\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.111197\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.019814\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.175532\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.007301\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.144531\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.145071\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.004328\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.020970\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.065814\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.109402\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.032378\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.057200\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.183860\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.020638\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.018290\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.033475\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.129387\n",
      "\n",
      "Test dataset: Overall Loss: 0.0423, Overall Accuracy: 9854/10000 (99%)\n",
      "\n",
      "epoch: 3 [0/60000 (0%)]\t training loss: 0.031736\n",
      "epoch: 3 [320/60000 (1%)]\t training loss: 0.013309\n",
      "epoch: 3 [640/60000 (1%)]\t training loss: 0.049899\n",
      "epoch: 3 [960/60000 (2%)]\t training loss: 0.043195\n",
      "epoch: 3 [1280/60000 (2%)]\t training loss: 0.069021\n",
      "epoch: 3 [1600/60000 (3%)]\t training loss: 0.336189\n",
      "epoch: 3 [1920/60000 (3%)]\t training loss: 0.011964\n",
      "epoch: 3 [2240/60000 (4%)]\t training loss: 0.007031\n",
      "epoch: 3 [2560/60000 (4%)]\t training loss: 0.061530\n",
      "epoch: 3 [2880/60000 (5%)]\t training loss: 0.010302\n",
      "epoch: 3 [3200/60000 (5%)]\t training loss: 0.452379\n",
      "epoch: 3 [3520/60000 (6%)]\t training loss: 0.043012\n",
      "epoch: 3 [3840/60000 (6%)]\t training loss: 0.002367\n",
      "epoch: 3 [4160/60000 (7%)]\t training loss: 0.010880\n",
      "epoch: 3 [4480/60000 (7%)]\t training loss: 0.001902\n",
      "epoch: 3 [4800/60000 (8%)]\t training loss: 0.246788\n",
      "epoch: 3 [5120/60000 (9%)]\t training loss: 0.091232\n",
      "epoch: 3 [5440/60000 (9%)]\t training loss: 0.048611\n",
      "epoch: 3 [5760/60000 (10%)]\t training loss: 0.022272\n",
      "epoch: 3 [6080/60000 (10%)]\t training loss: 0.004984\n",
      "epoch: 3 [6400/60000 (11%)]\t training loss: 0.004403\n",
      "epoch: 3 [6720/60000 (11%)]\t training loss: 0.077499\n",
      "epoch: 3 [7040/60000 (12%)]\t training loss: 0.016051\n",
      "epoch: 3 [7360/60000 (12%)]\t training loss: 0.002596\n",
      "epoch: 3 [7680/60000 (13%)]\t training loss: 0.010858\n",
      "epoch: 3 [8000/60000 (13%)]\t training loss: 0.122042\n",
      "epoch: 3 [8320/60000 (14%)]\t training loss: 0.006074\n",
      "epoch: 3 [8640/60000 (14%)]\t training loss: 0.126385\n",
      "epoch: 3 [8960/60000 (15%)]\t training loss: 0.088666\n",
      "epoch: 3 [9280/60000 (15%)]\t training loss: 0.018922\n",
      "epoch: 3 [9600/60000 (16%)]\t training loss: 0.003494\n",
      "epoch: 3 [9920/60000 (17%)]\t training loss: 0.042493\n",
      "epoch: 3 [10240/60000 (17%)]\t training loss: 0.011048\n",
      "epoch: 3 [10560/60000 (18%)]\t training loss: 0.254515\n",
      "epoch: 3 [10880/60000 (18%)]\t training loss: 0.026633\n",
      "epoch: 3 [11200/60000 (19%)]\t training loss: 0.040337\n",
      "epoch: 3 [11520/60000 (19%)]\t training loss: 0.030116\n",
      "epoch: 3 [11840/60000 (20%)]\t training loss: 0.451688\n",
      "epoch: 3 [12160/60000 (20%)]\t training loss: 0.004801\n",
      "epoch: 3 [12480/60000 (21%)]\t training loss: 0.175925\n",
      "epoch: 3 [12800/60000 (21%)]\t training loss: 0.062714\n",
      "epoch: 3 [13120/60000 (22%)]\t training loss: 0.002483\n",
      "epoch: 3 [13440/60000 (22%)]\t training loss: 0.000315\n",
      "epoch: 3 [13760/60000 (23%)]\t training loss: 0.028852\n",
      "epoch: 3 [14080/60000 (23%)]\t training loss: 0.045055\n",
      "epoch: 3 [14400/60000 (24%)]\t training loss: 0.022300\n",
      "epoch: 3 [14720/60000 (25%)]\t training loss: 0.038939\n",
      "epoch: 3 [15040/60000 (25%)]\t training loss: 0.026218\n",
      "epoch: 3 [15360/60000 (26%)]\t training loss: 0.026704\n",
      "epoch: 3 [15680/60000 (26%)]\t training loss: 0.027539\n",
      "epoch: 3 [16000/60000 (27%)]\t training loss: 0.085172\n",
      "epoch: 3 [16320/60000 (27%)]\t training loss: 0.010876\n",
      "epoch: 3 [16640/60000 (28%)]\t training loss: 0.001687\n",
      "epoch: 3 [16960/60000 (28%)]\t training loss: 0.015597\n",
      "epoch: 3 [17280/60000 (29%)]\t training loss: 0.096135\n",
      "epoch: 3 [17600/60000 (29%)]\t training loss: 0.003511\n",
      "epoch: 3 [17920/60000 (30%)]\t training loss: 0.045726\n",
      "epoch: 3 [18240/60000 (30%)]\t training loss: 0.001801\n",
      "epoch: 3 [18560/60000 (31%)]\t training loss: 0.009200\n",
      "epoch: 3 [18880/60000 (31%)]\t training loss: 0.001853\n",
      "epoch: 3 [19200/60000 (32%)]\t training loss: 0.008813\n",
      "epoch: 3 [19520/60000 (33%)]\t training loss: 0.055834\n",
      "epoch: 3 [19840/60000 (33%)]\t training loss: 0.005756\n",
      "epoch: 3 [20160/60000 (34%)]\t training loss: 0.001992\n",
      "epoch: 3 [20480/60000 (34%)]\t training loss: 0.054619\n",
      "epoch: 3 [20800/60000 (35%)]\t training loss: 0.045919\n",
      "epoch: 3 [21120/60000 (35%)]\t training loss: 0.164951\n",
      "epoch: 3 [21440/60000 (36%)]\t training loss: 0.093142\n",
      "epoch: 3 [21760/60000 (36%)]\t training loss: 0.042771\n",
      "epoch: 3 [22080/60000 (37%)]\t training loss: 0.003864\n",
      "epoch: 3 [22400/60000 (37%)]\t training loss: 0.010794\n",
      "epoch: 3 [22720/60000 (38%)]\t training loss: 0.000703\n",
      "epoch: 3 [23040/60000 (38%)]\t training loss: 0.014234\n",
      "epoch: 3 [23360/60000 (39%)]\t training loss: 0.145878\n",
      "epoch: 3 [23680/60000 (39%)]\t training loss: 0.004224\n",
      "epoch: 3 [24000/60000 (40%)]\t training loss: 0.375163\n",
      "epoch: 3 [24320/60000 (41%)]\t training loss: 0.016318\n",
      "epoch: 3 [24640/60000 (41%)]\t training loss: 0.003495\n",
      "epoch: 3 [24960/60000 (42%)]\t training loss: 0.037598\n",
      "epoch: 3 [25280/60000 (42%)]\t training loss: 0.024690\n",
      "epoch: 3 [25600/60000 (43%)]\t training loss: 0.021582\n",
      "epoch: 3 [25920/60000 (43%)]\t training loss: 0.010440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 [26240/60000 (44%)]\t training loss: 0.001953\n",
      "epoch: 3 [26560/60000 (44%)]\t training loss: 0.010675\n",
      "epoch: 3 [26880/60000 (45%)]\t training loss: 0.020778\n",
      "epoch: 3 [27200/60000 (45%)]\t training loss: 0.037570\n",
      "epoch: 3 [27520/60000 (46%)]\t training loss: 0.003347\n",
      "epoch: 3 [27840/60000 (46%)]\t training loss: 0.003525\n",
      "epoch: 3 [28160/60000 (47%)]\t training loss: 0.177683\n",
      "epoch: 3 [28480/60000 (47%)]\t training loss: 0.139199\n",
      "epoch: 3 [28800/60000 (48%)]\t training loss: 0.011186\n",
      "epoch: 3 [29120/60000 (49%)]\t training loss: 0.072172\n",
      "epoch: 3 [29440/60000 (49%)]\t training loss: 0.347907\n",
      "epoch: 3 [29760/60000 (50%)]\t training loss: 0.034505\n",
      "epoch: 3 [30080/60000 (50%)]\t training loss: 0.017343\n",
      "epoch: 3 [30400/60000 (51%)]\t training loss: 0.002645\n",
      "epoch: 3 [30720/60000 (51%)]\t training loss: 0.016769\n",
      "epoch: 3 [31040/60000 (52%)]\t training loss: 0.008808\n",
      "epoch: 3 [31360/60000 (52%)]\t training loss: 0.017758\n",
      "epoch: 3 [31680/60000 (53%)]\t training loss: 0.037998\n",
      "epoch: 3 [32000/60000 (53%)]\t training loss: 0.072362\n",
      "epoch: 3 [32320/60000 (54%)]\t training loss: 0.004867\n",
      "epoch: 3 [32640/60000 (54%)]\t training loss: 0.378647\n",
      "epoch: 3 [32960/60000 (55%)]\t training loss: 0.282386\n",
      "epoch: 3 [33280/60000 (55%)]\t training loss: 0.093698\n",
      "epoch: 3 [33600/60000 (56%)]\t training loss: 0.285123\n",
      "epoch: 3 [33920/60000 (57%)]\t training loss: 0.086884\n",
      "epoch: 3 [34240/60000 (57%)]\t training loss: 0.011486\n",
      "epoch: 3 [34560/60000 (58%)]\t training loss: 0.062443\n",
      "epoch: 3 [34880/60000 (58%)]\t training loss: 0.017322\n",
      "epoch: 3 [35200/60000 (59%)]\t training loss: 0.040772\n",
      "epoch: 3 [35520/60000 (59%)]\t training loss: 0.009624\n",
      "epoch: 3 [35840/60000 (60%)]\t training loss: 0.006877\n",
      "epoch: 3 [36160/60000 (60%)]\t training loss: 0.119888\n",
      "epoch: 3 [36480/60000 (61%)]\t training loss: 0.026599\n",
      "epoch: 3 [36800/60000 (61%)]\t training loss: 0.045681\n",
      "epoch: 3 [37120/60000 (62%)]\t training loss: 0.452744\n",
      "epoch: 3 [37440/60000 (62%)]\t training loss: 0.028978\n",
      "epoch: 3 [37760/60000 (63%)]\t training loss: 0.026094\n",
      "epoch: 3 [38080/60000 (63%)]\t training loss: 0.009402\n",
      "epoch: 3 [38400/60000 (64%)]\t training loss: 0.042089\n",
      "epoch: 3 [38720/60000 (65%)]\t training loss: 0.053569\n",
      "epoch: 3 [39040/60000 (65%)]\t training loss: 0.057518\n",
      "epoch: 3 [39360/60000 (66%)]\t training loss: 0.063745\n",
      "epoch: 3 [39680/60000 (66%)]\t training loss: 0.108839\n",
      "epoch: 3 [40000/60000 (67%)]\t training loss: 0.066208\n",
      "epoch: 3 [40320/60000 (67%)]\t training loss: 0.003405\n",
      "epoch: 3 [40640/60000 (68%)]\t training loss: 0.019169\n",
      "epoch: 3 [40960/60000 (68%)]\t training loss: 0.034765\n",
      "epoch: 3 [41280/60000 (69%)]\t training loss: 0.007883\n",
      "epoch: 3 [41600/60000 (69%)]\t training loss: 0.189843\n",
      "epoch: 3 [41920/60000 (70%)]\t training loss: 0.001663\n",
      "epoch: 3 [42240/60000 (70%)]\t training loss: 0.050414\n",
      "epoch: 3 [42560/60000 (71%)]\t training loss: 0.158848\n",
      "epoch: 3 [42880/60000 (71%)]\t training loss: 0.002264\n",
      "epoch: 3 [43200/60000 (72%)]\t training loss: 0.000849\n",
      "epoch: 3 [43520/60000 (73%)]\t training loss: 0.052409\n",
      "epoch: 3 [43840/60000 (73%)]\t training loss: 0.002952\n",
      "epoch: 3 [44160/60000 (74%)]\t training loss: 0.026551\n",
      "epoch: 3 [44480/60000 (74%)]\t training loss: 0.001034\n",
      "epoch: 3 [44800/60000 (75%)]\t training loss: 0.183020\n",
      "epoch: 3 [45120/60000 (75%)]\t training loss: 0.232692\n",
      "epoch: 3 [45440/60000 (76%)]\t training loss: 0.077582\n",
      "epoch: 3 [45760/60000 (76%)]\t training loss: 0.027759\n",
      "epoch: 3 [46080/60000 (77%)]\t training loss: 0.133246\n",
      "epoch: 3 [46400/60000 (77%)]\t training loss: 0.024099\n",
      "epoch: 3 [46720/60000 (78%)]\t training loss: 0.005899\n",
      "epoch: 3 [47040/60000 (78%)]\t training loss: 0.016036\n",
      "epoch: 3 [47360/60000 (79%)]\t training loss: 0.002120\n",
      "epoch: 3 [47680/60000 (79%)]\t training loss: 0.005654\n",
      "epoch: 3 [48000/60000 (80%)]\t training loss: 0.068529\n",
      "epoch: 3 [48320/60000 (81%)]\t training loss: 0.054822\n",
      "epoch: 3 [48640/60000 (81%)]\t training loss: 0.022540\n",
      "epoch: 3 [48960/60000 (82%)]\t training loss: 0.031332\n",
      "epoch: 3 [49280/60000 (82%)]\t training loss: 0.018493\n",
      "epoch: 3 [49600/60000 (83%)]\t training loss: 0.174379\n",
      "epoch: 3 [49920/60000 (83%)]\t training loss: 0.066494\n",
      "epoch: 3 [50240/60000 (84%)]\t training loss: 0.038130\n",
      "epoch: 3 [50560/60000 (84%)]\t training loss: 0.002011\n",
      "epoch: 3 [50880/60000 (85%)]\t training loss: 0.065830\n",
      "epoch: 3 [51200/60000 (85%)]\t training loss: 0.104638\n",
      "epoch: 3 [51520/60000 (86%)]\t training loss: 0.005022\n",
      "epoch: 3 [51840/60000 (86%)]\t training loss: 0.008914\n",
      "epoch: 3 [52160/60000 (87%)]\t training loss: 0.010023\n",
      "epoch: 3 [52480/60000 (87%)]\t training loss: 0.006246\n",
      "epoch: 3 [52800/60000 (88%)]\t training loss: 0.107709\n",
      "epoch: 3 [53120/60000 (89%)]\t training loss: 0.018001\n",
      "epoch: 3 [53440/60000 (89%)]\t training loss: 0.002020\n",
      "epoch: 3 [53760/60000 (90%)]\t training loss: 0.000575\n",
      "epoch: 3 [54080/60000 (90%)]\t training loss: 0.009858\n",
      "epoch: 3 [54400/60000 (91%)]\t training loss: 0.002735\n",
      "epoch: 3 [54720/60000 (91%)]\t training loss: 0.129732\n",
      "epoch: 3 [55040/60000 (92%)]\t training loss: 0.145369\n",
      "epoch: 3 [55360/60000 (92%)]\t training loss: 0.007010\n",
      "epoch: 3 [55680/60000 (93%)]\t training loss: 0.010163\n",
      "epoch: 3 [56000/60000 (93%)]\t training loss: 0.039747\n",
      "epoch: 3 [56320/60000 (94%)]\t training loss: 0.010967\n",
      "epoch: 3 [56640/60000 (94%)]\t training loss: 0.240634\n",
      "epoch: 3 [56960/60000 (95%)]\t training loss: 0.040819\n",
      "epoch: 3 [57280/60000 (95%)]\t training loss: 0.011174\n",
      "epoch: 3 [57600/60000 (96%)]\t training loss: 0.017964\n",
      "epoch: 3 [57920/60000 (97%)]\t training loss: 0.008630\n",
      "epoch: 3 [58240/60000 (97%)]\t training loss: 0.088833\n",
      "epoch: 3 [58560/60000 (98%)]\t training loss: 0.023059\n",
      "epoch: 3 [58880/60000 (98%)]\t training loss: 0.122450\n",
      "epoch: 3 [59200/60000 (99%)]\t training loss: 0.003230\n",
      "epoch: 3 [59520/60000 (99%)]\t training loss: 0.000898\n",
      "epoch: 3 [59840/60000 (100%)]\t training loss: 0.035224\n",
      "\n",
      "Test dataset: Overall Loss: 0.0376, Overall Accuracy: 9880/10000 (99%)\n",
      "\n",
      "epoch: 4 [0/60000 (0%)]\t training loss: 0.007816\n",
      "epoch: 4 [320/60000 (1%)]\t training loss: 0.005817\n",
      "epoch: 4 [640/60000 (1%)]\t training loss: 0.095532\n",
      "epoch: 4 [960/60000 (2%)]\t training loss: 0.079845\n",
      "epoch: 4 [1280/60000 (2%)]\t training loss: 0.179984\n",
      "epoch: 4 [1600/60000 (3%)]\t training loss: 0.127744\n",
      "epoch: 4 [1920/60000 (3%)]\t training loss: 0.005674\n",
      "epoch: 4 [2240/60000 (4%)]\t training loss: 0.029973\n",
      "epoch: 4 [2560/60000 (4%)]\t training loss: 0.017383\n",
      "epoch: 4 [2880/60000 (5%)]\t training loss: 0.003495\n",
      "epoch: 4 [3200/60000 (5%)]\t training loss: 0.007645\n",
      "epoch: 4 [3520/60000 (6%)]\t training loss: 0.002621\n",
      "epoch: 4 [3840/60000 (6%)]\t training loss: 0.000836\n",
      "epoch: 4 [4160/60000 (7%)]\t training loss: 0.001829\n",
      "epoch: 4 [4480/60000 (7%)]\t training loss: 0.054987\n",
      "epoch: 4 [4800/60000 (8%)]\t training loss: 0.001504\n",
      "epoch: 4 [5120/60000 (9%)]\t training loss: 0.020642\n",
      "epoch: 4 [5440/60000 (9%)]\t training loss: 0.001137\n",
      "epoch: 4 [5760/60000 (10%)]\t training loss: 0.138270\n",
      "epoch: 4 [6080/60000 (10%)]\t training loss: 0.071007\n",
      "epoch: 4 [6400/60000 (11%)]\t training loss: 0.054608\n",
      "epoch: 4 [6720/60000 (11%)]\t training loss: 0.040598\n",
      "epoch: 4 [7040/60000 (12%)]\t training loss: 0.134063\n",
      "epoch: 4 [7360/60000 (12%)]\t training loss: 0.006385\n",
      "epoch: 4 [7680/60000 (13%)]\t training loss: 0.008051\n",
      "epoch: 4 [8000/60000 (13%)]\t training loss: 0.027839\n",
      "epoch: 4 [8320/60000 (14%)]\t training loss: 0.000546\n",
      "epoch: 4 [8640/60000 (14%)]\t training loss: 0.028883\n",
      "epoch: 4 [8960/60000 (15%)]\t training loss: 0.036886\n",
      "epoch: 4 [9280/60000 (15%)]\t training loss: 0.028720\n",
      "epoch: 4 [9600/60000 (16%)]\t training loss: 0.009010\n",
      "epoch: 4 [9920/60000 (17%)]\t training loss: 0.003159\n",
      "epoch: 4 [10240/60000 (17%)]\t training loss: 0.131802\n",
      "epoch: 4 [10560/60000 (18%)]\t training loss: 0.000124\n",
      "epoch: 4 [10880/60000 (18%)]\t training loss: 0.009205\n",
      "epoch: 4 [11200/60000 (19%)]\t training loss: 0.039740\n",
      "epoch: 4 [11520/60000 (19%)]\t training loss: 0.029937\n",
      "epoch: 4 [11840/60000 (20%)]\t training loss: 0.045270\n",
      "epoch: 4 [12160/60000 (20%)]\t training loss: 0.002495\n",
      "epoch: 4 [12480/60000 (21%)]\t training loss: 0.097782\n",
      "epoch: 4 [12800/60000 (21%)]\t training loss: 0.159339\n",
      "epoch: 4 [13120/60000 (22%)]\t training loss: 0.000728\n",
      "epoch: 4 [13440/60000 (22%)]\t training loss: 0.002572\n",
      "epoch: 4 [13760/60000 (23%)]\t training loss: 0.008051\n",
      "epoch: 4 [14080/60000 (23%)]\t training loss: 0.002703\n",
      "epoch: 4 [14400/60000 (24%)]\t training loss: 0.402741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 [14720/60000 (25%)]\t training loss: 0.003741\n",
      "epoch: 4 [15040/60000 (25%)]\t training loss: 0.000813\n",
      "epoch: 4 [15360/60000 (26%)]\t training loss: 0.068797\n",
      "epoch: 4 [15680/60000 (26%)]\t training loss: 0.224664\n",
      "epoch: 4 [16000/60000 (27%)]\t training loss: 0.065075\n",
      "epoch: 4 [16320/60000 (27%)]\t training loss: 0.076946\n",
      "epoch: 4 [16640/60000 (28%)]\t training loss: 0.146455\n",
      "epoch: 4 [16960/60000 (28%)]\t training loss: 0.074993\n",
      "epoch: 4 [17280/60000 (29%)]\t training loss: 0.184979\n",
      "epoch: 4 [17600/60000 (29%)]\t training loss: 0.013511\n",
      "epoch: 4 [17920/60000 (30%)]\t training loss: 0.009074\n",
      "epoch: 4 [18240/60000 (30%)]\t training loss: 0.006931\n",
      "epoch: 4 [18560/60000 (31%)]\t training loss: 0.140665\n",
      "epoch: 4 [18880/60000 (31%)]\t training loss: 0.003836\n",
      "epoch: 4 [19200/60000 (32%)]\t training loss: 0.006270\n",
      "epoch: 4 [19520/60000 (33%)]\t training loss: 0.001274\n",
      "epoch: 4 [19840/60000 (33%)]\t training loss: 0.004944\n",
      "epoch: 4 [20160/60000 (34%)]\t training loss: 0.027002\n",
      "epoch: 4 [20480/60000 (34%)]\t training loss: 0.025768\n",
      "epoch: 4 [20800/60000 (35%)]\t training loss: 0.045333\n",
      "epoch: 4 [21120/60000 (35%)]\t training loss: 0.016152\n",
      "epoch: 4 [21440/60000 (36%)]\t training loss: 0.056124\n",
      "epoch: 4 [21760/60000 (36%)]\t training loss: 0.005882\n",
      "epoch: 4 [22080/60000 (37%)]\t training loss: 0.003053\n",
      "epoch: 4 [22400/60000 (37%)]\t training loss: 0.010362\n",
      "epoch: 4 [22720/60000 (38%)]\t training loss: 0.001657\n",
      "epoch: 4 [23040/60000 (38%)]\t training loss: 0.006272\n",
      "epoch: 4 [23360/60000 (39%)]\t training loss: 0.002091\n",
      "epoch: 4 [23680/60000 (39%)]\t training loss: 0.006519\n",
      "epoch: 4 [24000/60000 (40%)]\t training loss: 0.005013\n",
      "epoch: 4 [24320/60000 (41%)]\t training loss: 0.021789\n",
      "epoch: 4 [24640/60000 (41%)]\t training loss: 0.004656\n",
      "epoch: 4 [24960/60000 (42%)]\t training loss: 0.009062\n",
      "epoch: 4 [25280/60000 (42%)]\t training loss: 0.007748\n",
      "epoch: 4 [25600/60000 (43%)]\t training loss: 0.206436\n",
      "epoch: 4 [25920/60000 (43%)]\t training loss: 0.174343\n",
      "epoch: 4 [26240/60000 (44%)]\t training loss: 0.030981\n",
      "epoch: 4 [26560/60000 (44%)]\t training loss: 0.044553\n",
      "epoch: 4 [26880/60000 (45%)]\t training loss: 0.001978\n",
      "epoch: 4 [27200/60000 (45%)]\t training loss: 0.027663\n",
      "epoch: 4 [27520/60000 (46%)]\t training loss: 0.017172\n",
      "epoch: 4 [27840/60000 (46%)]\t training loss: 0.008493\n",
      "epoch: 4 [28160/60000 (47%)]\t training loss: 0.255870\n",
      "epoch: 4 [28480/60000 (47%)]\t training loss: 0.007299\n",
      "epoch: 4 [28800/60000 (48%)]\t training loss: 0.028313\n",
      "epoch: 4 [29120/60000 (49%)]\t training loss: 0.001132\n",
      "epoch: 4 [29440/60000 (49%)]\t training loss: 0.333061\n",
      "epoch: 4 [29760/60000 (50%)]\t training loss: 0.194014\n",
      "epoch: 4 [30080/60000 (50%)]\t training loss: 0.001620\n",
      "epoch: 4 [30400/60000 (51%)]\t training loss: 0.026771\n",
      "epoch: 4 [30720/60000 (51%)]\t training loss: 0.227674\n",
      "epoch: 4 [31040/60000 (52%)]\t training loss: 0.002701\n",
      "epoch: 4 [31360/60000 (52%)]\t training loss: 0.180754\n",
      "epoch: 4 [31680/60000 (53%)]\t training loss: 0.114224\n",
      "epoch: 4 [32000/60000 (53%)]\t training loss: 0.002474\n",
      "epoch: 4 [32320/60000 (54%)]\t training loss: 0.262275\n",
      "epoch: 4 [32640/60000 (54%)]\t training loss: 0.382629\n",
      "epoch: 4 [32960/60000 (55%)]\t training loss: 0.002380\n",
      "epoch: 4 [33280/60000 (55%)]\t training loss: 0.222471\n",
      "epoch: 4 [33600/60000 (56%)]\t training loss: 0.113147\n",
      "epoch: 4 [33920/60000 (57%)]\t training loss: 0.097626\n",
      "epoch: 4 [34240/60000 (57%)]\t training loss: 0.002053\n",
      "epoch: 4 [34560/60000 (58%)]\t training loss: 0.029388\n",
      "epoch: 4 [34880/60000 (58%)]\t training loss: 0.013864\n",
      "epoch: 4 [35200/60000 (59%)]\t training loss: 0.028255\n",
      "epoch: 4 [35520/60000 (59%)]\t training loss: 0.000784\n",
      "epoch: 4 [35840/60000 (60%)]\t training loss: 0.111125\n",
      "epoch: 4 [36160/60000 (60%)]\t training loss: 0.090987\n",
      "epoch: 4 [36480/60000 (61%)]\t training loss: 0.113590\n",
      "epoch: 4 [36800/60000 (61%)]\t training loss: 0.005755\n",
      "epoch: 4 [37120/60000 (62%)]\t training loss: 0.001091\n",
      "epoch: 4 [37440/60000 (62%)]\t training loss: 0.001959\n",
      "epoch: 4 [37760/60000 (63%)]\t training loss: 0.009586\n",
      "epoch: 4 [38080/60000 (63%)]\t training loss: 0.062555\n",
      "epoch: 4 [38400/60000 (64%)]\t training loss: 0.012261\n",
      "epoch: 4 [38720/60000 (65%)]\t training loss: 0.001508\n",
      "epoch: 4 [39040/60000 (65%)]\t training loss: 0.008312\n",
      "epoch: 4 [39360/60000 (66%)]\t training loss: 0.001327\n",
      "epoch: 4 [39680/60000 (66%)]\t training loss: 0.030662\n",
      "epoch: 4 [40000/60000 (67%)]\t training loss: 0.022747\n",
      "epoch: 4 [40320/60000 (67%)]\t training loss: 0.096541\n",
      "epoch: 4 [40640/60000 (68%)]\t training loss: 0.031449\n",
      "epoch: 4 [40960/60000 (68%)]\t training loss: 0.061628\n",
      "epoch: 4 [41280/60000 (69%)]\t training loss: 0.010452\n",
      "epoch: 4 [41600/60000 (69%)]\t training loss: 0.015453\n",
      "epoch: 4 [41920/60000 (70%)]\t training loss: 0.009215\n",
      "epoch: 4 [42240/60000 (70%)]\t training loss: 0.001914\n",
      "epoch: 4 [42560/60000 (71%)]\t training loss: 0.024056\n",
      "epoch: 4 [42880/60000 (71%)]\t training loss: 0.010400\n",
      "epoch: 4 [43200/60000 (72%)]\t training loss: 0.002728\n",
      "epoch: 4 [43520/60000 (73%)]\t training loss: 0.003862\n",
      "epoch: 4 [43840/60000 (73%)]\t training loss: 0.005479\n",
      "epoch: 4 [44160/60000 (74%)]\t training loss: 0.079303\n",
      "epoch: 4 [44480/60000 (74%)]\t training loss: 0.084231\n",
      "epoch: 4 [44800/60000 (75%)]\t training loss: 0.001420\n",
      "epoch: 4 [45120/60000 (75%)]\t training loss: 0.012652\n",
      "epoch: 4 [45440/60000 (76%)]\t training loss: 0.010992\n",
      "epoch: 4 [45760/60000 (76%)]\t training loss: 0.029535\n",
      "epoch: 4 [46080/60000 (77%)]\t training loss: 0.012766\n",
      "epoch: 4 [46400/60000 (77%)]\t training loss: 0.000242\n",
      "epoch: 4 [46720/60000 (78%)]\t training loss: 0.008070\n",
      "epoch: 4 [47040/60000 (78%)]\t training loss: 0.060609\n",
      "epoch: 4 [47360/60000 (79%)]\t training loss: 0.009959\n",
      "epoch: 4 [47680/60000 (79%)]\t training loss: 0.002576\n",
      "epoch: 4 [48000/60000 (80%)]\t training loss: 0.002620\n",
      "epoch: 4 [48320/60000 (81%)]\t training loss: 0.028463\n",
      "epoch: 4 [48640/60000 (81%)]\t training loss: 0.000164\n",
      "epoch: 4 [48960/60000 (82%)]\t training loss: 0.334162\n",
      "epoch: 4 [49280/60000 (82%)]\t training loss: 0.031333\n",
      "epoch: 4 [49600/60000 (83%)]\t training loss: 0.039827\n",
      "epoch: 4 [49920/60000 (83%)]\t training loss: 0.001590\n",
      "epoch: 4 [50240/60000 (84%)]\t training loss: 0.008349\n",
      "epoch: 4 [50560/60000 (84%)]\t training loss: 0.006349\n",
      "epoch: 4 [50880/60000 (85%)]\t training loss: 0.004282\n",
      "epoch: 4 [51200/60000 (85%)]\t training loss: 0.013738\n",
      "epoch: 4 [51520/60000 (86%)]\t training loss: 0.001238\n",
      "epoch: 4 [51840/60000 (86%)]\t training loss: 0.024195\n",
      "epoch: 4 [52160/60000 (87%)]\t training loss: 0.097111\n",
      "epoch: 4 [52480/60000 (87%)]\t training loss: 0.030421\n",
      "epoch: 4 [52800/60000 (88%)]\t training loss: 0.006924\n",
      "epoch: 4 [53120/60000 (89%)]\t training loss: 0.010576\n",
      "epoch: 4 [53440/60000 (89%)]\t training loss: 0.122068\n",
      "epoch: 4 [53760/60000 (90%)]\t training loss: 0.007172\n",
      "epoch: 4 [54080/60000 (90%)]\t training loss: 0.043696\n",
      "epoch: 4 [54400/60000 (91%)]\t training loss: 0.020576\n",
      "epoch: 4 [54720/60000 (91%)]\t training loss: 0.046032\n",
      "epoch: 4 [55040/60000 (92%)]\t training loss: 0.000366\n",
      "epoch: 4 [55360/60000 (92%)]\t training loss: 0.017683\n",
      "epoch: 4 [55680/60000 (93%)]\t training loss: 0.037273\n",
      "epoch: 4 [56000/60000 (93%)]\t training loss: 0.056606\n",
      "epoch: 4 [56320/60000 (94%)]\t training loss: 0.039612\n",
      "epoch: 4 [56640/60000 (94%)]\t training loss: 0.019219\n",
      "epoch: 4 [56960/60000 (95%)]\t training loss: 0.031650\n",
      "epoch: 4 [57280/60000 (95%)]\t training loss: 0.037404\n",
      "epoch: 4 [57600/60000 (96%)]\t training loss: 0.074292\n",
      "epoch: 4 [57920/60000 (97%)]\t training loss: 0.004227\n",
      "epoch: 4 [58240/60000 (97%)]\t training loss: 0.035461\n",
      "epoch: 4 [58560/60000 (98%)]\t training loss: 0.001987\n",
      "epoch: 4 [58880/60000 (98%)]\t training loss: 0.205433\n",
      "epoch: 4 [59200/60000 (99%)]\t training loss: 0.013687\n",
      "epoch: 4 [59520/60000 (99%)]\t training loss: 0.008200\n",
      "epoch: 4 [59840/60000 (100%)]\t training loss: 0.092145\n",
      "\n",
      "Test dataset: Overall Loss: 0.0323, Overall Accuracy: 9901/10000 (99%)\n",
      "\n",
      "epoch: 5 [0/60000 (0%)]\t training loss: 0.004650\n",
      "epoch: 5 [320/60000 (1%)]\t training loss: 0.000488\n",
      "epoch: 5 [640/60000 (1%)]\t training loss: 0.028786\n",
      "epoch: 5 [960/60000 (2%)]\t training loss: 0.025814\n",
      "epoch: 5 [1280/60000 (2%)]\t training loss: 0.028528\n",
      "epoch: 5 [1600/60000 (3%)]\t training loss: 0.013040\n",
      "epoch: 5 [1920/60000 (3%)]\t training loss: 0.003453\n",
      "epoch: 5 [2240/60000 (4%)]\t training loss: 0.045001\n",
      "epoch: 5 [2560/60000 (4%)]\t training loss: 0.061754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 [2880/60000 (5%)]\t training loss: 0.018190\n",
      "epoch: 5 [3200/60000 (5%)]\t training loss: 0.002218\n",
      "epoch: 5 [3520/60000 (6%)]\t training loss: 0.007195\n",
      "epoch: 5 [3840/60000 (6%)]\t training loss: 0.045235\n",
      "epoch: 5 [4160/60000 (7%)]\t training loss: 0.025732\n",
      "epoch: 5 [4480/60000 (7%)]\t training loss: 0.055758\n",
      "epoch: 5 [4800/60000 (8%)]\t training loss: 0.077755\n",
      "epoch: 5 [5120/60000 (9%)]\t training loss: 0.071746\n",
      "epoch: 5 [5440/60000 (9%)]\t training loss: 0.007385\n",
      "epoch: 5 [5760/60000 (10%)]\t training loss: 0.003910\n",
      "epoch: 5 [6080/60000 (10%)]\t training loss: 0.005028\n",
      "epoch: 5 [6400/60000 (11%)]\t training loss: 0.092706\n",
      "epoch: 5 [6720/60000 (11%)]\t training loss: 0.006512\n",
      "epoch: 5 [7040/60000 (12%)]\t training loss: 0.020855\n",
      "epoch: 5 [7360/60000 (12%)]\t training loss: 0.000657\n",
      "epoch: 5 [7680/60000 (13%)]\t training loss: 0.000102\n",
      "epoch: 5 [8000/60000 (13%)]\t training loss: 0.043879\n",
      "epoch: 5 [8320/60000 (14%)]\t training loss: 0.017966\n",
      "epoch: 5 [8640/60000 (14%)]\t training loss: 0.008892\n",
      "epoch: 5 [8960/60000 (15%)]\t training loss: 0.000578\n",
      "epoch: 5 [9280/60000 (15%)]\t training loss: 0.002221\n",
      "epoch: 5 [9600/60000 (16%)]\t training loss: 0.001904\n",
      "epoch: 5 [9920/60000 (17%)]\t training loss: 0.010654\n",
      "epoch: 5 [10240/60000 (17%)]\t training loss: 0.005652\n",
      "epoch: 5 [10560/60000 (18%)]\t training loss: 0.006549\n",
      "epoch: 5 [10880/60000 (18%)]\t training loss: 0.003582\n",
      "epoch: 5 [11200/60000 (19%)]\t training loss: 0.052239\n",
      "epoch: 5 [11520/60000 (19%)]\t training loss: 0.007484\n",
      "epoch: 5 [11840/60000 (20%)]\t training loss: 0.003687\n",
      "epoch: 5 [12160/60000 (20%)]\t training loss: 0.058647\n",
      "epoch: 5 [12480/60000 (21%)]\t training loss: 0.027853\n",
      "epoch: 5 [12800/60000 (21%)]\t training loss: 0.131618\n",
      "epoch: 5 [13120/60000 (22%)]\t training loss: 0.009149\n",
      "epoch: 5 [13440/60000 (22%)]\t training loss: 0.023154\n",
      "epoch: 5 [13760/60000 (23%)]\t training loss: 0.009235\n",
      "epoch: 5 [14080/60000 (23%)]\t training loss: 0.003872\n",
      "epoch: 5 [14400/60000 (24%)]\t training loss: 0.004979\n",
      "epoch: 5 [14720/60000 (25%)]\t training loss: 0.211830\n",
      "epoch: 5 [15040/60000 (25%)]\t training loss: 0.068208\n",
      "epoch: 5 [15360/60000 (26%)]\t training loss: 0.003670\n",
      "epoch: 5 [15680/60000 (26%)]\t training loss: 0.355840\n",
      "epoch: 5 [16000/60000 (27%)]\t training loss: 0.013605\n",
      "epoch: 5 [16320/60000 (27%)]\t training loss: 0.002708\n",
      "epoch: 5 [16640/60000 (28%)]\t training loss: 0.003084\n",
      "epoch: 5 [16960/60000 (28%)]\t training loss: 0.010479\n",
      "epoch: 5 [17280/60000 (29%)]\t training loss: 0.004940\n",
      "epoch: 5 [17600/60000 (29%)]\t training loss: 0.042987\n",
      "epoch: 5 [17920/60000 (30%)]\t training loss: 0.014880\n",
      "epoch: 5 [18240/60000 (30%)]\t training loss: 0.003508\n",
      "epoch: 5 [18560/60000 (31%)]\t training loss: 0.001534\n",
      "epoch: 5 [18880/60000 (31%)]\t training loss: 0.008373\n",
      "epoch: 5 [19200/60000 (32%)]\t training loss: 0.000758\n",
      "epoch: 5 [19520/60000 (33%)]\t training loss: 0.050042\n",
      "epoch: 5 [19840/60000 (33%)]\t training loss: 0.032123\n",
      "epoch: 5 [20160/60000 (34%)]\t training loss: 0.005268\n",
      "epoch: 5 [20480/60000 (34%)]\t training loss: 0.096781\n",
      "epoch: 5 [20800/60000 (35%)]\t training loss: 0.136668\n",
      "epoch: 5 [21120/60000 (35%)]\t training loss: 0.041153\n",
      "epoch: 5 [21440/60000 (36%)]\t training loss: 0.031725\n",
      "epoch: 5 [21760/60000 (36%)]\t training loss: 0.000754\n",
      "epoch: 5 [22080/60000 (37%)]\t training loss: 0.030025\n",
      "epoch: 5 [22400/60000 (37%)]\t training loss: 0.098685\n",
      "epoch: 5 [22720/60000 (38%)]\t training loss: 0.128120\n",
      "epoch: 5 [23040/60000 (38%)]\t training loss: 0.148604\n",
      "epoch: 5 [23360/60000 (39%)]\t training loss: 0.009033\n",
      "epoch: 5 [23680/60000 (39%)]\t training loss: 0.012936\n",
      "epoch: 5 [24000/60000 (40%)]\t training loss: 0.706043\n",
      "epoch: 5 [24320/60000 (41%)]\t training loss: 0.046699\n",
      "epoch: 5 [24640/60000 (41%)]\t training loss: 0.103027\n",
      "epoch: 5 [24960/60000 (42%)]\t training loss: 0.014533\n",
      "epoch: 5 [25280/60000 (42%)]\t training loss: 0.013849\n",
      "epoch: 5 [25600/60000 (43%)]\t training loss: 0.007564\n",
      "epoch: 5 [25920/60000 (43%)]\t training loss: 0.000979\n",
      "epoch: 5 [26240/60000 (44%)]\t training loss: 0.002783\n",
      "epoch: 5 [26560/60000 (44%)]\t training loss: 0.001792\n",
      "epoch: 5 [26880/60000 (45%)]\t training loss: 0.006244\n",
      "epoch: 5 [27200/60000 (45%)]\t training loss: 0.000698\n",
      "epoch: 5 [27520/60000 (46%)]\t training loss: 0.010512\n",
      "epoch: 5 [27840/60000 (46%)]\t training loss: 0.113084\n",
      "epoch: 5 [28160/60000 (47%)]\t training loss: 0.000319\n",
      "epoch: 5 [28480/60000 (47%)]\t training loss: 0.002585\n",
      "epoch: 5 [28800/60000 (48%)]\t training loss: 0.088744\n",
      "epoch: 5 [29120/60000 (49%)]\t training loss: 0.011535\n",
      "epoch: 5 [29440/60000 (49%)]\t training loss: 0.002331\n",
      "epoch: 5 [29760/60000 (50%)]\t training loss: 0.006364\n",
      "epoch: 5 [30080/60000 (50%)]\t training loss: 0.011804\n",
      "epoch: 5 [30400/60000 (51%)]\t training loss: 0.046118\n",
      "epoch: 5 [30720/60000 (51%)]\t training loss: 0.091301\n",
      "epoch: 5 [31040/60000 (52%)]\t training loss: 0.009207\n",
      "epoch: 5 [31360/60000 (52%)]\t training loss: 0.018199\n",
      "epoch: 5 [31680/60000 (53%)]\t training loss: 0.001366\n",
      "epoch: 5 [32000/60000 (53%)]\t training loss: 0.012185\n",
      "epoch: 5 [32320/60000 (54%)]\t training loss: 0.215171\n",
      "epoch: 5 [32640/60000 (54%)]\t training loss: 0.046741\n",
      "epoch: 5 [32960/60000 (55%)]\t training loss: 0.004009\n",
      "epoch: 5 [33280/60000 (55%)]\t training loss: 0.263167\n",
      "epoch: 5 [33600/60000 (56%)]\t training loss: 0.022981\n",
      "epoch: 5 [33920/60000 (57%)]\t training loss: 0.054469\n",
      "epoch: 5 [34240/60000 (57%)]\t training loss: 0.000600\n",
      "epoch: 5 [34560/60000 (58%)]\t training loss: 0.040304\n",
      "epoch: 5 [34880/60000 (58%)]\t training loss: 0.077766\n",
      "epoch: 5 [35200/60000 (59%)]\t training loss: 0.009225\n",
      "epoch: 5 [35520/60000 (59%)]\t training loss: 0.006721\n",
      "epoch: 5 [35840/60000 (60%)]\t training loss: 0.007453\n",
      "epoch: 5 [36160/60000 (60%)]\t training loss: 0.007646\n",
      "epoch: 5 [36480/60000 (61%)]\t training loss: 0.002838\n",
      "epoch: 5 [36800/60000 (61%)]\t training loss: 0.000324\n",
      "epoch: 5 [37120/60000 (62%)]\t training loss: 0.040074\n",
      "epoch: 5 [37440/60000 (62%)]\t training loss: 0.004059\n",
      "epoch: 5 [37760/60000 (63%)]\t training loss: 0.035025\n",
      "epoch: 5 [38080/60000 (63%)]\t training loss: 0.078703\n",
      "epoch: 5 [38400/60000 (64%)]\t training loss: 0.021135\n",
      "epoch: 5 [38720/60000 (65%)]\t training loss: 0.054524\n",
      "epoch: 5 [39040/60000 (65%)]\t training loss: 0.003205\n",
      "epoch: 5 [39360/60000 (66%)]\t training loss: 0.011270\n",
      "epoch: 5 [39680/60000 (66%)]\t training loss: 0.001363\n",
      "epoch: 5 [40000/60000 (67%)]\t training loss: 0.028749\n",
      "epoch: 5 [40320/60000 (67%)]\t training loss: 0.042951\n",
      "epoch: 5 [40640/60000 (68%)]\t training loss: 0.000815\n",
      "epoch: 5 [40960/60000 (68%)]\t training loss: 0.003779\n",
      "epoch: 5 [41280/60000 (69%)]\t training loss: 0.079464\n",
      "epoch: 5 [41600/60000 (69%)]\t training loss: 0.023876\n",
      "epoch: 5 [41920/60000 (70%)]\t training loss: 0.255084\n",
      "epoch: 5 [42240/60000 (70%)]\t training loss: 0.002720\n",
      "epoch: 5 [42560/60000 (71%)]\t training loss: 0.007795\n",
      "epoch: 5 [42880/60000 (71%)]\t training loss: 0.002141\n",
      "epoch: 5 [43200/60000 (72%)]\t training loss: 0.003617\n",
      "epoch: 5 [43520/60000 (73%)]\t training loss: 0.304875\n",
      "epoch: 5 [43840/60000 (73%)]\t training loss: 0.001682\n",
      "epoch: 5 [44160/60000 (74%)]\t training loss: 0.033148\n",
      "epoch: 5 [44480/60000 (74%)]\t training loss: 0.000671\n",
      "epoch: 5 [44800/60000 (75%)]\t training loss: 0.019732\n",
      "epoch: 5 [45120/60000 (75%)]\t training loss: 0.042335\n",
      "epoch: 5 [45440/60000 (76%)]\t training loss: 0.000147\n",
      "epoch: 5 [45760/60000 (76%)]\t training loss: 0.002510\n",
      "epoch: 5 [46080/60000 (77%)]\t training loss: 0.038071\n",
      "epoch: 5 [46400/60000 (77%)]\t training loss: 0.119329\n",
      "epoch: 5 [46720/60000 (78%)]\t training loss: 0.332029\n",
      "epoch: 5 [47040/60000 (78%)]\t training loss: 0.004910\n",
      "epoch: 5 [47360/60000 (79%)]\t training loss: 0.042647\n",
      "epoch: 5 [47680/60000 (79%)]\t training loss: 0.001090\n",
      "epoch: 5 [48000/60000 (80%)]\t training loss: 0.007457\n",
      "epoch: 5 [48320/60000 (81%)]\t training loss: 0.031446\n",
      "epoch: 5 [48640/60000 (81%)]\t training loss: 0.029946\n",
      "epoch: 5 [48960/60000 (82%)]\t training loss: 0.158900\n",
      "epoch: 5 [49280/60000 (82%)]\t training loss: 0.001885\n",
      "epoch: 5 [49600/60000 (83%)]\t training loss: 0.022795\n",
      "epoch: 5 [49920/60000 (83%)]\t training loss: 0.163102\n",
      "epoch: 5 [50240/60000 (84%)]\t training loss: 0.003181\n",
      "epoch: 5 [50560/60000 (84%)]\t training loss: 0.002570\n",
      "epoch: 5 [50880/60000 (85%)]\t training loss: 0.002756\n",
      "epoch: 5 [51200/60000 (85%)]\t training loss: 0.029537\n",
      "epoch: 5 [51520/60000 (86%)]\t training loss: 0.022649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 [51840/60000 (86%)]\t training loss: 0.040532\n",
      "epoch: 5 [52160/60000 (87%)]\t training loss: 0.050741\n",
      "epoch: 5 [52480/60000 (87%)]\t training loss: 0.008119\n",
      "epoch: 5 [52800/60000 (88%)]\t training loss: 0.000579\n",
      "epoch: 5 [53120/60000 (89%)]\t training loss: 0.006972\n",
      "epoch: 5 [53440/60000 (89%)]\t training loss: 0.078877\n",
      "epoch: 5 [53760/60000 (90%)]\t training loss: 0.000191\n",
      "epoch: 5 [54080/60000 (90%)]\t training loss: 0.148032\n",
      "epoch: 5 [54400/60000 (91%)]\t training loss: 0.130252\n",
      "epoch: 5 [54720/60000 (91%)]\t training loss: 0.003896\n",
      "epoch: 5 [55040/60000 (92%)]\t training loss: 0.045337\n",
      "epoch: 5 [55360/60000 (92%)]\t training loss: 0.119234\n",
      "epoch: 5 [55680/60000 (93%)]\t training loss: 0.011376\n",
      "epoch: 5 [56000/60000 (93%)]\t training loss: 0.094953\n",
      "epoch: 5 [56320/60000 (94%)]\t training loss: 0.007692\n",
      "epoch: 5 [56640/60000 (94%)]\t training loss: 0.065174\n",
      "epoch: 5 [56960/60000 (95%)]\t training loss: 0.037893\n",
      "epoch: 5 [57280/60000 (95%)]\t training loss: 0.038788\n",
      "epoch: 5 [57600/60000 (96%)]\t training loss: 0.020072\n",
      "epoch: 5 [57920/60000 (97%)]\t training loss: 0.041185\n",
      "epoch: 5 [58240/60000 (97%)]\t training loss: 0.004313\n",
      "epoch: 5 [58560/60000 (98%)]\t training loss: 0.002913\n",
      "epoch: 5 [58880/60000 (98%)]\t training loss: 0.157466\n",
      "epoch: 5 [59200/60000 (99%)]\t training loss: 0.103567\n",
      "epoch: 5 [59520/60000 (99%)]\t training loss: 0.015888\n",
      "epoch: 5 [59840/60000 (100%)]\t training loss: 0.041920\n",
      "\n",
      "Test dataset: Overall Loss: 0.0395, Overall Accuracy: 9885/10000 (99%)\n",
      "\n",
      "epoch: 6 [0/60000 (0%)]\t training loss: 0.021512\n",
      "epoch: 6 [320/60000 (1%)]\t training loss: 0.000171\n",
      "epoch: 6 [640/60000 (1%)]\t training loss: 0.013914\n",
      "epoch: 6 [960/60000 (2%)]\t training loss: 0.055000\n",
      "epoch: 6 [1280/60000 (2%)]\t training loss: 0.001151\n",
      "epoch: 6 [1600/60000 (3%)]\t training loss: 0.005175\n",
      "epoch: 6 [1920/60000 (3%)]\t training loss: 0.222675\n",
      "epoch: 6 [2240/60000 (4%)]\t training loss: 0.007091\n",
      "epoch: 6 [2560/60000 (4%)]\t training loss: 0.000372\n",
      "epoch: 6 [2880/60000 (5%)]\t training loss: 0.001639\n",
      "epoch: 6 [3200/60000 (5%)]\t training loss: 0.230255\n",
      "epoch: 6 [3520/60000 (6%)]\t training loss: 0.052727\n",
      "epoch: 6 [3840/60000 (6%)]\t training loss: 0.012916\n",
      "epoch: 6 [4160/60000 (7%)]\t training loss: 0.010933\n",
      "epoch: 6 [4480/60000 (7%)]\t training loss: 0.003036\n",
      "epoch: 6 [4800/60000 (8%)]\t training loss: 0.002368\n",
      "epoch: 6 [5120/60000 (9%)]\t training loss: 0.001419\n",
      "epoch: 6 [5440/60000 (9%)]\t training loss: 0.008995\n",
      "epoch: 6 [5760/60000 (10%)]\t training loss: 0.002247\n",
      "epoch: 6 [6080/60000 (10%)]\t training loss: 0.014631\n",
      "epoch: 6 [6400/60000 (11%)]\t training loss: 0.005977\n",
      "epoch: 6 [6720/60000 (11%)]\t training loss: 0.020381\n",
      "epoch: 6 [7040/60000 (12%)]\t training loss: 0.003737\n",
      "epoch: 6 [7360/60000 (12%)]\t training loss: 0.034941\n",
      "epoch: 6 [7680/60000 (13%)]\t training loss: 0.126762\n",
      "epoch: 6 [8000/60000 (13%)]\t training loss: 0.010101\n",
      "epoch: 6 [8320/60000 (14%)]\t training loss: 0.044190\n",
      "epoch: 6 [8640/60000 (14%)]\t training loss: 0.010093\n",
      "epoch: 6 [8960/60000 (15%)]\t training loss: 0.007398\n",
      "epoch: 6 [9280/60000 (15%)]\t training loss: 0.000835\n",
      "epoch: 6 [9600/60000 (16%)]\t training loss: 0.135697\n",
      "epoch: 6 [9920/60000 (17%)]\t training loss: 0.000406\n",
      "epoch: 6 [10240/60000 (17%)]\t training loss: 0.000858\n",
      "epoch: 6 [10560/60000 (18%)]\t training loss: 0.004311\n",
      "epoch: 6 [10880/60000 (18%)]\t training loss: 0.006577\n",
      "epoch: 6 [11200/60000 (19%)]\t training loss: 0.018175\n",
      "epoch: 6 [11520/60000 (19%)]\t training loss: 0.022991\n",
      "epoch: 6 [11840/60000 (20%)]\t training loss: 0.000928\n",
      "epoch: 6 [12160/60000 (20%)]\t training loss: 0.014201\n",
      "epoch: 6 [12480/60000 (21%)]\t training loss: 0.000816\n",
      "epoch: 6 [12800/60000 (21%)]\t training loss: 0.010329\n",
      "epoch: 6 [13120/60000 (22%)]\t training loss: 0.102604\n",
      "epoch: 6 [13440/60000 (22%)]\t training loss: 0.009113\n",
      "epoch: 6 [13760/60000 (23%)]\t training loss: 0.018244\n",
      "epoch: 6 [14080/60000 (23%)]\t training loss: 0.002824\n",
      "epoch: 6 [14400/60000 (24%)]\t training loss: 0.056817\n",
      "epoch: 6 [14720/60000 (25%)]\t training loss: 0.004142\n",
      "epoch: 6 [15040/60000 (25%)]\t training loss: 0.103497\n",
      "epoch: 6 [15360/60000 (26%)]\t training loss: 0.016736\n",
      "epoch: 6 [15680/60000 (26%)]\t training loss: 0.000252\n",
      "epoch: 6 [16000/60000 (27%)]\t training loss: 0.022340\n",
      "epoch: 6 [16320/60000 (27%)]\t training loss: 0.148325\n",
      "epoch: 6 [16640/60000 (28%)]\t training loss: 0.001848\n",
      "epoch: 6 [16960/60000 (28%)]\t training loss: 0.016520\n",
      "epoch: 6 [17280/60000 (29%)]\t training loss: 0.018872\n",
      "epoch: 6 [17600/60000 (29%)]\t training loss: 0.052963\n",
      "epoch: 6 [17920/60000 (30%)]\t training loss: 0.011826\n",
      "epoch: 6 [18240/60000 (30%)]\t training loss: 0.002239\n",
      "epoch: 6 [18560/60000 (31%)]\t training loss: 0.125960\n",
      "epoch: 6 [18880/60000 (31%)]\t training loss: 0.014657\n",
      "epoch: 6 [19200/60000 (32%)]\t training loss: 0.008389\n",
      "epoch: 6 [19520/60000 (33%)]\t training loss: 0.001592\n",
      "epoch: 6 [19840/60000 (33%)]\t training loss: 0.042713\n",
      "epoch: 6 [20160/60000 (34%)]\t training loss: 0.005446\n",
      "epoch: 6 [20480/60000 (34%)]\t training loss: 0.156752\n",
      "epoch: 6 [20800/60000 (35%)]\t training loss: 0.006549\n",
      "epoch: 6 [21120/60000 (35%)]\t training loss: 0.113889\n",
      "epoch: 6 [21440/60000 (36%)]\t training loss: 0.030046\n",
      "epoch: 6 [21760/60000 (36%)]\t training loss: 0.005912\n",
      "epoch: 6 [22080/60000 (37%)]\t training loss: 0.003305\n",
      "epoch: 6 [22400/60000 (37%)]\t training loss: 0.000790\n",
      "epoch: 6 [22720/60000 (38%)]\t training loss: 0.160360\n",
      "epoch: 6 [23040/60000 (38%)]\t training loss: 0.003156\n",
      "epoch: 6 [23360/60000 (39%)]\t training loss: 0.004927\n",
      "epoch: 6 [23680/60000 (39%)]\t training loss: 0.027326\n",
      "epoch: 6 [24000/60000 (40%)]\t training loss: 0.000560\n",
      "epoch: 6 [24320/60000 (41%)]\t training loss: 0.046787\n",
      "epoch: 6 [24640/60000 (41%)]\t training loss: 0.054256\n",
      "epoch: 6 [24960/60000 (42%)]\t training loss: 0.293003\n",
      "epoch: 6 [25280/60000 (42%)]\t training loss: 0.008611\n",
      "epoch: 6 [25600/60000 (43%)]\t training loss: 0.005083\n",
      "epoch: 6 [25920/60000 (43%)]\t training loss: 0.001906\n",
      "epoch: 6 [26240/60000 (44%)]\t training loss: 0.000612\n",
      "epoch: 6 [26560/60000 (44%)]\t training loss: 0.009549\n",
      "epoch: 6 [26880/60000 (45%)]\t training loss: 0.015579\n",
      "epoch: 6 [27200/60000 (45%)]\t training loss: 0.000207\n",
      "epoch: 6 [27520/60000 (46%)]\t training loss: 0.004881\n",
      "epoch: 6 [27840/60000 (46%)]\t training loss: 0.190341\n",
      "epoch: 6 [28160/60000 (47%)]\t training loss: 0.028860\n",
      "epoch: 6 [28480/60000 (47%)]\t training loss: 0.000947\n",
      "epoch: 6 [28800/60000 (48%)]\t training loss: 0.000593\n",
      "epoch: 6 [29120/60000 (49%)]\t training loss: 0.031823\n",
      "epoch: 6 [29440/60000 (49%)]\t training loss: 0.448494\n",
      "epoch: 6 [29760/60000 (50%)]\t training loss: 0.031076\n",
      "epoch: 6 [30080/60000 (50%)]\t training loss: 0.027418\n",
      "epoch: 6 [30400/60000 (51%)]\t training loss: 0.162534\n",
      "epoch: 6 [30720/60000 (51%)]\t training loss: 0.032812\n",
      "epoch: 6 [31040/60000 (52%)]\t training loss: 0.147315\n",
      "epoch: 6 [31360/60000 (52%)]\t training loss: 0.000382\n",
      "epoch: 6 [31680/60000 (53%)]\t training loss: 0.001628\n",
      "epoch: 6 [32000/60000 (53%)]\t training loss: 0.003648\n",
      "epoch: 6 [32320/60000 (54%)]\t training loss: 0.002503\n",
      "epoch: 6 [32640/60000 (54%)]\t training loss: 0.071103\n",
      "epoch: 6 [32960/60000 (55%)]\t training loss: 0.003363\n",
      "epoch: 6 [33280/60000 (55%)]\t training loss: 0.012514\n",
      "epoch: 6 [33600/60000 (56%)]\t training loss: 0.023977\n",
      "epoch: 6 [33920/60000 (57%)]\t training loss: 0.000970\n",
      "epoch: 6 [34240/60000 (57%)]\t training loss: 0.008086\n",
      "epoch: 6 [34560/60000 (58%)]\t training loss: 0.008244\n",
      "epoch: 6 [34880/60000 (58%)]\t training loss: 0.006829\n",
      "epoch: 6 [35200/60000 (59%)]\t training loss: 0.141610\n",
      "epoch: 6 [35520/60000 (59%)]\t training loss: 0.005576\n",
      "epoch: 6 [35840/60000 (60%)]\t training loss: 0.005873\n",
      "epoch: 6 [36160/60000 (60%)]\t training loss: 0.001270\n",
      "epoch: 6 [36480/60000 (61%)]\t training loss: 0.045942\n",
      "epoch: 6 [36800/60000 (61%)]\t training loss: 0.005185\n",
      "epoch: 6 [37120/60000 (62%)]\t training loss: 0.012953\n",
      "epoch: 6 [37440/60000 (62%)]\t training loss: 0.021640\n",
      "epoch: 6 [37760/60000 (63%)]\t training loss: 0.181598\n",
      "epoch: 6 [38080/60000 (63%)]\t training loss: 0.010774\n",
      "epoch: 6 [38400/60000 (64%)]\t training loss: 0.074771\n",
      "epoch: 6 [38720/60000 (65%)]\t training loss: 0.007428\n",
      "epoch: 6 [39040/60000 (65%)]\t training loss: 0.069695\n",
      "epoch: 6 [39360/60000 (66%)]\t training loss: 0.012100\n",
      "epoch: 6 [39680/60000 (66%)]\t training loss: 0.001101\n",
      "epoch: 6 [40000/60000 (67%)]\t training loss: 0.043388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 [40320/60000 (67%)]\t training loss: 0.003660\n",
      "epoch: 6 [40640/60000 (68%)]\t training loss: 0.000429\n",
      "epoch: 6 [40960/60000 (68%)]\t training loss: 0.015669\n",
      "epoch: 6 [41280/60000 (69%)]\t training loss: 0.003110\n",
      "epoch: 6 [41600/60000 (69%)]\t training loss: 0.048850\n",
      "epoch: 6 [41920/60000 (70%)]\t training loss: 0.005604\n",
      "epoch: 6 [42240/60000 (70%)]\t training loss: 0.002697\n",
      "epoch: 6 [42560/60000 (71%)]\t training loss: 0.075975\n",
      "epoch: 6 [42880/60000 (71%)]\t training loss: 0.020993\n",
      "epoch: 6 [43200/60000 (72%)]\t training loss: 0.022629\n",
      "epoch: 6 [43520/60000 (73%)]\t training loss: 0.012391\n",
      "epoch: 6 [43840/60000 (73%)]\t training loss: 0.011194\n",
      "epoch: 6 [44160/60000 (74%)]\t training loss: 0.023555\n",
      "epoch: 6 [44480/60000 (74%)]\t training loss: 0.058529\n",
      "epoch: 6 [44800/60000 (75%)]\t training loss: 0.017773\n",
      "epoch: 6 [45120/60000 (75%)]\t training loss: 0.011848\n",
      "epoch: 6 [45440/60000 (76%)]\t training loss: 0.007662\n",
      "epoch: 6 [45760/60000 (76%)]\t training loss: 0.013875\n",
      "epoch: 6 [46080/60000 (77%)]\t training loss: 0.082685\n",
      "epoch: 6 [46400/60000 (77%)]\t training loss: 0.028208\n",
      "epoch: 6 [46720/60000 (78%)]\t training loss: 0.152994\n",
      "epoch: 6 [47040/60000 (78%)]\t training loss: 0.019174\n",
      "epoch: 6 [47360/60000 (79%)]\t training loss: 0.177762\n",
      "epoch: 6 [47680/60000 (79%)]\t training loss: 0.007809\n",
      "epoch: 6 [48000/60000 (80%)]\t training loss: 0.037851\n",
      "epoch: 6 [48320/60000 (81%)]\t training loss: 0.012053\n",
      "epoch: 6 [48640/60000 (81%)]\t training loss: 0.028443\n",
      "epoch: 6 [48960/60000 (82%)]\t training loss: 0.002968\n",
      "epoch: 6 [49280/60000 (82%)]\t training loss: 0.063658\n",
      "epoch: 6 [49600/60000 (83%)]\t training loss: 0.139358\n",
      "epoch: 6 [49920/60000 (83%)]\t training loss: 0.000160\n",
      "epoch: 6 [50240/60000 (84%)]\t training loss: 0.025607\n",
      "epoch: 6 [50560/60000 (84%)]\t training loss: 0.035266\n",
      "epoch: 6 [50880/60000 (85%)]\t training loss: 0.019404\n",
      "epoch: 6 [51200/60000 (85%)]\t training loss: 0.154303\n",
      "epoch: 6 [51520/60000 (86%)]\t training loss: 0.013728\n",
      "epoch: 6 [51840/60000 (86%)]\t training loss: 0.018245\n",
      "epoch: 6 [52160/60000 (87%)]\t training loss: 0.030029\n",
      "epoch: 6 [52480/60000 (87%)]\t training loss: 0.002478\n",
      "epoch: 6 [52800/60000 (88%)]\t training loss: 0.005901\n",
      "epoch: 6 [53120/60000 (89%)]\t training loss: 0.000450\n",
      "epoch: 6 [53440/60000 (89%)]\t training loss: 0.013341\n",
      "epoch: 6 [53760/60000 (90%)]\t training loss: 0.002663\n",
      "epoch: 6 [54080/60000 (90%)]\t training loss: 0.004658\n",
      "epoch: 6 [54400/60000 (91%)]\t training loss: 0.004713\n",
      "epoch: 6 [54720/60000 (91%)]\t training loss: 0.026550\n",
      "epoch: 6 [55040/60000 (92%)]\t training loss: 0.028347\n",
      "epoch: 6 [55360/60000 (92%)]\t training loss: 0.004499\n",
      "epoch: 6 [55680/60000 (93%)]\t training loss: 0.019771\n",
      "epoch: 6 [56000/60000 (93%)]\t training loss: 0.056127\n",
      "epoch: 6 [56320/60000 (94%)]\t training loss: 0.070130\n",
      "epoch: 6 [56640/60000 (94%)]\t training loss: 0.003051\n",
      "epoch: 6 [56960/60000 (95%)]\t training loss: 0.016099\n",
      "epoch: 6 [57280/60000 (95%)]\t training loss: 0.131742\n",
      "epoch: 6 [57600/60000 (96%)]\t training loss: 0.002104\n",
      "epoch: 6 [57920/60000 (97%)]\t training loss: 0.002069\n",
      "epoch: 6 [58240/60000 (97%)]\t training loss: 0.055804\n",
      "epoch: 6 [58560/60000 (98%)]\t training loss: 0.006385\n",
      "epoch: 6 [58880/60000 (98%)]\t training loss: 0.091115\n",
      "epoch: 6 [59200/60000 (99%)]\t training loss: 0.000572\n",
      "epoch: 6 [59520/60000 (99%)]\t training loss: 0.172951\n",
      "epoch: 6 [59840/60000 (100%)]\t training loss: 0.003876\n",
      "\n",
      "Test dataset: Overall Loss: 0.0318, Overall Accuracy: 9905/10000 (99%)\n",
      "\n",
      "epoch: 7 [0/60000 (0%)]\t training loss: 0.000860\n",
      "epoch: 7 [320/60000 (1%)]\t training loss: 0.010930\n",
      "epoch: 7 [640/60000 (1%)]\t training loss: 0.015159\n",
      "epoch: 7 [960/60000 (2%)]\t training loss: 0.013719\n",
      "epoch: 7 [1280/60000 (2%)]\t training loss: 0.059653\n",
      "epoch: 7 [1600/60000 (3%)]\t training loss: 0.028855\n",
      "epoch: 7 [1920/60000 (3%)]\t training loss: 0.018434\n",
      "epoch: 7 [2240/60000 (4%)]\t training loss: 0.002318\n",
      "epoch: 7 [2560/60000 (4%)]\t training loss: 0.072420\n",
      "epoch: 7 [2880/60000 (5%)]\t training loss: 0.002315\n",
      "epoch: 7 [3200/60000 (5%)]\t training loss: 0.004007\n",
      "epoch: 7 [3520/60000 (6%)]\t training loss: 0.032238\n",
      "epoch: 7 [3840/60000 (6%)]\t training loss: 0.000732\n",
      "epoch: 7 [4160/60000 (7%)]\t training loss: 0.004396\n",
      "epoch: 7 [4480/60000 (7%)]\t training loss: 0.000596\n",
      "epoch: 7 [4800/60000 (8%)]\t training loss: 0.028233\n",
      "epoch: 7 [5120/60000 (9%)]\t training loss: 0.000742\n",
      "epoch: 7 [5440/60000 (9%)]\t training loss: 0.000452\n",
      "epoch: 7 [5760/60000 (10%)]\t training loss: 0.000855\n",
      "epoch: 7 [6080/60000 (10%)]\t training loss: 0.026330\n",
      "epoch: 7 [6400/60000 (11%)]\t training loss: 0.201056\n",
      "epoch: 7 [6720/60000 (11%)]\t training loss: 0.042279\n",
      "epoch: 7 [7040/60000 (12%)]\t training loss: 0.024549\n",
      "epoch: 7 [7360/60000 (12%)]\t training loss: 0.122774\n",
      "epoch: 7 [7680/60000 (13%)]\t training loss: 0.000906\n",
      "epoch: 7 [8000/60000 (13%)]\t training loss: 0.005081\n",
      "epoch: 7 [8320/60000 (14%)]\t training loss: 0.036788\n",
      "epoch: 7 [8640/60000 (14%)]\t training loss: 0.074821\n",
      "epoch: 7 [8960/60000 (15%)]\t training loss: 0.010650\n",
      "epoch: 7 [9280/60000 (15%)]\t training loss: 0.002519\n",
      "epoch: 7 [9600/60000 (16%)]\t training loss: 0.094940\n",
      "epoch: 7 [9920/60000 (17%)]\t training loss: 0.006218\n",
      "epoch: 7 [10240/60000 (17%)]\t training loss: 0.000085\n",
      "epoch: 7 [10560/60000 (18%)]\t training loss: 0.023410\n",
      "epoch: 7 [10880/60000 (18%)]\t training loss: 0.014854\n",
      "epoch: 7 [11200/60000 (19%)]\t training loss: 0.007681\n",
      "epoch: 7 [11520/60000 (19%)]\t training loss: 0.024568\n",
      "epoch: 7 [11840/60000 (20%)]\t training loss: 0.101623\n",
      "epoch: 7 [12160/60000 (20%)]\t training loss: 0.002262\n",
      "epoch: 7 [12480/60000 (21%)]\t training loss: 0.133953\n",
      "epoch: 7 [12800/60000 (21%)]\t training loss: 0.000550\n",
      "epoch: 7 [13120/60000 (22%)]\t training loss: 0.003898\n",
      "epoch: 7 [13440/60000 (22%)]\t training loss: 0.011447\n",
      "epoch: 7 [13760/60000 (23%)]\t training loss: 0.006190\n",
      "epoch: 7 [14080/60000 (23%)]\t training loss: 0.004898\n",
      "epoch: 7 [14400/60000 (24%)]\t training loss: 0.066162\n",
      "epoch: 7 [14720/60000 (25%)]\t training loss: 0.124299\n",
      "epoch: 7 [15040/60000 (25%)]\t training loss: 0.014872\n",
      "epoch: 7 [15360/60000 (26%)]\t training loss: 0.006692\n",
      "epoch: 7 [15680/60000 (26%)]\t training loss: 0.000783\n",
      "epoch: 7 [16000/60000 (27%)]\t training loss: 0.012512\n",
      "epoch: 7 [16320/60000 (27%)]\t training loss: 0.000957\n",
      "epoch: 7 [16640/60000 (28%)]\t training loss: 0.001631\n",
      "epoch: 7 [16960/60000 (28%)]\t training loss: 0.005042\n",
      "epoch: 7 [17280/60000 (29%)]\t training loss: 0.004065\n",
      "epoch: 7 [17600/60000 (29%)]\t training loss: 0.002925\n",
      "epoch: 7 [17920/60000 (30%)]\t training loss: 0.093028\n",
      "epoch: 7 [18240/60000 (30%)]\t training loss: 0.050054\n",
      "epoch: 7 [18560/60000 (31%)]\t training loss: 0.084936\n",
      "epoch: 7 [18880/60000 (31%)]\t training loss: 0.013945\n",
      "epoch: 7 [19200/60000 (32%)]\t training loss: 0.001250\n",
      "epoch: 7 [19520/60000 (33%)]\t training loss: 0.061724\n",
      "epoch: 7 [19840/60000 (33%)]\t training loss: 0.005060\n",
      "epoch: 7 [20160/60000 (34%)]\t training loss: 0.052797\n",
      "epoch: 7 [20480/60000 (34%)]\t training loss: 0.016489\n",
      "epoch: 7 [20800/60000 (35%)]\t training loss: 0.010203\n",
      "epoch: 7 [21120/60000 (35%)]\t training loss: 0.030838\n",
      "epoch: 7 [21440/60000 (36%)]\t training loss: 0.001877\n",
      "epoch: 7 [21760/60000 (36%)]\t training loss: 0.057096\n",
      "epoch: 7 [22080/60000 (37%)]\t training loss: 0.045383\n",
      "epoch: 7 [22400/60000 (37%)]\t training loss: 0.017140\n",
      "epoch: 7 [22720/60000 (38%)]\t training loss: 0.001202\n",
      "epoch: 7 [23040/60000 (38%)]\t training loss: 0.000380\n",
      "epoch: 7 [23360/60000 (39%)]\t training loss: 0.001182\n",
      "epoch: 7 [23680/60000 (39%)]\t training loss: 0.097483\n",
      "epoch: 7 [24000/60000 (40%)]\t training loss: 0.110954\n",
      "epoch: 7 [24320/60000 (41%)]\t training loss: 0.002411\n",
      "epoch: 7 [24640/60000 (41%)]\t training loss: 0.004338\n",
      "epoch: 7 [24960/60000 (42%)]\t training loss: 0.003929\n",
      "epoch: 7 [25280/60000 (42%)]\t training loss: 0.029100\n",
      "epoch: 7 [25600/60000 (43%)]\t training loss: 0.029112\n",
      "epoch: 7 [25920/60000 (43%)]\t training loss: 0.003407\n",
      "epoch: 7 [26240/60000 (44%)]\t training loss: 0.001258\n",
      "epoch: 7 [26560/60000 (44%)]\t training loss: 0.183673\n",
      "epoch: 7 [26880/60000 (45%)]\t training loss: 0.001194\n",
      "epoch: 7 [27200/60000 (45%)]\t training loss: 0.074442\n",
      "epoch: 7 [27520/60000 (46%)]\t training loss: 0.009214\n",
      "epoch: 7 [27840/60000 (46%)]\t training loss: 0.039633\n",
      "epoch: 7 [28160/60000 (47%)]\t training loss: 0.002881\n",
      "epoch: 7 [28480/60000 (47%)]\t training loss: 0.017195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 [28800/60000 (48%)]\t training loss: 0.005807\n",
      "epoch: 7 [29120/60000 (49%)]\t training loss: 0.003252\n",
      "epoch: 7 [29440/60000 (49%)]\t training loss: 0.027020\n",
      "epoch: 7 [29760/60000 (50%)]\t training loss: 0.021654\n",
      "epoch: 7 [30080/60000 (50%)]\t training loss: 0.003781\n",
      "epoch: 7 [30400/60000 (51%)]\t training loss: 0.014285\n",
      "epoch: 7 [30720/60000 (51%)]\t training loss: 0.003351\n",
      "epoch: 7 [31040/60000 (52%)]\t training loss: 0.012495\n",
      "epoch: 7 [31360/60000 (52%)]\t training loss: 0.026920\n",
      "epoch: 7 [31680/60000 (53%)]\t training loss: 0.000143\n",
      "epoch: 7 [32000/60000 (53%)]\t training loss: 0.062240\n",
      "epoch: 7 [32320/60000 (54%)]\t training loss: 0.015457\n",
      "epoch: 7 [32640/60000 (54%)]\t training loss: 0.002383\n",
      "epoch: 7 [32960/60000 (55%)]\t training loss: 0.000854\n",
      "epoch: 7 [33280/60000 (55%)]\t training loss: 0.002539\n",
      "epoch: 7 [33600/60000 (56%)]\t training loss: 0.004571\n",
      "epoch: 7 [33920/60000 (57%)]\t training loss: 0.000849\n",
      "epoch: 7 [34240/60000 (57%)]\t training loss: 0.128169\n",
      "epoch: 7 [34560/60000 (58%)]\t training loss: 0.340581\n",
      "epoch: 7 [34880/60000 (58%)]\t training loss: 0.000288\n",
      "epoch: 7 [35200/60000 (59%)]\t training loss: 0.002565\n",
      "epoch: 7 [35520/60000 (59%)]\t training loss: 0.026286\n",
      "epoch: 7 [35840/60000 (60%)]\t training loss: 0.000750\n",
      "epoch: 7 [36160/60000 (60%)]\t training loss: 0.012588\n",
      "epoch: 7 [36480/60000 (61%)]\t training loss: 0.006175\n",
      "epoch: 7 [36800/60000 (61%)]\t training loss: 0.061152\n",
      "epoch: 7 [37120/60000 (62%)]\t training loss: 0.002712\n",
      "epoch: 7 [37440/60000 (62%)]\t training loss: 0.037456\n",
      "epoch: 7 [37760/60000 (63%)]\t training loss: 0.004074\n",
      "epoch: 7 [38080/60000 (63%)]\t training loss: 0.043095\n",
      "epoch: 7 [38400/60000 (64%)]\t training loss: 0.007034\n",
      "epoch: 7 [38720/60000 (65%)]\t training loss: 0.224826\n",
      "epoch: 7 [39040/60000 (65%)]\t training loss: 0.002157\n",
      "epoch: 7 [39360/60000 (66%)]\t training loss: 0.001159\n",
      "epoch: 7 [39680/60000 (66%)]\t training loss: 0.089669\n",
      "epoch: 7 [40000/60000 (67%)]\t training loss: 0.088194\n",
      "epoch: 7 [40320/60000 (67%)]\t training loss: 0.018546\n",
      "epoch: 7 [40640/60000 (68%)]\t training loss: 0.008136\n",
      "epoch: 7 [40960/60000 (68%)]\t training loss: 0.001781\n",
      "epoch: 7 [41280/60000 (69%)]\t training loss: 0.026745\n",
      "epoch: 7 [41600/60000 (69%)]\t training loss: 0.001653\n",
      "epoch: 7 [41920/60000 (70%)]\t training loss: 0.005939\n",
      "epoch: 7 [42240/60000 (70%)]\t training loss: 0.000228\n",
      "epoch: 7 [42560/60000 (71%)]\t training loss: 0.008636\n",
      "epoch: 7 [42880/60000 (71%)]\t training loss: 0.073340\n",
      "epoch: 7 [43200/60000 (72%)]\t training loss: 0.004021\n",
      "epoch: 7 [43520/60000 (73%)]\t training loss: 0.001190\n",
      "epoch: 7 [43840/60000 (73%)]\t training loss: 0.000045\n",
      "epoch: 7 [44160/60000 (74%)]\t training loss: 0.002638\n",
      "epoch: 7 [44480/60000 (74%)]\t training loss: 0.000099\n",
      "epoch: 7 [44800/60000 (75%)]\t training loss: 0.010347\n",
      "epoch: 7 [45120/60000 (75%)]\t training loss: 0.002623\n",
      "epoch: 7 [45440/60000 (76%)]\t training loss: 0.010513\n",
      "epoch: 7 [45760/60000 (76%)]\t training loss: 0.005331\n",
      "epoch: 7 [46080/60000 (77%)]\t training loss: 0.023304\n",
      "epoch: 7 [46400/60000 (77%)]\t training loss: 0.011361\n",
      "epoch: 7 [46720/60000 (78%)]\t training loss: 0.037957\n",
      "epoch: 7 [47040/60000 (78%)]\t training loss: 0.005650\n",
      "epoch: 7 [47360/60000 (79%)]\t training loss: 0.005421\n",
      "epoch: 7 [47680/60000 (79%)]\t training loss: 0.009892\n",
      "epoch: 7 [48000/60000 (80%)]\t training loss: 0.022846\n",
      "epoch: 7 [48320/60000 (81%)]\t training loss: 0.021203\n",
      "epoch: 7 [48640/60000 (81%)]\t training loss: 0.017544\n",
      "epoch: 7 [48960/60000 (82%)]\t training loss: 0.118041\n",
      "epoch: 7 [49280/60000 (82%)]\t training loss: 0.000562\n",
      "epoch: 7 [49600/60000 (83%)]\t training loss: 0.034551\n",
      "epoch: 7 [49920/60000 (83%)]\t training loss: 0.003522\n",
      "epoch: 7 [50240/60000 (84%)]\t training loss: 0.000772\n",
      "epoch: 7 [50560/60000 (84%)]\t training loss: 0.056369\n",
      "epoch: 7 [50880/60000 (85%)]\t training loss: 0.077547\n",
      "epoch: 7 [51200/60000 (85%)]\t training loss: 0.003898\n",
      "epoch: 7 [51520/60000 (86%)]\t training loss: 0.010554\n",
      "epoch: 7 [51840/60000 (86%)]\t training loss: 0.062571\n",
      "epoch: 7 [52160/60000 (87%)]\t training loss: 0.103425\n",
      "epoch: 7 [52480/60000 (87%)]\t training loss: 0.005694\n",
      "epoch: 7 [52800/60000 (88%)]\t training loss: 0.121845\n",
      "epoch: 7 [53120/60000 (89%)]\t training loss: 0.000977\n",
      "epoch: 7 [53440/60000 (89%)]\t training loss: 0.002033\n",
      "epoch: 7 [53760/60000 (90%)]\t training loss: 0.423034\n",
      "epoch: 7 [54080/60000 (90%)]\t training loss: 0.023660\n",
      "epoch: 7 [54400/60000 (91%)]\t training loss: 0.000436\n",
      "epoch: 7 [54720/60000 (91%)]\t training loss: 0.008080\n",
      "epoch: 7 [55040/60000 (92%)]\t training loss: 0.008321\n",
      "epoch: 7 [55360/60000 (92%)]\t training loss: 0.025609\n",
      "epoch: 7 [55680/60000 (93%)]\t training loss: 0.009255\n",
      "epoch: 7 [56000/60000 (93%)]\t training loss: 0.003358\n",
      "epoch: 7 [56320/60000 (94%)]\t training loss: 0.134175\n",
      "epoch: 7 [56640/60000 (94%)]\t training loss: 0.001619\n",
      "epoch: 7 [56960/60000 (95%)]\t training loss: 0.036239\n",
      "epoch: 7 [57280/60000 (95%)]\t training loss: 0.001852\n",
      "epoch: 7 [57600/60000 (96%)]\t training loss: 0.010256\n",
      "epoch: 7 [57920/60000 (97%)]\t training loss: 0.003691\n",
      "epoch: 7 [58240/60000 (97%)]\t training loss: 0.081451\n",
      "epoch: 7 [58560/60000 (98%)]\t training loss: 0.007228\n",
      "epoch: 7 [58880/60000 (98%)]\t training loss: 0.059955\n",
      "epoch: 7 [59200/60000 (99%)]\t training loss: 0.038952\n",
      "epoch: 7 [59520/60000 (99%)]\t training loss: 0.090430\n",
      "epoch: 7 [59840/60000 (100%)]\t training loss: 0.019527\n",
      "\n",
      "Test dataset: Overall Loss: 0.0330, Overall Accuracy: 9906/10000 (99%)\n",
      "\n",
      "epoch: 8 [0/60000 (0%)]\t training loss: 0.002543\n",
      "epoch: 8 [320/60000 (1%)]\t training loss: 0.005722\n",
      "epoch: 8 [640/60000 (1%)]\t training loss: 0.135988\n",
      "epoch: 8 [960/60000 (2%)]\t training loss: 0.072058\n",
      "epoch: 8 [1280/60000 (2%)]\t training loss: 0.003805\n",
      "epoch: 8 [1600/60000 (3%)]\t training loss: 0.000430\n",
      "epoch: 8 [1920/60000 (3%)]\t training loss: 0.000730\n",
      "epoch: 8 [2240/60000 (4%)]\t training loss: 0.002144\n",
      "epoch: 8 [2560/60000 (4%)]\t training loss: 0.002297\n",
      "epoch: 8 [2880/60000 (5%)]\t training loss: 0.040665\n",
      "epoch: 8 [3200/60000 (5%)]\t training loss: 0.006168\n",
      "epoch: 8 [3520/60000 (6%)]\t training loss: 0.023275\n",
      "epoch: 8 [3840/60000 (6%)]\t training loss: 0.026238\n",
      "epoch: 8 [4160/60000 (7%)]\t training loss: 0.000123\n",
      "epoch: 8 [4480/60000 (7%)]\t training loss: 0.001436\n",
      "epoch: 8 [4800/60000 (8%)]\t training loss: 0.021916\n",
      "epoch: 8 [5120/60000 (9%)]\t training loss: 0.002259\n",
      "epoch: 8 [5440/60000 (9%)]\t training loss: 0.011543\n",
      "epoch: 8 [5760/60000 (10%)]\t training loss: 0.000731\n",
      "epoch: 8 [6080/60000 (10%)]\t training loss: 0.041035\n",
      "epoch: 8 [6400/60000 (11%)]\t training loss: 0.002250\n",
      "epoch: 8 [6720/60000 (11%)]\t training loss: 0.002797\n",
      "epoch: 8 [7040/60000 (12%)]\t training loss: 0.010164\n",
      "epoch: 8 [7360/60000 (12%)]\t training loss: 0.005457\n",
      "epoch: 8 [7680/60000 (13%)]\t training loss: 0.116831\n",
      "epoch: 8 [8000/60000 (13%)]\t training loss: 0.000417\n",
      "epoch: 8 [8320/60000 (14%)]\t training loss: 0.027673\n",
      "epoch: 8 [8640/60000 (14%)]\t training loss: 0.004355\n",
      "epoch: 8 [8960/60000 (15%)]\t training loss: 0.010629\n",
      "epoch: 8 [9280/60000 (15%)]\t training loss: 0.015114\n",
      "epoch: 8 [9600/60000 (16%)]\t training loss: 0.038981\n",
      "epoch: 8 [9920/60000 (17%)]\t training loss: 0.001907\n",
      "epoch: 8 [10240/60000 (17%)]\t training loss: 0.006488\n",
      "epoch: 8 [10560/60000 (18%)]\t training loss: 0.187408\n",
      "epoch: 8 [10880/60000 (18%)]\t training loss: 0.000368\n",
      "epoch: 8 [11200/60000 (19%)]\t training loss: 0.000571\n",
      "epoch: 8 [11520/60000 (19%)]\t training loss: 0.002071\n",
      "epoch: 8 [11840/60000 (20%)]\t training loss: 0.002586\n",
      "epoch: 8 [12160/60000 (20%)]\t training loss: 0.006465\n",
      "epoch: 8 [12480/60000 (21%)]\t training loss: 0.007091\n",
      "epoch: 8 [12800/60000 (21%)]\t training loss: 0.002342\n",
      "epoch: 8 [13120/60000 (22%)]\t training loss: 0.004324\n",
      "epoch: 8 [13440/60000 (22%)]\t training loss: 0.001410\n",
      "epoch: 8 [13760/60000 (23%)]\t training loss: 0.034187\n",
      "epoch: 8 [14080/60000 (23%)]\t training loss: 0.001208\n",
      "epoch: 8 [14400/60000 (24%)]\t training loss: 0.152645\n",
      "epoch: 8 [14720/60000 (25%)]\t training loss: 0.001177\n",
      "epoch: 8 [15040/60000 (25%)]\t training loss: 0.023506\n",
      "epoch: 8 [15360/60000 (26%)]\t training loss: 0.006010\n",
      "epoch: 8 [15680/60000 (26%)]\t training loss: 0.061704\n",
      "epoch: 8 [16000/60000 (27%)]\t training loss: 0.002083\n",
      "epoch: 8 [16320/60000 (27%)]\t training loss: 0.086387\n",
      "epoch: 8 [16640/60000 (28%)]\t training loss: 0.000300\n",
      "epoch: 8 [16960/60000 (28%)]\t training loss: 0.030156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 [17280/60000 (29%)]\t training loss: 0.000097\n",
      "epoch: 8 [17600/60000 (29%)]\t training loss: 0.063902\n",
      "epoch: 8 [17920/60000 (30%)]\t training loss: 0.002406\n",
      "epoch: 8 [18240/60000 (30%)]\t training loss: 0.011788\n",
      "epoch: 8 [18560/60000 (31%)]\t training loss: 0.000605\n",
      "epoch: 8 [18880/60000 (31%)]\t training loss: 0.000376\n",
      "epoch: 8 [19200/60000 (32%)]\t training loss: 0.086203\n",
      "epoch: 8 [19520/60000 (33%)]\t training loss: 0.027951\n",
      "epoch: 8 [19840/60000 (33%)]\t training loss: 0.006056\n",
      "epoch: 8 [20160/60000 (34%)]\t training loss: 0.003202\n",
      "epoch: 8 [20480/60000 (34%)]\t training loss: 0.097283\n",
      "epoch: 8 [20800/60000 (35%)]\t training loss: 0.000174\n",
      "epoch: 8 [21120/60000 (35%)]\t training loss: 0.002739\n",
      "epoch: 8 [21440/60000 (36%)]\t training loss: 0.000307\n",
      "epoch: 8 [21760/60000 (36%)]\t training loss: 0.020901\n",
      "epoch: 8 [22080/60000 (37%)]\t training loss: 0.008474\n",
      "epoch: 8 [22400/60000 (37%)]\t training loss: 0.004543\n",
      "epoch: 8 [22720/60000 (38%)]\t training loss: 0.011096\n",
      "epoch: 8 [23040/60000 (38%)]\t training loss: 0.007308\n",
      "epoch: 8 [23360/60000 (39%)]\t training loss: 0.000015\n",
      "epoch: 8 [23680/60000 (39%)]\t training loss: 0.104049\n",
      "epoch: 8 [24000/60000 (40%)]\t training loss: 0.201820\n",
      "epoch: 8 [24320/60000 (41%)]\t training loss: 0.000170\n",
      "epoch: 8 [24640/60000 (41%)]\t training loss: 0.001519\n",
      "epoch: 8 [24960/60000 (42%)]\t training loss: 0.000209\n",
      "epoch: 8 [25280/60000 (42%)]\t training loss: 0.000888\n",
      "epoch: 8 [25600/60000 (43%)]\t training loss: 0.046530\n",
      "epoch: 8 [25920/60000 (43%)]\t training loss: 0.000473\n",
      "epoch: 8 [26240/60000 (44%)]\t training loss: 0.317141\n",
      "epoch: 8 [26560/60000 (44%)]\t training loss: 0.004907\n",
      "epoch: 8 [26880/60000 (45%)]\t training loss: 0.000458\n",
      "epoch: 8 [27200/60000 (45%)]\t training loss: 0.018104\n",
      "epoch: 8 [27520/60000 (46%)]\t training loss: 0.061320\n",
      "epoch: 8 [27840/60000 (46%)]\t training loss: 0.414538\n",
      "epoch: 8 [28160/60000 (47%)]\t training loss: 0.016383\n",
      "epoch: 8 [28480/60000 (47%)]\t training loss: 0.003727\n",
      "epoch: 8 [28800/60000 (48%)]\t training loss: 0.010406\n",
      "epoch: 8 [29120/60000 (49%)]\t training loss: 0.025264\n",
      "epoch: 8 [29440/60000 (49%)]\t training loss: 0.005768\n",
      "epoch: 8 [29760/60000 (50%)]\t training loss: 0.016282\n",
      "epoch: 8 [30080/60000 (50%)]\t training loss: 0.029589\n",
      "epoch: 8 [30400/60000 (51%)]\t training loss: 0.006908\n",
      "epoch: 8 [30720/60000 (51%)]\t training loss: 0.002353\n",
      "epoch: 8 [31040/60000 (52%)]\t training loss: 0.035855\n",
      "epoch: 8 [31360/60000 (52%)]\t training loss: 0.066684\n",
      "epoch: 8 [31680/60000 (53%)]\t training loss: 0.000253\n",
      "epoch: 8 [32000/60000 (53%)]\t training loss: 0.005680\n",
      "epoch: 8 [32320/60000 (54%)]\t training loss: 0.000635\n",
      "epoch: 8 [32640/60000 (54%)]\t training loss: 0.083735\n",
      "epoch: 8 [32960/60000 (55%)]\t training loss: 0.034724\n",
      "epoch: 8 [33280/60000 (55%)]\t training loss: 0.002790\n",
      "epoch: 8 [33600/60000 (56%)]\t training loss: 0.032143\n",
      "epoch: 8 [33920/60000 (57%)]\t training loss: 0.002642\n",
      "epoch: 8 [34240/60000 (57%)]\t training loss: 0.007595\n",
      "epoch: 8 [34560/60000 (58%)]\t training loss: 0.031221\n",
      "epoch: 8 [34880/60000 (58%)]\t training loss: 0.120842\n",
      "epoch: 8 [35200/60000 (59%)]\t training loss: 0.002743\n",
      "epoch: 8 [35520/60000 (59%)]\t training loss: 0.000411\n",
      "epoch: 8 [35840/60000 (60%)]\t training loss: 0.025379\n",
      "epoch: 8 [36160/60000 (60%)]\t training loss: 0.002692\n",
      "epoch: 8 [36480/60000 (61%)]\t training loss: 0.065416\n",
      "epoch: 8 [36800/60000 (61%)]\t training loss: 0.070927\n",
      "epoch: 8 [37120/60000 (62%)]\t training loss: 0.525198\n",
      "epoch: 8 [37440/60000 (62%)]\t training loss: 0.138375\n",
      "epoch: 8 [37760/60000 (63%)]\t training loss: 0.031405\n",
      "epoch: 8 [38080/60000 (63%)]\t training loss: 0.011610\n",
      "epoch: 8 [38400/60000 (64%)]\t training loss: 0.092762\n",
      "epoch: 8 [38720/60000 (65%)]\t training loss: 0.012089\n",
      "epoch: 8 [39040/60000 (65%)]\t training loss: 0.154722\n",
      "epoch: 8 [39360/60000 (66%)]\t training loss: 0.012750\n",
      "epoch: 8 [39680/60000 (66%)]\t training loss: 0.015967\n",
      "epoch: 8 [40000/60000 (67%)]\t training loss: 0.005050\n",
      "epoch: 8 [40320/60000 (67%)]\t training loss: 0.001378\n",
      "epoch: 8 [40640/60000 (68%)]\t training loss: 0.007945\n",
      "epoch: 8 [40960/60000 (68%)]\t training loss: 0.010188\n",
      "epoch: 8 [41280/60000 (69%)]\t training loss: 0.021730\n",
      "epoch: 8 [41600/60000 (69%)]\t training loss: 0.005126\n",
      "epoch: 8 [41920/60000 (70%)]\t training loss: 0.002372\n",
      "epoch: 8 [42240/60000 (70%)]\t training loss: 0.000489\n",
      "epoch: 8 [42560/60000 (71%)]\t training loss: 0.000808\n",
      "epoch: 8 [42880/60000 (71%)]\t training loss: 0.005269\n",
      "epoch: 8 [43200/60000 (72%)]\t training loss: 0.000153\n",
      "epoch: 8 [43520/60000 (73%)]\t training loss: 0.005530\n",
      "epoch: 8 [43840/60000 (73%)]\t training loss: 0.000848\n",
      "epoch: 8 [44160/60000 (74%)]\t training loss: 0.020858\n",
      "epoch: 8 [44480/60000 (74%)]\t training loss: 0.063677\n",
      "epoch: 8 [44800/60000 (75%)]\t training loss: 0.018958\n",
      "epoch: 8 [45120/60000 (75%)]\t training loss: 0.005418\n",
      "epoch: 8 [45440/60000 (76%)]\t training loss: 0.027774\n",
      "epoch: 8 [45760/60000 (76%)]\t training loss: 0.000082\n",
      "epoch: 8 [46080/60000 (77%)]\t training loss: 0.012417\n",
      "epoch: 8 [46400/60000 (77%)]\t training loss: 0.000198\n",
      "epoch: 8 [46720/60000 (78%)]\t training loss: 0.001034\n",
      "epoch: 8 [47040/60000 (78%)]\t training loss: 0.000027\n",
      "epoch: 8 [47360/60000 (79%)]\t training loss: 0.001138\n",
      "epoch: 8 [47680/60000 (79%)]\t training loss: 0.000189\n",
      "epoch: 8 [48000/60000 (80%)]\t training loss: 0.010205\n",
      "epoch: 8 [48320/60000 (81%)]\t training loss: 0.003272\n",
      "epoch: 8 [48640/60000 (81%)]\t training loss: 0.009331\n",
      "epoch: 8 [48960/60000 (82%)]\t training loss: 0.015739\n",
      "epoch: 8 [49280/60000 (82%)]\t training loss: 0.012418\n",
      "epoch: 8 [49600/60000 (83%)]\t training loss: 0.126714\n",
      "epoch: 8 [49920/60000 (83%)]\t training loss: 0.008230\n",
      "epoch: 8 [50240/60000 (84%)]\t training loss: 0.004915\n",
      "epoch: 8 [50560/60000 (84%)]\t training loss: 0.011502\n",
      "epoch: 8 [50880/60000 (85%)]\t training loss: 0.041248\n",
      "epoch: 8 [51200/60000 (85%)]\t training loss: 0.008898\n",
      "epoch: 8 [51520/60000 (86%)]\t training loss: 0.017665\n",
      "epoch: 8 [51840/60000 (86%)]\t training loss: 0.006707\n",
      "epoch: 8 [52160/60000 (87%)]\t training loss: 0.004434\n",
      "epoch: 8 [52480/60000 (87%)]\t training loss: 0.094430\n",
      "epoch: 8 [52800/60000 (88%)]\t training loss: 0.000976\n",
      "epoch: 8 [53120/60000 (89%)]\t training loss: 0.019608\n",
      "epoch: 8 [53440/60000 (89%)]\t training loss: 0.002413\n",
      "epoch: 8 [53760/60000 (90%)]\t training loss: 0.017046\n",
      "epoch: 8 [54080/60000 (90%)]\t training loss: 0.023373\n",
      "epoch: 8 [54400/60000 (91%)]\t training loss: 0.013972\n",
      "epoch: 8 [54720/60000 (91%)]\t training loss: 0.105024\n",
      "epoch: 8 [55040/60000 (92%)]\t training loss: 0.094344\n",
      "epoch: 8 [55360/60000 (92%)]\t training loss: 0.000258\n",
      "epoch: 8 [55680/60000 (93%)]\t training loss: 0.015421\n",
      "epoch: 8 [56000/60000 (93%)]\t training loss: 0.076110\n",
      "epoch: 8 [56320/60000 (94%)]\t training loss: 0.037401\n",
      "epoch: 8 [56640/60000 (94%)]\t training loss: 0.023429\n",
      "epoch: 8 [56960/60000 (95%)]\t training loss: 0.004524\n",
      "epoch: 8 [57280/60000 (95%)]\t training loss: 0.027435\n",
      "epoch: 8 [57600/60000 (96%)]\t training loss: 0.074070\n",
      "epoch: 8 [57920/60000 (97%)]\t training loss: 0.227144\n",
      "epoch: 8 [58240/60000 (97%)]\t training loss: 0.021663\n",
      "epoch: 8 [58560/60000 (98%)]\t training loss: 0.088318\n",
      "epoch: 8 [58880/60000 (98%)]\t training loss: 0.241900\n",
      "epoch: 8 [59200/60000 (99%)]\t training loss: 0.026931\n",
      "epoch: 8 [59520/60000 (99%)]\t training loss: 0.026172\n",
      "epoch: 8 [59840/60000 (100%)]\t training loss: 0.001792\n",
      "\n",
      "Test dataset: Overall Loss: 0.0389, Overall Accuracy: 9888/10000 (99%)\n",
      "\n",
      "epoch: 9 [0/60000 (0%)]\t training loss: 0.001644\n",
      "epoch: 9 [320/60000 (1%)]\t training loss: 0.063786\n",
      "epoch: 9 [640/60000 (1%)]\t training loss: 0.002865\n",
      "epoch: 9 [960/60000 (2%)]\t training loss: 0.002144\n",
      "epoch: 9 [1280/60000 (2%)]\t training loss: 0.008681\n",
      "epoch: 9 [1600/60000 (3%)]\t training loss: 0.036286\n",
      "epoch: 9 [1920/60000 (3%)]\t training loss: 0.006652\n",
      "epoch: 9 [2240/60000 (4%)]\t training loss: 0.000449\n",
      "epoch: 9 [2560/60000 (4%)]\t training loss: 0.062722\n",
      "epoch: 9 [2880/60000 (5%)]\t training loss: 0.000037\n",
      "epoch: 9 [3200/60000 (5%)]\t training loss: 0.012131\n",
      "epoch: 9 [3520/60000 (6%)]\t training loss: 0.009947\n",
      "epoch: 9 [3840/60000 (6%)]\t training loss: 0.001681\n",
      "epoch: 9 [4160/60000 (7%)]\t training loss: 0.014441\n",
      "epoch: 9 [4480/60000 (7%)]\t training loss: 0.000872\n",
      "epoch: 9 [4800/60000 (8%)]\t training loss: 0.003755\n",
      "epoch: 9 [5120/60000 (9%)]\t training loss: 0.001493\n",
      "epoch: 9 [5440/60000 (9%)]\t training loss: 0.186955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 [5760/60000 (10%)]\t training loss: 0.006193\n",
      "epoch: 9 [6080/60000 (10%)]\t training loss: 0.003409\n",
      "epoch: 9 [6400/60000 (11%)]\t training loss: 0.080023\n",
      "epoch: 9 [6720/60000 (11%)]\t training loss: 0.000781\n",
      "epoch: 9 [7040/60000 (12%)]\t training loss: 0.034253\n",
      "epoch: 9 [7360/60000 (12%)]\t training loss: 0.145042\n",
      "epoch: 9 [7680/60000 (13%)]\t training loss: 0.002235\n",
      "epoch: 9 [8000/60000 (13%)]\t training loss: 0.022299\n",
      "epoch: 9 [8320/60000 (14%)]\t training loss: 0.014746\n",
      "epoch: 9 [8640/60000 (14%)]\t training loss: 0.006864\n",
      "epoch: 9 [8960/60000 (15%)]\t training loss: 0.019474\n",
      "epoch: 9 [9280/60000 (15%)]\t training loss: 0.001319\n",
      "epoch: 9 [9600/60000 (16%)]\t training loss: 0.057379\n",
      "epoch: 9 [9920/60000 (17%)]\t training loss: 0.004524\n",
      "epoch: 9 [10240/60000 (17%)]\t training loss: 0.002960\n",
      "epoch: 9 [10560/60000 (18%)]\t training loss: 0.000753\n",
      "epoch: 9 [10880/60000 (18%)]\t training loss: 0.001434\n",
      "epoch: 9 [11200/60000 (19%)]\t training loss: 0.169119\n",
      "epoch: 9 [11520/60000 (19%)]\t training loss: 0.000328\n",
      "epoch: 9 [11840/60000 (20%)]\t training loss: 0.261084\n",
      "epoch: 9 [12160/60000 (20%)]\t training loss: 0.005853\n",
      "epoch: 9 [12480/60000 (21%)]\t training loss: 0.014572\n",
      "epoch: 9 [12800/60000 (21%)]\t training loss: 0.000909\n",
      "epoch: 9 [13120/60000 (22%)]\t training loss: 0.000134\n",
      "epoch: 9 [13440/60000 (22%)]\t training loss: 0.004721\n",
      "epoch: 9 [13760/60000 (23%)]\t training loss: 0.003045\n",
      "epoch: 9 [14080/60000 (23%)]\t training loss: 0.206199\n",
      "epoch: 9 [14400/60000 (24%)]\t training loss: 0.001059\n",
      "epoch: 9 [14720/60000 (25%)]\t training loss: 0.195800\n",
      "epoch: 9 [15040/60000 (25%)]\t training loss: 0.056919\n",
      "epoch: 9 [15360/60000 (26%)]\t training loss: 0.000042\n",
      "epoch: 9 [15680/60000 (26%)]\t training loss: 0.025149\n",
      "epoch: 9 [16000/60000 (27%)]\t training loss: 0.012528\n",
      "epoch: 9 [16320/60000 (27%)]\t training loss: 0.000316\n",
      "epoch: 9 [16640/60000 (28%)]\t training loss: 0.008703\n",
      "epoch: 9 [16960/60000 (28%)]\t training loss: 0.095939\n",
      "epoch: 9 [17280/60000 (29%)]\t training loss: 0.116292\n",
      "epoch: 9 [17600/60000 (29%)]\t training loss: 0.093551\n",
      "epoch: 9 [17920/60000 (30%)]\t training loss: 0.035829\n",
      "epoch: 9 [18240/60000 (30%)]\t training loss: 0.001572\n",
      "epoch: 9 [18560/60000 (31%)]\t training loss: 0.037043\n",
      "epoch: 9 [18880/60000 (31%)]\t training loss: 0.005541\n",
      "epoch: 9 [19200/60000 (32%)]\t training loss: 0.001733\n",
      "epoch: 9 [19520/60000 (33%)]\t training loss: 0.006489\n",
      "epoch: 9 [19840/60000 (33%)]\t training loss: 0.005329\n",
      "epoch: 9 [20160/60000 (34%)]\t training loss: 0.000110\n",
      "epoch: 9 [20480/60000 (34%)]\t training loss: 0.042445\n",
      "epoch: 9 [20800/60000 (35%)]\t training loss: 0.027483\n",
      "epoch: 9 [21120/60000 (35%)]\t training loss: 0.002533\n",
      "epoch: 9 [21440/60000 (36%)]\t training loss: 0.028220\n",
      "epoch: 9 [21760/60000 (36%)]\t training loss: 0.109060\n",
      "epoch: 9 [22080/60000 (37%)]\t training loss: 0.001577\n",
      "epoch: 9 [22400/60000 (37%)]\t training loss: 0.382640\n",
      "epoch: 9 [22720/60000 (38%)]\t training loss: 0.007498\n",
      "epoch: 9 [23040/60000 (38%)]\t training loss: 0.006341\n",
      "epoch: 9 [23360/60000 (39%)]\t training loss: 0.005835\n",
      "epoch: 9 [23680/60000 (39%)]\t training loss: 0.171468\n",
      "epoch: 9 [24000/60000 (40%)]\t training loss: 0.018647\n",
      "epoch: 9 [24320/60000 (41%)]\t training loss: 0.052601\n",
      "epoch: 9 [24640/60000 (41%)]\t training loss: 0.003150\n",
      "epoch: 9 [24960/60000 (42%)]\t training loss: 0.008893\n",
      "epoch: 9 [25280/60000 (42%)]\t training loss: 0.028718\n",
      "epoch: 9 [25600/60000 (43%)]\t training loss: 0.000390\n",
      "epoch: 9 [25920/60000 (43%)]\t training loss: 0.040003\n",
      "epoch: 9 [26240/60000 (44%)]\t training loss: 0.003741\n",
      "epoch: 9 [26560/60000 (44%)]\t training loss: 0.339681\n",
      "epoch: 9 [26880/60000 (45%)]\t training loss: 0.001976\n",
      "epoch: 9 [27200/60000 (45%)]\t training loss: 0.120759\n",
      "epoch: 9 [27520/60000 (46%)]\t training loss: 0.032014\n",
      "epoch: 9 [27840/60000 (46%)]\t training loss: 0.026110\n",
      "epoch: 9 [28160/60000 (47%)]\t training loss: 0.000819\n",
      "epoch: 9 [28480/60000 (47%)]\t training loss: 0.152492\n",
      "epoch: 9 [28800/60000 (48%)]\t training loss: 0.513510\n",
      "epoch: 9 [29120/60000 (49%)]\t training loss: 0.067364\n",
      "epoch: 9 [29440/60000 (49%)]\t training loss: 0.000679\n",
      "epoch: 9 [29760/60000 (50%)]\t training loss: 0.018276\n",
      "epoch: 9 [30080/60000 (50%)]\t training loss: 0.001643\n",
      "epoch: 9 [30400/60000 (51%)]\t training loss: 0.077645\n",
      "epoch: 9 [30720/60000 (51%)]\t training loss: 0.004251\n",
      "epoch: 9 [31040/60000 (52%)]\t training loss: 0.007524\n",
      "epoch: 9 [31360/60000 (52%)]\t training loss: 0.027899\n",
      "epoch: 9 [31680/60000 (53%)]\t training loss: 0.000078\n",
      "epoch: 9 [32000/60000 (53%)]\t training loss: 0.069721\n",
      "epoch: 9 [32320/60000 (54%)]\t training loss: 0.042295\n",
      "epoch: 9 [32640/60000 (54%)]\t training loss: 0.001021\n",
      "epoch: 9 [32960/60000 (55%)]\t training loss: 0.001437\n",
      "epoch: 9 [33280/60000 (55%)]\t training loss: 0.000414\n",
      "epoch: 9 [33600/60000 (56%)]\t training loss: 0.157665\n",
      "epoch: 9 [33920/60000 (57%)]\t training loss: 0.014448\n",
      "epoch: 9 [34240/60000 (57%)]\t training loss: 0.123623\n",
      "epoch: 9 [34560/60000 (58%)]\t training loss: 0.146846\n",
      "epoch: 9 [34880/60000 (58%)]\t training loss: 0.000562\n",
      "epoch: 9 [35200/60000 (59%)]\t training loss: 0.022359\n",
      "epoch: 9 [35520/60000 (59%)]\t training loss: 0.015815\n",
      "epoch: 9 [35840/60000 (60%)]\t training loss: 0.004997\n",
      "epoch: 9 [36160/60000 (60%)]\t training loss: 0.006318\n",
      "epoch: 9 [36480/60000 (61%)]\t training loss: 0.134763\n",
      "epoch: 9 [36800/60000 (61%)]\t training loss: 0.049667\n",
      "epoch: 9 [37120/60000 (62%)]\t training loss: 0.002268\n",
      "epoch: 9 [37440/60000 (62%)]\t training loss: 0.002765\n",
      "epoch: 9 [37760/60000 (63%)]\t training loss: 0.006073\n",
      "epoch: 9 [38080/60000 (63%)]\t training loss: 0.007258\n",
      "epoch: 9 [38400/60000 (64%)]\t training loss: 0.001006\n",
      "epoch: 9 [38720/60000 (65%)]\t training loss: 0.159652\n",
      "epoch: 9 [39040/60000 (65%)]\t training loss: 0.002775\n",
      "epoch: 9 [39360/60000 (66%)]\t training loss: 0.012017\n",
      "epoch: 9 [39680/60000 (66%)]\t training loss: 0.066918\n",
      "epoch: 9 [40000/60000 (67%)]\t training loss: 0.005386\n",
      "epoch: 9 [40320/60000 (67%)]\t training loss: 0.028949\n",
      "epoch: 9 [40640/60000 (68%)]\t training loss: 0.006221\n",
      "epoch: 9 [40960/60000 (68%)]\t training loss: 0.005633\n",
      "epoch: 9 [41280/60000 (69%)]\t training loss: 0.023049\n",
      "epoch: 9 [41600/60000 (69%)]\t training loss: 0.001218\n",
      "epoch: 9 [41920/60000 (70%)]\t training loss: 0.010091\n",
      "epoch: 9 [42240/60000 (70%)]\t training loss: 0.000746\n",
      "epoch: 9 [42560/60000 (71%)]\t training loss: 0.077366\n",
      "epoch: 9 [42880/60000 (71%)]\t training loss: 0.070038\n",
      "epoch: 9 [43200/60000 (72%)]\t training loss: 0.000351\n",
      "epoch: 9 [43520/60000 (73%)]\t training loss: 0.000080\n",
      "epoch: 9 [43840/60000 (73%)]\t training loss: 0.001875\n",
      "epoch: 9 [44160/60000 (74%)]\t training loss: 0.001270\n",
      "epoch: 9 [44480/60000 (74%)]\t training loss: 0.000488\n",
      "epoch: 9 [44800/60000 (75%)]\t training loss: 0.004371\n",
      "epoch: 9 [45120/60000 (75%)]\t training loss: 0.044954\n",
      "epoch: 9 [45440/60000 (76%)]\t training loss: 0.006248\n",
      "epoch: 9 [45760/60000 (76%)]\t training loss: 0.003387\n",
      "epoch: 9 [46080/60000 (77%)]\t training loss: 0.182069\n",
      "epoch: 9 [46400/60000 (77%)]\t training loss: 0.090457\n",
      "epoch: 9 [46720/60000 (78%)]\t training loss: 0.032938\n",
      "epoch: 9 [47040/60000 (78%)]\t training loss: 0.001383\n",
      "epoch: 9 [47360/60000 (79%)]\t training loss: 0.003136\n",
      "epoch: 9 [47680/60000 (79%)]\t training loss: 0.011296\n",
      "epoch: 9 [48000/60000 (80%)]\t training loss: 0.036284\n",
      "epoch: 9 [48320/60000 (81%)]\t training loss: 0.013459\n",
      "epoch: 9 [48640/60000 (81%)]\t training loss: 0.000494\n",
      "epoch: 9 [48960/60000 (82%)]\t training loss: 0.002985\n",
      "epoch: 9 [49280/60000 (82%)]\t training loss: 0.001693\n",
      "epoch: 9 [49600/60000 (83%)]\t training loss: 0.003381\n",
      "epoch: 9 [49920/60000 (83%)]\t training loss: 0.047046\n",
      "epoch: 9 [50240/60000 (84%)]\t training loss: 0.034790\n",
      "epoch: 9 [50560/60000 (84%)]\t training loss: 0.006742\n",
      "epoch: 9 [50880/60000 (85%)]\t training loss: 0.000282\n",
      "epoch: 9 [51200/60000 (85%)]\t training loss: 0.000038\n",
      "epoch: 9 [51520/60000 (86%)]\t training loss: 0.000080\n",
      "epoch: 9 [51840/60000 (86%)]\t training loss: 0.003084\n",
      "epoch: 9 [52160/60000 (87%)]\t training loss: 0.001232\n",
      "epoch: 9 [52480/60000 (87%)]\t training loss: 0.002103\n",
      "epoch: 9 [52800/60000 (88%)]\t training loss: 0.029389\n",
      "epoch: 9 [53120/60000 (89%)]\t training loss: 0.003876\n",
      "epoch: 9 [53440/60000 (89%)]\t training loss: 0.006951\n",
      "epoch: 9 [53760/60000 (90%)]\t training loss: 0.001659\n",
      "epoch: 9 [54080/60000 (90%)]\t training loss: 0.014693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 [54400/60000 (91%)]\t training loss: 0.007623\n",
      "epoch: 9 [54720/60000 (91%)]\t training loss: 0.003892\n",
      "epoch: 9 [55040/60000 (92%)]\t training loss: 0.017158\n",
      "epoch: 9 [55360/60000 (92%)]\t training loss: 0.001837\n",
      "epoch: 9 [55680/60000 (93%)]\t training loss: 0.019947\n",
      "epoch: 9 [56000/60000 (93%)]\t training loss: 0.000730\n",
      "epoch: 9 [56320/60000 (94%)]\t training loss: 0.001058\n",
      "epoch: 9 [56640/60000 (94%)]\t training loss: 0.014184\n",
      "epoch: 9 [56960/60000 (95%)]\t training loss: 0.012189\n",
      "epoch: 9 [57280/60000 (95%)]\t training loss: 0.008740\n",
      "epoch: 9 [57600/60000 (96%)]\t training loss: 0.015054\n",
      "epoch: 9 [57920/60000 (97%)]\t training loss: 0.004392\n",
      "epoch: 9 [58240/60000 (97%)]\t training loss: 0.009528\n",
      "epoch: 9 [58560/60000 (98%)]\t training loss: 0.055119\n",
      "epoch: 9 [58880/60000 (98%)]\t training loss: 0.009891\n",
      "epoch: 9 [59200/60000 (99%)]\t training loss: 0.003818\n",
      "epoch: 9 [59520/60000 (99%)]\t training loss: 0.004166\n",
      "epoch: 9 [59840/60000 (100%)]\t training loss: 0.178950\n",
      "\n",
      "Test dataset: Overall Loss: 0.0352, Overall Accuracy: 9901/10000 (99%)\n",
      "\n",
      "epoch: 10 [0/60000 (0%)]\t training loss: 0.000179\n",
      "epoch: 10 [320/60000 (1%)]\t training loss: 0.001350\n",
      "epoch: 10 [640/60000 (1%)]\t training loss: 0.008197\n",
      "epoch: 10 [960/60000 (2%)]\t training loss: 0.000254\n",
      "epoch: 10 [1280/60000 (2%)]\t training loss: 0.004943\n",
      "epoch: 10 [1600/60000 (3%)]\t training loss: 0.000158\n",
      "epoch: 10 [1920/60000 (3%)]\t training loss: 0.004809\n",
      "epoch: 10 [2240/60000 (4%)]\t training loss: 0.004961\n",
      "epoch: 10 [2560/60000 (4%)]\t training loss: 0.016695\n",
      "epoch: 10 [2880/60000 (5%)]\t training loss: 0.027163\n",
      "epoch: 10 [3200/60000 (5%)]\t training loss: 0.000016\n",
      "epoch: 10 [3520/60000 (6%)]\t training loss: 0.000076\n",
      "epoch: 10 [3840/60000 (6%)]\t training loss: 0.002377\n",
      "epoch: 10 [4160/60000 (7%)]\t training loss: 0.047552\n",
      "epoch: 10 [4480/60000 (7%)]\t training loss: 0.000235\n",
      "epoch: 10 [4800/60000 (8%)]\t training loss: 0.040134\n",
      "epoch: 10 [5120/60000 (9%)]\t training loss: 0.190324\n",
      "epoch: 10 [5440/60000 (9%)]\t training loss: 0.000488\n",
      "epoch: 10 [5760/60000 (10%)]\t training loss: 0.000455\n",
      "epoch: 10 [6080/60000 (10%)]\t training loss: 0.497948\n",
      "epoch: 10 [6400/60000 (11%)]\t training loss: 0.008751\n",
      "epoch: 10 [6720/60000 (11%)]\t training loss: 0.002485\n",
      "epoch: 10 [7040/60000 (12%)]\t training loss: 0.003948\n",
      "epoch: 10 [7360/60000 (12%)]\t training loss: 0.000301\n",
      "epoch: 10 [7680/60000 (13%)]\t training loss: 0.000184\n",
      "epoch: 10 [8000/60000 (13%)]\t training loss: 0.000470\n",
      "epoch: 10 [8320/60000 (14%)]\t training loss: 0.000404\n",
      "epoch: 10 [8640/60000 (14%)]\t training loss: 0.000062\n",
      "epoch: 10 [8960/60000 (15%)]\t training loss: 0.003768\n",
      "epoch: 10 [9280/60000 (15%)]\t training loss: 0.000253\n",
      "epoch: 10 [9600/60000 (16%)]\t training loss: 0.000940\n",
      "epoch: 10 [9920/60000 (17%)]\t training loss: 0.016149\n",
      "epoch: 10 [10240/60000 (17%)]\t training loss: 0.005386\n",
      "epoch: 10 [10560/60000 (18%)]\t training loss: 0.006147\n",
      "epoch: 10 [10880/60000 (18%)]\t training loss: 0.008812\n",
      "epoch: 10 [11200/60000 (19%)]\t training loss: 0.007605\n",
      "epoch: 10 [11520/60000 (19%)]\t training loss: 0.100856\n",
      "epoch: 10 [11840/60000 (20%)]\t training loss: 0.023882\n",
      "epoch: 10 [12160/60000 (20%)]\t training loss: 0.009731\n",
      "epoch: 10 [12480/60000 (21%)]\t training loss: 0.016510\n",
      "epoch: 10 [12800/60000 (21%)]\t training loss: 0.067892\n",
      "epoch: 10 [13120/60000 (22%)]\t training loss: 0.009503\n",
      "epoch: 10 [13440/60000 (22%)]\t training loss: 0.000528\n",
      "epoch: 10 [13760/60000 (23%)]\t training loss: 0.001952\n",
      "epoch: 10 [14080/60000 (23%)]\t training loss: 0.000755\n",
      "epoch: 10 [14400/60000 (24%)]\t training loss: 0.078433\n",
      "epoch: 10 [14720/60000 (25%)]\t training loss: 0.001359\n",
      "epoch: 10 [15040/60000 (25%)]\t training loss: 0.096306\n",
      "epoch: 10 [15360/60000 (26%)]\t training loss: 0.127262\n",
      "epoch: 10 [15680/60000 (26%)]\t training loss: 0.001470\n",
      "epoch: 10 [16000/60000 (27%)]\t training loss: 0.003649\n",
      "epoch: 10 [16320/60000 (27%)]\t training loss: 0.000098\n",
      "epoch: 10 [16640/60000 (28%)]\t training loss: 0.000290\n",
      "epoch: 10 [16960/60000 (28%)]\t training loss: 0.002049\n",
      "epoch: 10 [17280/60000 (29%)]\t training loss: 0.042258\n",
      "epoch: 10 [17600/60000 (29%)]\t training loss: 0.000411\n",
      "epoch: 10 [17920/60000 (30%)]\t training loss: 0.267730\n",
      "epoch: 10 [18240/60000 (30%)]\t training loss: 0.009938\n",
      "epoch: 10 [18560/60000 (31%)]\t training loss: 0.007933\n",
      "epoch: 10 [18880/60000 (31%)]\t training loss: 0.307190\n",
      "epoch: 10 [19200/60000 (32%)]\t training loss: 0.079699\n",
      "epoch: 10 [19520/60000 (33%)]\t training loss: 0.004463\n",
      "epoch: 10 [19840/60000 (33%)]\t training loss: 0.002457\n",
      "epoch: 10 [20160/60000 (34%)]\t training loss: 0.080324\n",
      "epoch: 10 [20480/60000 (34%)]\t training loss: 0.147649\n",
      "epoch: 10 [20800/60000 (35%)]\t training loss: 0.107820\n",
      "epoch: 10 [21120/60000 (35%)]\t training loss: 0.001540\n",
      "epoch: 10 [21440/60000 (36%)]\t training loss: 0.004514\n",
      "epoch: 10 [21760/60000 (36%)]\t training loss: 0.002174\n",
      "epoch: 10 [22080/60000 (37%)]\t training loss: 0.001064\n",
      "epoch: 10 [22400/60000 (37%)]\t training loss: 0.000839\n",
      "epoch: 10 [22720/60000 (38%)]\t training loss: 0.004025\n",
      "epoch: 10 [23040/60000 (38%)]\t training loss: 0.008637\n",
      "epoch: 10 [23360/60000 (39%)]\t training loss: 0.010971\n",
      "epoch: 10 [23680/60000 (39%)]\t training loss: 0.015731\n",
      "epoch: 10 [24000/60000 (40%)]\t training loss: 0.001220\n",
      "epoch: 10 [24320/60000 (41%)]\t training loss: 0.559568\n",
      "epoch: 10 [24640/60000 (41%)]\t training loss: 0.001435\n",
      "epoch: 10 [24960/60000 (42%)]\t training loss: 0.073781\n",
      "epoch: 10 [25280/60000 (42%)]\t training loss: 0.001169\n",
      "epoch: 10 [25600/60000 (43%)]\t training loss: 0.014497\n",
      "epoch: 10 [25920/60000 (43%)]\t training loss: 0.007663\n",
      "epoch: 10 [26240/60000 (44%)]\t training loss: 0.027547\n",
      "epoch: 10 [26560/60000 (44%)]\t training loss: 0.000447\n",
      "epoch: 10 [26880/60000 (45%)]\t training loss: 0.005137\n",
      "epoch: 10 [27200/60000 (45%)]\t training loss: 0.007517\n",
      "epoch: 10 [27520/60000 (46%)]\t training loss: 0.001015\n",
      "epoch: 10 [27840/60000 (46%)]\t training loss: 0.009873\n",
      "epoch: 10 [28160/60000 (47%)]\t training loss: 0.009515\n",
      "epoch: 10 [28480/60000 (47%)]\t training loss: 0.016632\n",
      "epoch: 10 [28800/60000 (48%)]\t training loss: 0.011772\n",
      "epoch: 10 [29120/60000 (49%)]\t training loss: 0.006574\n",
      "epoch: 10 [29440/60000 (49%)]\t training loss: 0.000411\n",
      "epoch: 10 [29760/60000 (50%)]\t training loss: 0.009119\n",
      "epoch: 10 [30080/60000 (50%)]\t training loss: 0.234564\n",
      "epoch: 10 [30400/60000 (51%)]\t training loss: 0.003626\n",
      "epoch: 10 [30720/60000 (51%)]\t training loss: 0.000195\n",
      "epoch: 10 [31040/60000 (52%)]\t training loss: 0.007993\n",
      "epoch: 10 [31360/60000 (52%)]\t training loss: 0.001546\n",
      "epoch: 10 [31680/60000 (53%)]\t training loss: 0.011093\n",
      "epoch: 10 [32000/60000 (53%)]\t training loss: 0.000327\n",
      "epoch: 10 [32320/60000 (54%)]\t training loss: 0.002118\n",
      "epoch: 10 [32640/60000 (54%)]\t training loss: 0.011985\n",
      "epoch: 10 [32960/60000 (55%)]\t training loss: 0.054599\n",
      "epoch: 10 [33280/60000 (55%)]\t training loss: 0.014489\n",
      "epoch: 10 [33600/60000 (56%)]\t training loss: 0.035039\n",
      "epoch: 10 [33920/60000 (57%)]\t training loss: 0.000157\n",
      "epoch: 10 [34240/60000 (57%)]\t training loss: 0.056464\n",
      "epoch: 10 [34560/60000 (58%)]\t training loss: 0.000087\n",
      "epoch: 10 [34880/60000 (58%)]\t training loss: 0.000235\n",
      "epoch: 10 [35200/60000 (59%)]\t training loss: 0.000859\n",
      "epoch: 10 [35520/60000 (59%)]\t training loss: 0.018036\n",
      "epoch: 10 [35840/60000 (60%)]\t training loss: 0.151209\n",
      "epoch: 10 [36160/60000 (60%)]\t training loss: 0.043520\n",
      "epoch: 10 [36480/60000 (61%)]\t training loss: 0.064396\n",
      "epoch: 10 [36800/60000 (61%)]\t training loss: 0.001701\n",
      "epoch: 10 [37120/60000 (62%)]\t training loss: 0.017121\n",
      "epoch: 10 [37440/60000 (62%)]\t training loss: 0.002677\n",
      "epoch: 10 [37760/60000 (63%)]\t training loss: 0.006492\n",
      "epoch: 10 [38080/60000 (63%)]\t training loss: 0.000084\n",
      "epoch: 10 [38400/60000 (64%)]\t training loss: 0.000260\n",
      "epoch: 10 [38720/60000 (65%)]\t training loss: 0.043292\n",
      "epoch: 10 [39040/60000 (65%)]\t training loss: 0.002214\n",
      "epoch: 10 [39360/60000 (66%)]\t training loss: 0.021658\n",
      "epoch: 10 [39680/60000 (66%)]\t training loss: 0.000030\n",
      "epoch: 10 [40000/60000 (67%)]\t training loss: 0.164582\n",
      "epoch: 10 [40320/60000 (67%)]\t training loss: 0.000356\n",
      "epoch: 10 [40640/60000 (68%)]\t training loss: 0.000600\n",
      "epoch: 10 [40960/60000 (68%)]\t training loss: 0.063014\n",
      "epoch: 10 [41280/60000 (69%)]\t training loss: 0.081612\n",
      "epoch: 10 [41600/60000 (69%)]\t training loss: 0.000100\n",
      "epoch: 10 [41920/60000 (70%)]\t training loss: 0.350524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 [42240/60000 (70%)]\t training loss: 0.004667\n",
      "epoch: 10 [42560/60000 (71%)]\t training loss: 0.003273\n",
      "epoch: 10 [42880/60000 (71%)]\t training loss: 0.159519\n",
      "epoch: 10 [43200/60000 (72%)]\t training loss: 0.044993\n",
      "epoch: 10 [43520/60000 (73%)]\t training loss: 0.007863\n",
      "epoch: 10 [43840/60000 (73%)]\t training loss: 0.002072\n",
      "epoch: 10 [44160/60000 (74%)]\t training loss: 0.014165\n",
      "epoch: 10 [44480/60000 (74%)]\t training loss: 0.329461\n",
      "epoch: 10 [44800/60000 (75%)]\t training loss: 0.002623\n",
      "epoch: 10 [45120/60000 (75%)]\t training loss: 0.092762\n",
      "epoch: 10 [45440/60000 (76%)]\t training loss: 0.003307\n",
      "epoch: 10 [45760/60000 (76%)]\t training loss: 0.002195\n",
      "epoch: 10 [46080/60000 (77%)]\t training loss: 0.001148\n",
      "epoch: 10 [46400/60000 (77%)]\t training loss: 0.018594\n",
      "epoch: 10 [46720/60000 (78%)]\t training loss: 0.000153\n",
      "epoch: 10 [47040/60000 (78%)]\t training loss: 0.083433\n",
      "epoch: 10 [47360/60000 (79%)]\t training loss: 0.004734\n",
      "epoch: 10 [47680/60000 (79%)]\t training loss: 0.015791\n",
      "epoch: 10 [48000/60000 (80%)]\t training loss: 0.212949\n",
      "epoch: 10 [48320/60000 (81%)]\t training loss: 0.001569\n",
      "epoch: 10 [48640/60000 (81%)]\t training loss: 0.002188\n",
      "epoch: 10 [48960/60000 (82%)]\t training loss: 0.003921\n",
      "epoch: 10 [49280/60000 (82%)]\t training loss: 0.074242\n",
      "epoch: 10 [49600/60000 (83%)]\t training loss: 0.093723\n",
      "epoch: 10 [49920/60000 (83%)]\t training loss: 0.021261\n",
      "epoch: 10 [50240/60000 (84%)]\t training loss: 0.000643\n",
      "epoch: 10 [50560/60000 (84%)]\t training loss: 0.010053\n",
      "epoch: 10 [50880/60000 (85%)]\t training loss: 0.011559\n",
      "epoch: 10 [51200/60000 (85%)]\t training loss: 0.116247\n",
      "epoch: 10 [51520/60000 (86%)]\t training loss: 0.000678\n",
      "epoch: 10 [51840/60000 (86%)]\t training loss: 0.045127\n",
      "epoch: 10 [52160/60000 (87%)]\t training loss: 0.000986\n",
      "epoch: 10 [52480/60000 (87%)]\t training loss: 0.005028\n",
      "epoch: 10 [52800/60000 (88%)]\t training loss: 0.007225\n",
      "epoch: 10 [53120/60000 (89%)]\t training loss: 0.048392\n",
      "epoch: 10 [53440/60000 (89%)]\t training loss: 0.046206\n",
      "epoch: 10 [53760/60000 (90%)]\t training loss: 0.000037\n",
      "epoch: 10 [54080/60000 (90%)]\t training loss: 0.007742\n",
      "epoch: 10 [54400/60000 (91%)]\t training loss: 0.399387\n",
      "epoch: 10 [54720/60000 (91%)]\t training loss: 0.000498\n",
      "epoch: 10 [55040/60000 (92%)]\t training loss: 0.059232\n",
      "epoch: 10 [55360/60000 (92%)]\t training loss: 0.063506\n",
      "epoch: 10 [55680/60000 (93%)]\t training loss: 0.000199\n",
      "epoch: 10 [56000/60000 (93%)]\t training loss: 0.123436\n",
      "epoch: 10 [56320/60000 (94%)]\t training loss: 0.000627\n",
      "epoch: 10 [56640/60000 (94%)]\t training loss: 0.147749\n",
      "epoch: 10 [56960/60000 (95%)]\t training loss: 0.067129\n",
      "epoch: 10 [57280/60000 (95%)]\t training loss: 0.012688\n",
      "epoch: 10 [57600/60000 (96%)]\t training loss: 0.008669\n",
      "epoch: 10 [57920/60000 (97%)]\t training loss: 0.058401\n",
      "epoch: 10 [58240/60000 (97%)]\t training loss: 0.079139\n",
      "epoch: 10 [58560/60000 (98%)]\t training loss: 0.005179\n",
      "epoch: 10 [58880/60000 (98%)]\t training loss: 0.005083\n",
      "epoch: 10 [59200/60000 (99%)]\t training loss: 0.025003\n",
      "epoch: 10 [59520/60000 (99%)]\t training loss: 0.019594\n",
      "epoch: 10 [59840/60000 (100%)]\t training loss: 0.127333\n",
      "\n",
      "Test dataset: Overall Loss: 0.0442, Overall Accuracy: 9896/10000 (99%)\n",
      "\n",
      "epoch: 11 [0/60000 (0%)]\t training loss: 0.005075\n",
      "epoch: 11 [320/60000 (1%)]\t training loss: 0.035892\n",
      "epoch: 11 [640/60000 (1%)]\t training loss: 0.004662\n",
      "epoch: 11 [960/60000 (2%)]\t training loss: 0.080236\n",
      "epoch: 11 [1280/60000 (2%)]\t training loss: 0.001869\n",
      "epoch: 11 [1600/60000 (3%)]\t training loss: 0.052246\n",
      "epoch: 11 [1920/60000 (3%)]\t training loss: 0.000869\n",
      "epoch: 11 [2240/60000 (4%)]\t training loss: 0.001130\n",
      "epoch: 11 [2560/60000 (4%)]\t training loss: 0.000205\n",
      "epoch: 11 [2880/60000 (5%)]\t training loss: 0.027513\n",
      "epoch: 11 [3200/60000 (5%)]\t training loss: 0.002390\n",
      "epoch: 11 [3520/60000 (6%)]\t training loss: 0.035072\n",
      "epoch: 11 [3840/60000 (6%)]\t training loss: 0.141034\n",
      "epoch: 11 [4160/60000 (7%)]\t training loss: 0.038843\n",
      "epoch: 11 [4480/60000 (7%)]\t training loss: 0.061675\n",
      "epoch: 11 [4800/60000 (8%)]\t training loss: 0.025123\n",
      "epoch: 11 [5120/60000 (9%)]\t training loss: 0.052774\n",
      "epoch: 11 [5440/60000 (9%)]\t training loss: 0.009268\n",
      "epoch: 11 [5760/60000 (10%)]\t training loss: 0.001583\n",
      "epoch: 11 [6080/60000 (10%)]\t training loss: 0.001229\n",
      "epoch: 11 [6400/60000 (11%)]\t training loss: 0.000269\n",
      "epoch: 11 [6720/60000 (11%)]\t training loss: 0.132034\n",
      "epoch: 11 [7040/60000 (12%)]\t training loss: 0.001332\n",
      "epoch: 11 [7360/60000 (12%)]\t training loss: 0.000382\n",
      "epoch: 11 [7680/60000 (13%)]\t training loss: 0.000180\n",
      "epoch: 11 [8000/60000 (13%)]\t training loss: 0.000418\n",
      "epoch: 11 [8320/60000 (14%)]\t training loss: 0.001552\n",
      "epoch: 11 [8640/60000 (14%)]\t training loss: 0.000673\n",
      "epoch: 11 [8960/60000 (15%)]\t training loss: 0.022041\n",
      "epoch: 11 [9280/60000 (15%)]\t training loss: 0.021448\n",
      "epoch: 11 [9600/60000 (16%)]\t training loss: 0.012043\n",
      "epoch: 11 [9920/60000 (17%)]\t training loss: 0.014506\n",
      "epoch: 11 [10240/60000 (17%)]\t training loss: 0.234580\n",
      "epoch: 11 [10560/60000 (18%)]\t training loss: 0.006167\n",
      "epoch: 11 [10880/60000 (18%)]\t training loss: 0.022432\n",
      "epoch: 11 [11200/60000 (19%)]\t training loss: 0.028766\n",
      "epoch: 11 [11520/60000 (19%)]\t training loss: 0.027609\n",
      "epoch: 11 [11840/60000 (20%)]\t training loss: 0.006592\n",
      "epoch: 11 [12160/60000 (20%)]\t training loss: 0.003919\n",
      "epoch: 11 [12480/60000 (21%)]\t training loss: 0.003467\n",
      "epoch: 11 [12800/60000 (21%)]\t training loss: 0.000101\n",
      "epoch: 11 [13120/60000 (22%)]\t training loss: 0.046745\n",
      "epoch: 11 [13440/60000 (22%)]\t training loss: 0.047196\n",
      "epoch: 11 [13760/60000 (23%)]\t training loss: 0.004405\n",
      "epoch: 11 [14080/60000 (23%)]\t training loss: 0.000096\n",
      "epoch: 11 [14400/60000 (24%)]\t training loss: 0.149175\n",
      "epoch: 11 [14720/60000 (25%)]\t training loss: 0.015552\n",
      "epoch: 11 [15040/60000 (25%)]\t training loss: 0.037159\n",
      "epoch: 11 [15360/60000 (26%)]\t training loss: 0.000125\n",
      "epoch: 11 [15680/60000 (26%)]\t training loss: 0.008198\n",
      "epoch: 11 [16000/60000 (27%)]\t training loss: 0.000034\n",
      "epoch: 11 [16320/60000 (27%)]\t training loss: 0.001471\n",
      "epoch: 11 [16640/60000 (28%)]\t training loss: 0.004780\n",
      "epoch: 11 [16960/60000 (28%)]\t training loss: 0.002098\n",
      "epoch: 11 [17280/60000 (29%)]\t training loss: 0.008605\n",
      "epoch: 11 [17600/60000 (29%)]\t training loss: 0.136734\n",
      "epoch: 11 [17920/60000 (30%)]\t training loss: 0.016637\n",
      "epoch: 11 [18240/60000 (30%)]\t training loss: 0.109346\n",
      "epoch: 11 [18560/60000 (31%)]\t training loss: 0.005430\n",
      "epoch: 11 [18880/60000 (31%)]\t training loss: 0.009195\n",
      "epoch: 11 [19200/60000 (32%)]\t training loss: 0.005230\n",
      "epoch: 11 [19520/60000 (33%)]\t training loss: 0.000136\n",
      "epoch: 11 [19840/60000 (33%)]\t training loss: 0.035912\n",
      "epoch: 11 [20160/60000 (34%)]\t training loss: 0.016900\n",
      "epoch: 11 [20480/60000 (34%)]\t training loss: 0.004962\n",
      "epoch: 11 [20800/60000 (35%)]\t training loss: 0.000631\n",
      "epoch: 11 [21120/60000 (35%)]\t training loss: 0.000694\n",
      "epoch: 11 [21440/60000 (36%)]\t training loss: 0.004176\n",
      "epoch: 11 [21760/60000 (36%)]\t training loss: 0.012578\n",
      "epoch: 11 [22080/60000 (37%)]\t training loss: 0.002748\n",
      "epoch: 11 [22400/60000 (37%)]\t training loss: 0.004115\n",
      "epoch: 11 [22720/60000 (38%)]\t training loss: 0.009460\n",
      "epoch: 11 [23040/60000 (38%)]\t training loss: 0.133072\n",
      "epoch: 11 [23360/60000 (39%)]\t training loss: 0.190364\n",
      "epoch: 11 [23680/60000 (39%)]\t training loss: 0.006166\n",
      "epoch: 11 [24000/60000 (40%)]\t training loss: 0.000394\n",
      "epoch: 11 [24320/60000 (41%)]\t training loss: 0.003918\n",
      "epoch: 11 [24640/60000 (41%)]\t training loss: 0.004487\n",
      "epoch: 11 [24960/60000 (42%)]\t training loss: 0.012290\n",
      "epoch: 11 [25280/60000 (42%)]\t training loss: 0.000833\n",
      "epoch: 11 [25600/60000 (43%)]\t training loss: 0.008250\n",
      "epoch: 11 [25920/60000 (43%)]\t training loss: 0.001220\n",
      "epoch: 11 [26240/60000 (44%)]\t training loss: 0.155035\n",
      "epoch: 11 [26560/60000 (44%)]\t training loss: 0.005977\n",
      "epoch: 11 [26880/60000 (45%)]\t training loss: 0.012344\n",
      "epoch: 11 [27200/60000 (45%)]\t training loss: 0.003899\n",
      "epoch: 11 [27520/60000 (46%)]\t training loss: 0.011058\n",
      "epoch: 11 [27840/60000 (46%)]\t training loss: 0.001494\n",
      "epoch: 11 [28160/60000 (47%)]\t training loss: 0.024877\n",
      "epoch: 11 [28480/60000 (47%)]\t training loss: 0.000154\n",
      "epoch: 11 [28800/60000 (48%)]\t training loss: 0.007949\n",
      "epoch: 11 [29120/60000 (49%)]\t training loss: 0.008855\n",
      "epoch: 11 [29440/60000 (49%)]\t training loss: 0.014060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 [29760/60000 (50%)]\t training loss: 0.000132\n",
      "epoch: 11 [30080/60000 (50%)]\t training loss: 0.010404\n",
      "epoch: 11 [30400/60000 (51%)]\t training loss: 0.002291\n",
      "epoch: 11 [30720/60000 (51%)]\t training loss: 0.042407\n",
      "epoch: 11 [31040/60000 (52%)]\t training loss: 0.004871\n",
      "epoch: 11 [31360/60000 (52%)]\t training loss: 0.001691\n",
      "epoch: 11 [31680/60000 (53%)]\t training loss: 0.000192\n",
      "epoch: 11 [32000/60000 (53%)]\t training loss: 0.097722\n",
      "epoch: 11 [32320/60000 (54%)]\t training loss: 0.007583\n",
      "epoch: 11 [32640/60000 (54%)]\t training loss: 0.001757\n",
      "epoch: 11 [32960/60000 (55%)]\t training loss: 0.005953\n",
      "epoch: 11 [33280/60000 (55%)]\t training loss: 0.002415\n",
      "epoch: 11 [33600/60000 (56%)]\t training loss: 0.061409\n",
      "epoch: 11 [33920/60000 (57%)]\t training loss: 0.000374\n",
      "epoch: 11 [34240/60000 (57%)]\t training loss: 0.000012\n",
      "epoch: 11 [34560/60000 (58%)]\t training loss: 0.000087\n",
      "epoch: 11 [34880/60000 (58%)]\t training loss: 0.155124\n",
      "epoch: 11 [35200/60000 (59%)]\t training loss: 0.005680\n",
      "epoch: 11 [35520/60000 (59%)]\t training loss: 0.307512\n",
      "epoch: 11 [35840/60000 (60%)]\t training loss: 0.061605\n",
      "epoch: 11 [36160/60000 (60%)]\t training loss: 0.016499\n",
      "epoch: 11 [36480/60000 (61%)]\t training loss: 0.008418\n",
      "epoch: 11 [36800/60000 (61%)]\t training loss: 0.017914\n",
      "epoch: 11 [37120/60000 (62%)]\t training loss: 0.000041\n",
      "epoch: 11 [37440/60000 (62%)]\t training loss: 0.028419\n",
      "epoch: 11 [37760/60000 (63%)]\t training loss: 0.009445\n",
      "epoch: 11 [38080/60000 (63%)]\t training loss: 0.018534\n",
      "epoch: 11 [38400/60000 (64%)]\t training loss: 0.001742\n",
      "epoch: 11 [38720/60000 (65%)]\t training loss: 0.001121\n",
      "epoch: 11 [39040/60000 (65%)]\t training loss: 0.012849\n",
      "epoch: 11 [39360/60000 (66%)]\t training loss: 0.000006\n",
      "epoch: 11 [39680/60000 (66%)]\t training loss: 0.291693\n",
      "epoch: 11 [40000/60000 (67%)]\t training loss: 0.003554\n",
      "epoch: 11 [40320/60000 (67%)]\t training loss: 0.235347\n",
      "epoch: 11 [40640/60000 (68%)]\t training loss: 0.004546\n",
      "epoch: 11 [40960/60000 (68%)]\t training loss: 0.014469\n",
      "epoch: 11 [41280/60000 (69%)]\t training loss: 0.069382\n",
      "epoch: 11 [41600/60000 (69%)]\t training loss: 0.023331\n",
      "epoch: 11 [41920/60000 (70%)]\t training loss: 0.170587\n",
      "epoch: 11 [42240/60000 (70%)]\t training loss: 0.003688\n",
      "epoch: 11 [42560/60000 (71%)]\t training loss: 0.000289\n",
      "epoch: 11 [42880/60000 (71%)]\t training loss: 0.016543\n",
      "epoch: 11 [43200/60000 (72%)]\t training loss: 0.090751\n",
      "epoch: 11 [43520/60000 (73%)]\t training loss: 0.000495\n",
      "epoch: 11 [43840/60000 (73%)]\t training loss: 0.021781\n",
      "epoch: 11 [44160/60000 (74%)]\t training loss: 0.002441\n",
      "epoch: 11 [44480/60000 (74%)]\t training loss: 0.001213\n",
      "epoch: 11 [44800/60000 (75%)]\t training loss: 0.001023\n",
      "epoch: 11 [45120/60000 (75%)]\t training loss: 0.000144\n",
      "epoch: 11 [45440/60000 (76%)]\t training loss: 0.000345\n",
      "epoch: 11 [45760/60000 (76%)]\t training loss: 0.001126\n",
      "epoch: 11 [46080/60000 (77%)]\t training loss: 0.000010\n",
      "epoch: 11 [46400/60000 (77%)]\t training loss: 0.055788\n",
      "epoch: 11 [46720/60000 (78%)]\t training loss: 0.010196\n",
      "epoch: 11 [47040/60000 (78%)]\t training loss: 0.001683\n",
      "epoch: 11 [47360/60000 (79%)]\t training loss: 0.000028\n",
      "epoch: 11 [47680/60000 (79%)]\t training loss: 0.000004\n",
      "epoch: 11 [48000/60000 (80%)]\t training loss: 0.007580\n",
      "epoch: 11 [48320/60000 (81%)]\t training loss: 0.002433\n",
      "epoch: 11 [48640/60000 (81%)]\t training loss: 0.005956\n",
      "epoch: 11 [48960/60000 (82%)]\t training loss: 0.000454\n",
      "epoch: 11 [49280/60000 (82%)]\t training loss: 0.002010\n",
      "epoch: 11 [49600/60000 (83%)]\t training loss: 0.034382\n",
      "epoch: 11 [49920/60000 (83%)]\t training loss: 0.009053\n",
      "epoch: 11 [50240/60000 (84%)]\t training loss: 0.000402\n",
      "epoch: 11 [50560/60000 (84%)]\t training loss: 0.000750\n",
      "epoch: 11 [50880/60000 (85%)]\t training loss: 0.000382\n",
      "epoch: 11 [51200/60000 (85%)]\t training loss: 0.017749\n",
      "epoch: 11 [51520/60000 (86%)]\t training loss: 0.017985\n",
      "epoch: 11 [51840/60000 (86%)]\t training loss: 0.056468\n",
      "epoch: 11 [52160/60000 (87%)]\t training loss: 0.046967\n",
      "epoch: 11 [52480/60000 (87%)]\t training loss: 0.020932\n",
      "epoch: 11 [52800/60000 (88%)]\t training loss: 0.164340\n",
      "epoch: 11 [53120/60000 (89%)]\t training loss: 0.063784\n",
      "epoch: 11 [53440/60000 (89%)]\t training loss: 0.021290\n",
      "epoch: 11 [53760/60000 (90%)]\t training loss: 0.008352\n",
      "epoch: 11 [54080/60000 (90%)]\t training loss: 0.006906\n",
      "epoch: 11 [54400/60000 (91%)]\t training loss: 0.208719\n",
      "epoch: 11 [54720/60000 (91%)]\t training loss: 0.186641\n",
      "epoch: 11 [55040/60000 (92%)]\t training loss: 0.006795\n",
      "epoch: 11 [55360/60000 (92%)]\t training loss: 0.011286\n",
      "epoch: 11 [55680/60000 (93%)]\t training loss: 0.000427\n",
      "epoch: 11 [56000/60000 (93%)]\t training loss: 0.013238\n",
      "epoch: 11 [56320/60000 (94%)]\t training loss: 0.000034\n",
      "epoch: 11 [56640/60000 (94%)]\t training loss: 0.008550\n",
      "epoch: 11 [56960/60000 (95%)]\t training loss: 0.000106\n",
      "epoch: 11 [57280/60000 (95%)]\t training loss: 0.019373\n",
      "epoch: 11 [57600/60000 (96%)]\t training loss: 0.038142\n",
      "epoch: 11 [57920/60000 (97%)]\t training loss: 0.022725\n",
      "epoch: 11 [58240/60000 (97%)]\t training loss: 0.000293\n",
      "epoch: 11 [58560/60000 (98%)]\t training loss: 0.000580\n",
      "epoch: 11 [58880/60000 (98%)]\t training loss: 0.011319\n",
      "epoch: 11 [59200/60000 (99%)]\t training loss: 0.004712\n",
      "epoch: 11 [59520/60000 (99%)]\t training loss: 0.001579\n",
      "epoch: 11 [59840/60000 (100%)]\t training loss: 0.012149\n",
      "\n",
      "Test dataset: Overall Loss: 0.0412, Overall Accuracy: 9891/10000 (99%)\n",
      "\n",
      "epoch: 12 [0/60000 (0%)]\t training loss: 0.001664\n",
      "epoch: 12 [320/60000 (1%)]\t training loss: 0.001515\n",
      "epoch: 12 [640/60000 (1%)]\t training loss: 0.013694\n",
      "epoch: 12 [960/60000 (2%)]\t training loss: 0.004045\n",
      "epoch: 12 [1280/60000 (2%)]\t training loss: 0.013976\n",
      "epoch: 12 [1600/60000 (3%)]\t training loss: 0.045743\n",
      "epoch: 12 [1920/60000 (3%)]\t training loss: 0.000531\n",
      "epoch: 12 [2240/60000 (4%)]\t training loss: 0.053777\n",
      "epoch: 12 [2560/60000 (4%)]\t training loss: 0.004471\n",
      "epoch: 12 [2880/60000 (5%)]\t training loss: 0.002382\n",
      "epoch: 12 [3200/60000 (5%)]\t training loss: 0.001868\n",
      "epoch: 12 [3520/60000 (6%)]\t training loss: 0.005330\n",
      "epoch: 12 [3840/60000 (6%)]\t training loss: 0.054048\n",
      "epoch: 12 [4160/60000 (7%)]\t training loss: 0.002645\n",
      "epoch: 12 [4480/60000 (7%)]\t training loss: 0.001822\n",
      "epoch: 12 [4800/60000 (8%)]\t training loss: 0.000787\n",
      "epoch: 12 [5120/60000 (9%)]\t training loss: 0.006616\n",
      "epoch: 12 [5440/60000 (9%)]\t training loss: 0.003266\n",
      "epoch: 12 [5760/60000 (10%)]\t training loss: 0.000575\n",
      "epoch: 12 [6080/60000 (10%)]\t training loss: 0.012541\n",
      "epoch: 12 [6400/60000 (11%)]\t training loss: 0.077203\n",
      "epoch: 12 [6720/60000 (11%)]\t training loss: 0.049378\n",
      "epoch: 12 [7040/60000 (12%)]\t training loss: 0.023788\n",
      "epoch: 12 [7360/60000 (12%)]\t training loss: 0.000111\n",
      "epoch: 12 [7680/60000 (13%)]\t training loss: 0.024486\n",
      "epoch: 12 [8000/60000 (13%)]\t training loss: 0.083015\n",
      "epoch: 12 [8320/60000 (14%)]\t training loss: 0.000084\n",
      "epoch: 12 [8640/60000 (14%)]\t training loss: 0.000689\n",
      "epoch: 12 [8960/60000 (15%)]\t training loss: 0.002669\n",
      "epoch: 12 [9280/60000 (15%)]\t training loss: 0.000020\n",
      "epoch: 12 [9600/60000 (16%)]\t training loss: 0.001809\n",
      "epoch: 12 [9920/60000 (17%)]\t training loss: 0.000087\n",
      "epoch: 12 [10240/60000 (17%)]\t training loss: 0.000385\n",
      "epoch: 12 [10560/60000 (18%)]\t training loss: 0.000485\n",
      "epoch: 12 [10880/60000 (18%)]\t training loss: 0.038524\n",
      "epoch: 12 [11200/60000 (19%)]\t training loss: 0.001025\n",
      "epoch: 12 [11520/60000 (19%)]\t training loss: 0.000420\n",
      "epoch: 12 [11840/60000 (20%)]\t training loss: 0.000805\n",
      "epoch: 12 [12160/60000 (20%)]\t training loss: 0.006897\n",
      "epoch: 12 [12480/60000 (21%)]\t training loss: 0.000280\n",
      "epoch: 12 [12800/60000 (21%)]\t training loss: 0.009410\n",
      "epoch: 12 [13120/60000 (22%)]\t training loss: 0.001203\n",
      "epoch: 12 [13440/60000 (22%)]\t training loss: 0.002924\n",
      "epoch: 12 [13760/60000 (23%)]\t training loss: 0.013929\n",
      "epoch: 12 [14080/60000 (23%)]\t training loss: 0.001056\n",
      "epoch: 12 [14400/60000 (24%)]\t training loss: 0.000825\n",
      "epoch: 12 [14720/60000 (25%)]\t training loss: 0.186766\n",
      "epoch: 12 [15040/60000 (25%)]\t training loss: 0.000369\n",
      "epoch: 12 [15360/60000 (26%)]\t training loss: 0.008808\n",
      "epoch: 12 [15680/60000 (26%)]\t training loss: 0.000253\n",
      "epoch: 12 [16000/60000 (27%)]\t training loss: 0.003518\n",
      "epoch: 12 [16320/60000 (27%)]\t training loss: 0.003939\n",
      "epoch: 12 [16640/60000 (28%)]\t training loss: 0.011666\n",
      "epoch: 12 [16960/60000 (28%)]\t training loss: 0.076993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 [17280/60000 (29%)]\t training loss: 0.000235\n",
      "epoch: 12 [17600/60000 (29%)]\t training loss: 0.050121\n",
      "epoch: 12 [17920/60000 (30%)]\t training loss: 0.016432\n",
      "epoch: 12 [18240/60000 (30%)]\t training loss: 0.000581\n",
      "epoch: 12 [18560/60000 (31%)]\t training loss: 0.032073\n",
      "epoch: 12 [18880/60000 (31%)]\t training loss: 0.019073\n",
      "epoch: 12 [19200/60000 (32%)]\t training loss: 0.000349\n",
      "epoch: 12 [19520/60000 (33%)]\t training loss: 0.004057\n",
      "epoch: 12 [19840/60000 (33%)]\t training loss: 0.002394\n",
      "epoch: 12 [20160/60000 (34%)]\t training loss: 0.000623\n",
      "epoch: 12 [20480/60000 (34%)]\t training loss: 0.181253\n",
      "epoch: 12 [20800/60000 (35%)]\t training loss: 0.009851\n",
      "epoch: 12 [21120/60000 (35%)]\t training loss: 0.161440\n",
      "epoch: 12 [21440/60000 (36%)]\t training loss: 0.008279\n",
      "epoch: 12 [21760/60000 (36%)]\t training loss: 0.014098\n",
      "epoch: 12 [22080/60000 (37%)]\t training loss: 0.290747\n",
      "epoch: 12 [22400/60000 (37%)]\t training loss: 0.000239\n",
      "epoch: 12 [22720/60000 (38%)]\t training loss: 0.003604\n",
      "epoch: 12 [23040/60000 (38%)]\t training loss: 0.028717\n",
      "epoch: 12 [23360/60000 (39%)]\t training loss: 0.000392\n",
      "epoch: 12 [23680/60000 (39%)]\t training loss: 0.000532\n",
      "epoch: 12 [24000/60000 (40%)]\t training loss: 0.000317\n",
      "epoch: 12 [24320/60000 (41%)]\t training loss: 0.002510\n",
      "epoch: 12 [24640/60000 (41%)]\t training loss: 0.001905\n",
      "epoch: 12 [24960/60000 (42%)]\t training loss: 0.000761\n",
      "epoch: 12 [25280/60000 (42%)]\t training loss: 0.208010\n",
      "epoch: 12 [25600/60000 (43%)]\t training loss: 0.001750\n",
      "epoch: 12 [25920/60000 (43%)]\t training loss: 0.000404\n",
      "epoch: 12 [26240/60000 (44%)]\t training loss: 0.001121\n",
      "epoch: 12 [26560/60000 (44%)]\t training loss: 0.004857\n",
      "epoch: 12 [26880/60000 (45%)]\t training loss: 0.144806\n",
      "epoch: 12 [27200/60000 (45%)]\t training loss: 0.000043\n",
      "epoch: 12 [27520/60000 (46%)]\t training loss: 0.005491\n",
      "epoch: 12 [27840/60000 (46%)]\t training loss: 0.002674\n",
      "epoch: 12 [28160/60000 (47%)]\t training loss: 0.000034\n",
      "epoch: 12 [28480/60000 (47%)]\t training loss: 0.001873\n",
      "epoch: 12 [28800/60000 (48%)]\t training loss: 0.145488\n",
      "epoch: 12 [29120/60000 (49%)]\t training loss: 0.058386\n",
      "epoch: 12 [29440/60000 (49%)]\t training loss: 0.000254\n",
      "epoch: 12 [29760/60000 (50%)]\t training loss: 0.010827\n",
      "epoch: 12 [30080/60000 (50%)]\t training loss: 0.011561\n",
      "epoch: 12 [30400/60000 (51%)]\t training loss: 0.000386\n",
      "epoch: 12 [30720/60000 (51%)]\t training loss: 0.042279\n",
      "epoch: 12 [31040/60000 (52%)]\t training loss: 0.000004\n",
      "epoch: 12 [31360/60000 (52%)]\t training loss: 0.088010\n",
      "epoch: 12 [31680/60000 (53%)]\t training loss: 0.026063\n",
      "epoch: 12 [32000/60000 (53%)]\t training loss: 0.008847\n",
      "epoch: 12 [32320/60000 (54%)]\t training loss: 0.000338\n",
      "epoch: 12 [32640/60000 (54%)]\t training loss: 0.001011\n",
      "epoch: 12 [32960/60000 (55%)]\t training loss: 0.000013\n",
      "epoch: 12 [33280/60000 (55%)]\t training loss: 0.001676\n",
      "epoch: 12 [33600/60000 (56%)]\t training loss: 0.000305\n",
      "epoch: 12 [33920/60000 (57%)]\t training loss: 0.001739\n",
      "epoch: 12 [34240/60000 (57%)]\t training loss: 0.082050\n",
      "epoch: 12 [34560/60000 (58%)]\t training loss: 0.181558\n",
      "epoch: 12 [34880/60000 (58%)]\t training loss: 0.000384\n",
      "epoch: 12 [35200/60000 (59%)]\t training loss: 0.003523\n",
      "epoch: 12 [35520/60000 (59%)]\t training loss: 0.021463\n",
      "epoch: 12 [35840/60000 (60%)]\t training loss: 0.010002\n",
      "epoch: 12 [36160/60000 (60%)]\t training loss: 0.000082\n",
      "epoch: 12 [36480/60000 (61%)]\t training loss: 0.000006\n",
      "epoch: 12 [36800/60000 (61%)]\t training loss: 0.015050\n",
      "epoch: 12 [37120/60000 (62%)]\t training loss: 0.001332\n",
      "epoch: 12 [37440/60000 (62%)]\t training loss: 0.011494\n",
      "epoch: 12 [37760/60000 (63%)]\t training loss: 0.000973\n",
      "epoch: 12 [38080/60000 (63%)]\t training loss: 0.010004\n",
      "epoch: 12 [38400/60000 (64%)]\t training loss: 0.000489\n",
      "epoch: 12 [38720/60000 (65%)]\t training loss: 0.004903\n",
      "epoch: 12 [39040/60000 (65%)]\t training loss: 0.003064\n",
      "epoch: 12 [39360/60000 (66%)]\t training loss: 0.036756\n",
      "epoch: 12 [39680/60000 (66%)]\t training loss: 0.019048\n",
      "epoch: 12 [40000/60000 (67%)]\t training loss: 0.019841\n",
      "epoch: 12 [40320/60000 (67%)]\t training loss: 0.002870\n",
      "epoch: 12 [40640/60000 (68%)]\t training loss: 0.004012\n",
      "epoch: 12 [40960/60000 (68%)]\t training loss: 0.103463\n",
      "epoch: 12 [41280/60000 (69%)]\t training loss: 0.000008\n",
      "epoch: 12 [41600/60000 (69%)]\t training loss: 0.000540\n",
      "epoch: 12 [41920/60000 (70%)]\t training loss: 0.027574\n",
      "epoch: 12 [42240/60000 (70%)]\t training loss: 0.000153\n",
      "epoch: 12 [42560/60000 (71%)]\t training loss: 0.016574\n",
      "epoch: 12 [42880/60000 (71%)]\t training loss: 0.016079\n",
      "epoch: 12 [43200/60000 (72%)]\t training loss: 0.000262\n",
      "epoch: 12 [43520/60000 (73%)]\t training loss: 0.076995\n",
      "epoch: 12 [43840/60000 (73%)]\t training loss: 0.001674\n",
      "epoch: 12 [44160/60000 (74%)]\t training loss: 0.044859\n",
      "epoch: 12 [44480/60000 (74%)]\t training loss: 0.000305\n",
      "epoch: 12 [44800/60000 (75%)]\t training loss: 0.030654\n",
      "epoch: 12 [45120/60000 (75%)]\t training loss: 0.001465\n",
      "epoch: 12 [45440/60000 (76%)]\t training loss: 0.000270\n",
      "epoch: 12 [45760/60000 (76%)]\t training loss: 0.004185\n",
      "epoch: 12 [46080/60000 (77%)]\t training loss: 0.022355\n",
      "epoch: 12 [46400/60000 (77%)]\t training loss: 0.136987\n",
      "epoch: 12 [46720/60000 (78%)]\t training loss: 0.007310\n",
      "epoch: 12 [47040/60000 (78%)]\t training loss: 0.000547\n",
      "epoch: 12 [47360/60000 (79%)]\t training loss: 0.037810\n",
      "epoch: 12 [47680/60000 (79%)]\t training loss: 0.002268\n",
      "epoch: 12 [48000/60000 (80%)]\t training loss: 0.000003\n",
      "epoch: 12 [48320/60000 (81%)]\t training loss: 0.216764\n",
      "epoch: 12 [48640/60000 (81%)]\t training loss: 0.125634\n",
      "epoch: 12 [48960/60000 (82%)]\t training loss: 0.000583\n",
      "epoch: 12 [49280/60000 (82%)]\t training loss: 0.002266\n",
      "epoch: 12 [49600/60000 (83%)]\t training loss: 0.023793\n",
      "epoch: 12 [49920/60000 (83%)]\t training loss: 0.012770\n",
      "epoch: 12 [50240/60000 (84%)]\t training loss: 0.020087\n",
      "epoch: 12 [50560/60000 (84%)]\t training loss: 0.250975\n",
      "epoch: 12 [50880/60000 (85%)]\t training loss: 0.050480\n",
      "epoch: 12 [51200/60000 (85%)]\t training loss: 0.060321\n",
      "epoch: 12 [51520/60000 (86%)]\t training loss: 0.001449\n",
      "epoch: 12 [51840/60000 (86%)]\t training loss: 0.002745\n",
      "epoch: 12 [52160/60000 (87%)]\t training loss: 0.000910\n",
      "epoch: 12 [52480/60000 (87%)]\t training loss: 0.016230\n",
      "epoch: 12 [52800/60000 (88%)]\t training loss: 0.081833\n",
      "epoch: 12 [53120/60000 (89%)]\t training loss: 0.000702\n",
      "epoch: 12 [53440/60000 (89%)]\t training loss: 0.137442\n",
      "epoch: 12 [53760/60000 (90%)]\t training loss: 0.024107\n",
      "epoch: 12 [54080/60000 (90%)]\t training loss: 0.058809\n",
      "epoch: 12 [54400/60000 (91%)]\t training loss: 0.001641\n",
      "epoch: 12 [54720/60000 (91%)]\t training loss: 0.000049\n",
      "epoch: 12 [55040/60000 (92%)]\t training loss: 0.000290\n",
      "epoch: 12 [55360/60000 (92%)]\t training loss: 0.029680\n",
      "epoch: 12 [55680/60000 (93%)]\t training loss: 0.000145\n",
      "epoch: 12 [56000/60000 (93%)]\t training loss: 0.000512\n",
      "epoch: 12 [56320/60000 (94%)]\t training loss: 0.008264\n",
      "epoch: 12 [56640/60000 (94%)]\t training loss: 0.002706\n",
      "epoch: 12 [56960/60000 (95%)]\t training loss: 0.029824\n",
      "epoch: 12 [57280/60000 (95%)]\t training loss: 0.000312\n",
      "epoch: 12 [57600/60000 (96%)]\t training loss: 0.031071\n",
      "epoch: 12 [57920/60000 (97%)]\t training loss: 0.188848\n",
      "epoch: 12 [58240/60000 (97%)]\t training loss: 0.000750\n",
      "epoch: 12 [58560/60000 (98%)]\t training loss: 0.000769\n",
      "epoch: 12 [58880/60000 (98%)]\t training loss: 0.121311\n",
      "epoch: 12 [59200/60000 (99%)]\t training loss: 0.000386\n",
      "epoch: 12 [59520/60000 (99%)]\t training loss: 0.002916\n",
      "epoch: 12 [59840/60000 (100%)]\t training loss: 0.001112\n",
      "\n",
      "Test dataset: Overall Loss: 0.0395, Overall Accuracy: 9907/10000 (99%)\n",
      "\n",
      "epoch: 13 [0/60000 (0%)]\t training loss: 0.000027\n",
      "epoch: 13 [320/60000 (1%)]\t training loss: 0.000097\n",
      "epoch: 13 [640/60000 (1%)]\t training loss: 0.000070\n",
      "epoch: 13 [960/60000 (2%)]\t training loss: 0.007450\n",
      "epoch: 13 [1280/60000 (2%)]\t training loss: 0.003502\n",
      "epoch: 13 [1600/60000 (3%)]\t training loss: 0.001357\n",
      "epoch: 13 [1920/60000 (3%)]\t training loss: 0.002532\n",
      "epoch: 13 [2240/60000 (4%)]\t training loss: 0.000604\n",
      "epoch: 13 [2560/60000 (4%)]\t training loss: 0.000371\n",
      "epoch: 13 [2880/60000 (5%)]\t training loss: 0.000159\n",
      "epoch: 13 [3200/60000 (5%)]\t training loss: 0.014288\n",
      "epoch: 13 [3520/60000 (6%)]\t training loss: 0.021012\n",
      "epoch: 13 [3840/60000 (6%)]\t training loss: 0.051635\n",
      "epoch: 13 [4160/60000 (7%)]\t training loss: 0.001468\n",
      "epoch: 13 [4480/60000 (7%)]\t training loss: 0.009639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 [4800/60000 (8%)]\t training loss: 0.000720\n",
      "epoch: 13 [5120/60000 (9%)]\t training loss: 0.006018\n",
      "epoch: 13 [5440/60000 (9%)]\t training loss: 0.011282\n",
      "epoch: 13 [5760/60000 (10%)]\t training loss: 0.015014\n",
      "epoch: 13 [6080/60000 (10%)]\t training loss: 0.019038\n",
      "epoch: 13 [6400/60000 (11%)]\t training loss: 0.000303\n",
      "epoch: 13 [6720/60000 (11%)]\t training loss: 0.008882\n",
      "epoch: 13 [7040/60000 (12%)]\t training loss: 0.004318\n",
      "epoch: 13 [7360/60000 (12%)]\t training loss: 0.004444\n",
      "epoch: 13 [7680/60000 (13%)]\t training loss: 0.004412\n",
      "epoch: 13 [8000/60000 (13%)]\t training loss: 0.000613\n",
      "epoch: 13 [8320/60000 (14%)]\t training loss: 0.003034\n",
      "epoch: 13 [8640/60000 (14%)]\t training loss: 0.026215\n",
      "epoch: 13 [8960/60000 (15%)]\t training loss: 0.000137\n",
      "epoch: 13 [9280/60000 (15%)]\t training loss: 0.003974\n",
      "epoch: 13 [9600/60000 (16%)]\t training loss: 0.002260\n",
      "epoch: 13 [9920/60000 (17%)]\t training loss: 0.000534\n",
      "epoch: 13 [10240/60000 (17%)]\t training loss: 0.001399\n",
      "epoch: 13 [10560/60000 (18%)]\t training loss: 0.000019\n",
      "epoch: 13 [10880/60000 (18%)]\t training loss: 0.028468\n",
      "epoch: 13 [11200/60000 (19%)]\t training loss: 0.009639\n",
      "epoch: 13 [11520/60000 (19%)]\t training loss: 0.010405\n",
      "epoch: 13 [11840/60000 (20%)]\t training loss: 0.061757\n",
      "epoch: 13 [12160/60000 (20%)]\t training loss: 0.099560\n",
      "epoch: 13 [12480/60000 (21%)]\t training loss: 0.022167\n",
      "epoch: 13 [12800/60000 (21%)]\t training loss: 0.029892\n",
      "epoch: 13 [13120/60000 (22%)]\t training loss: 0.000919\n",
      "epoch: 13 [13440/60000 (22%)]\t training loss: 0.000527\n",
      "epoch: 13 [13760/60000 (23%)]\t training loss: 0.033922\n",
      "epoch: 13 [14080/60000 (23%)]\t training loss: 0.021888\n",
      "epoch: 13 [14400/60000 (24%)]\t training loss: 0.004926\n",
      "epoch: 13 [14720/60000 (25%)]\t training loss: 0.012083\n",
      "epoch: 13 [15040/60000 (25%)]\t training loss: 0.033613\n",
      "epoch: 13 [15360/60000 (26%)]\t training loss: 0.027092\n",
      "epoch: 13 [15680/60000 (26%)]\t training loss: 0.120228\n",
      "epoch: 13 [16000/60000 (27%)]\t training loss: 0.039099\n",
      "epoch: 13 [16320/60000 (27%)]\t training loss: 0.057185\n",
      "epoch: 13 [16640/60000 (28%)]\t training loss: 0.016847\n",
      "epoch: 13 [16960/60000 (28%)]\t training loss: 0.026875\n",
      "epoch: 13 [17280/60000 (29%)]\t training loss: 0.017811\n",
      "epoch: 13 [17600/60000 (29%)]\t training loss: 0.171953\n",
      "epoch: 13 [17920/60000 (30%)]\t training loss: 0.000350\n",
      "epoch: 13 [18240/60000 (30%)]\t training loss: 0.014519\n",
      "epoch: 13 [18560/60000 (31%)]\t training loss: 0.000248\n",
      "epoch: 13 [18880/60000 (31%)]\t training loss: 0.001158\n",
      "epoch: 13 [19200/60000 (32%)]\t training loss: 0.216686\n",
      "epoch: 13 [19520/60000 (33%)]\t training loss: 0.033701\n",
      "epoch: 13 [19840/60000 (33%)]\t training loss: 0.012387\n",
      "epoch: 13 [20160/60000 (34%)]\t training loss: 0.001081\n",
      "epoch: 13 [20480/60000 (34%)]\t training loss: 0.032313\n",
      "epoch: 13 [20800/60000 (35%)]\t training loss: 0.038007\n",
      "epoch: 13 [21120/60000 (35%)]\t training loss: 0.003692\n",
      "epoch: 13 [21440/60000 (36%)]\t training loss: 0.004395\n",
      "epoch: 13 [21760/60000 (36%)]\t training loss: 0.004543\n",
      "epoch: 13 [22080/60000 (37%)]\t training loss: 0.000214\n",
      "epoch: 13 [22400/60000 (37%)]\t training loss: 0.016640\n",
      "epoch: 13 [22720/60000 (38%)]\t training loss: 0.004085\n",
      "epoch: 13 [23040/60000 (38%)]\t training loss: 0.070229\n",
      "epoch: 13 [23360/60000 (39%)]\t training loss: 0.000093\n",
      "epoch: 13 [23680/60000 (39%)]\t training loss: 0.015319\n",
      "epoch: 13 [24000/60000 (40%)]\t training loss: 0.049543\n",
      "epoch: 13 [24320/60000 (41%)]\t training loss: 0.010040\n",
      "epoch: 13 [24640/60000 (41%)]\t training loss: 0.041145\n",
      "epoch: 13 [24960/60000 (42%)]\t training loss: 0.002526\n",
      "epoch: 13 [25280/60000 (42%)]\t training loss: 0.004136\n",
      "epoch: 13 [25600/60000 (43%)]\t training loss: 0.000680\n",
      "epoch: 13 [25920/60000 (43%)]\t training loss: 0.028493\n",
      "epoch: 13 [26240/60000 (44%)]\t training loss: 0.006291\n",
      "epoch: 13 [26560/60000 (44%)]\t training loss: 0.000438\n",
      "epoch: 13 [26880/60000 (45%)]\t training loss: 0.004307\n",
      "epoch: 13 [27200/60000 (45%)]\t training loss: 0.000299\n",
      "epoch: 13 [27520/60000 (46%)]\t training loss: 0.010188\n",
      "epoch: 13 [27840/60000 (46%)]\t training loss: 0.004829\n",
      "epoch: 13 [28160/60000 (47%)]\t training loss: 0.000098\n",
      "epoch: 13 [28480/60000 (47%)]\t training loss: 0.082649\n",
      "epoch: 13 [28800/60000 (48%)]\t training loss: 0.016217\n",
      "epoch: 13 [29120/60000 (49%)]\t training loss: 0.069786\n",
      "epoch: 13 [29440/60000 (49%)]\t training loss: 0.004911\n",
      "epoch: 13 [29760/60000 (50%)]\t training loss: 0.097376\n",
      "epoch: 13 [30080/60000 (50%)]\t training loss: 0.000745\n",
      "epoch: 13 [30400/60000 (51%)]\t training loss: 0.002359\n",
      "epoch: 13 [30720/60000 (51%)]\t training loss: 0.002090\n",
      "epoch: 13 [31040/60000 (52%)]\t training loss: 0.001535\n",
      "epoch: 13 [31360/60000 (52%)]\t training loss: 0.106575\n",
      "epoch: 13 [31680/60000 (53%)]\t training loss: 0.091099\n",
      "epoch: 13 [32000/60000 (53%)]\t training loss: 0.000289\n",
      "epoch: 13 [32320/60000 (54%)]\t training loss: 0.007002\n",
      "epoch: 13 [32640/60000 (54%)]\t training loss: 0.002776\n",
      "epoch: 13 [32960/60000 (55%)]\t training loss: 0.000626\n",
      "epoch: 13 [33280/60000 (55%)]\t training loss: 0.000005\n",
      "epoch: 13 [33600/60000 (56%)]\t training loss: 0.020628\n",
      "epoch: 13 [33920/60000 (57%)]\t training loss: 0.203734\n",
      "epoch: 13 [34240/60000 (57%)]\t training loss: 0.000107\n",
      "epoch: 13 [34560/60000 (58%)]\t training loss: 0.004953\n",
      "epoch: 13 [34880/60000 (58%)]\t training loss: 0.016294\n",
      "epoch: 13 [35200/60000 (59%)]\t training loss: 0.085139\n",
      "epoch: 13 [35520/60000 (59%)]\t training loss: 0.111312\n",
      "epoch: 13 [35840/60000 (60%)]\t training loss: 0.000145\n",
      "epoch: 13 [36160/60000 (60%)]\t training loss: 0.206877\n",
      "epoch: 13 [36480/60000 (61%)]\t training loss: 0.001888\n",
      "epoch: 13 [36800/60000 (61%)]\t training loss: 0.019681\n",
      "epoch: 13 [37120/60000 (62%)]\t training loss: 0.001462\n",
      "epoch: 13 [37440/60000 (62%)]\t training loss: 0.001952\n",
      "epoch: 13 [37760/60000 (63%)]\t training loss: 0.015552\n",
      "epoch: 13 [38080/60000 (63%)]\t training loss: 0.005421\n",
      "epoch: 13 [38400/60000 (64%)]\t training loss: 0.000481\n",
      "epoch: 13 [38720/60000 (65%)]\t training loss: 0.001245\n",
      "epoch: 13 [39040/60000 (65%)]\t training loss: 0.008045\n",
      "epoch: 13 [39360/60000 (66%)]\t training loss: 0.000917\n",
      "epoch: 13 [39680/60000 (66%)]\t training loss: 0.000012\n",
      "epoch: 13 [40000/60000 (67%)]\t training loss: 0.009825\n",
      "epoch: 13 [40320/60000 (67%)]\t training loss: 0.135260\n",
      "epoch: 13 [40640/60000 (68%)]\t training loss: 0.002793\n",
      "epoch: 13 [40960/60000 (68%)]\t training loss: 0.038935\n",
      "epoch: 13 [41280/60000 (69%)]\t training loss: 0.031167\n",
      "epoch: 13 [41600/60000 (69%)]\t training loss: 0.021430\n",
      "epoch: 13 [41920/60000 (70%)]\t training loss: 0.005615\n",
      "epoch: 13 [42240/60000 (70%)]\t training loss: 0.000227\n",
      "epoch: 13 [42560/60000 (71%)]\t training loss: 0.026576\n",
      "epoch: 13 [42880/60000 (71%)]\t training loss: 0.005486\n",
      "epoch: 13 [43200/60000 (72%)]\t training loss: 0.054950\n",
      "epoch: 13 [43520/60000 (73%)]\t training loss: 0.001213\n",
      "epoch: 13 [43840/60000 (73%)]\t training loss: 0.101203\n",
      "epoch: 13 [44160/60000 (74%)]\t training loss: 0.000647\n",
      "epoch: 13 [44480/60000 (74%)]\t training loss: 0.028073\n",
      "epoch: 13 [44800/60000 (75%)]\t training loss: 0.001180\n",
      "epoch: 13 [45120/60000 (75%)]\t training loss: 0.007432\n",
      "epoch: 13 [45440/60000 (76%)]\t training loss: 0.013614\n",
      "epoch: 13 [45760/60000 (76%)]\t training loss: 0.001406\n",
      "epoch: 13 [46080/60000 (77%)]\t training loss: 0.008026\n",
      "epoch: 13 [46400/60000 (77%)]\t training loss: 0.000643\n",
      "epoch: 13 [46720/60000 (78%)]\t training loss: 0.016172\n",
      "epoch: 13 [47040/60000 (78%)]\t training loss: 0.156116\n",
      "epoch: 13 [47360/60000 (79%)]\t training loss: 0.002903\n",
      "epoch: 13 [47680/60000 (79%)]\t training loss: 0.002684\n",
      "epoch: 13 [48000/60000 (80%)]\t training loss: 0.000943\n",
      "epoch: 13 [48320/60000 (81%)]\t training loss: 0.000143\n",
      "epoch: 13 [48640/60000 (81%)]\t training loss: 0.009535\n",
      "epoch: 13 [48960/60000 (82%)]\t training loss: 0.015896\n",
      "epoch: 13 [49280/60000 (82%)]\t training loss: 0.003929\n",
      "epoch: 13 [49600/60000 (83%)]\t training loss: 0.000897\n",
      "epoch: 13 [49920/60000 (83%)]\t training loss: 0.039790\n",
      "epoch: 13 [50240/60000 (84%)]\t training loss: 0.090821\n",
      "epoch: 13 [50560/60000 (84%)]\t training loss: 0.062376\n",
      "epoch: 13 [50880/60000 (85%)]\t training loss: 0.000112\n",
      "epoch: 13 [51200/60000 (85%)]\t training loss: 0.069859\n",
      "epoch: 13 [51520/60000 (86%)]\t training loss: 0.002432\n",
      "epoch: 13 [51840/60000 (86%)]\t training loss: 0.001600\n",
      "epoch: 13 [52160/60000 (87%)]\t training loss: 0.005799\n",
      "epoch: 13 [52480/60000 (87%)]\t training loss: 0.014081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 [52800/60000 (88%)]\t training loss: 0.000047\n",
      "epoch: 13 [53120/60000 (89%)]\t training loss: 0.004283\n",
      "epoch: 13 [53440/60000 (89%)]\t training loss: 0.000205\n",
      "epoch: 13 [53760/60000 (90%)]\t training loss: 0.039162\n",
      "epoch: 13 [54080/60000 (90%)]\t training loss: 0.002861\n",
      "epoch: 13 [54400/60000 (91%)]\t training loss: 0.010838\n",
      "epoch: 13 [54720/60000 (91%)]\t training loss: 0.050820\n",
      "epoch: 13 [55040/60000 (92%)]\t training loss: 0.000033\n",
      "epoch: 13 [55360/60000 (92%)]\t training loss: 0.185890\n",
      "epoch: 13 [55680/60000 (93%)]\t training loss: 0.000187\n",
      "epoch: 13 [56000/60000 (93%)]\t training loss: 0.059440\n",
      "epoch: 13 [56320/60000 (94%)]\t training loss: 0.000224\n",
      "epoch: 13 [56640/60000 (94%)]\t training loss: 0.015784\n",
      "epoch: 13 [56960/60000 (95%)]\t training loss: 0.002524\n",
      "epoch: 13 [57280/60000 (95%)]\t training loss: 0.001106\n",
      "epoch: 13 [57600/60000 (96%)]\t training loss: 0.000138\n",
      "epoch: 13 [57920/60000 (97%)]\t training loss: 0.071033\n",
      "epoch: 13 [58240/60000 (97%)]\t training loss: 0.000162\n",
      "epoch: 13 [58560/60000 (98%)]\t training loss: 0.251351\n",
      "epoch: 13 [58880/60000 (98%)]\t training loss: 0.074196\n",
      "epoch: 13 [59200/60000 (99%)]\t training loss: 0.006337\n",
      "epoch: 13 [59520/60000 (99%)]\t training loss: 0.108680\n",
      "epoch: 13 [59840/60000 (100%)]\t training loss: 0.006767\n",
      "\n",
      "Test dataset: Overall Loss: 0.0312, Overall Accuracy: 9906/10000 (99%)\n",
      "\n",
      "epoch: 14 [0/60000 (0%)]\t training loss: 0.007436\n",
      "epoch: 14 [320/60000 (1%)]\t training loss: 0.013307\n",
      "epoch: 14 [640/60000 (1%)]\t training loss: 0.036380\n",
      "epoch: 14 [960/60000 (2%)]\t training loss: 0.044910\n",
      "epoch: 14 [1280/60000 (2%)]\t training loss: 0.005226\n",
      "epoch: 14 [1600/60000 (3%)]\t training loss: 0.011532\n",
      "epoch: 14 [1920/60000 (3%)]\t training loss: 0.003157\n",
      "epoch: 14 [2240/60000 (4%)]\t training loss: 0.049436\n",
      "epoch: 14 [2560/60000 (4%)]\t training loss: 0.030668\n",
      "epoch: 14 [2880/60000 (5%)]\t training loss: 0.034601\n",
      "epoch: 14 [3200/60000 (5%)]\t training loss: 0.010932\n",
      "epoch: 14 [3520/60000 (6%)]\t training loss: 0.006631\n",
      "epoch: 14 [3840/60000 (6%)]\t training loss: 0.000068\n",
      "epoch: 14 [4160/60000 (7%)]\t training loss: 0.001833\n",
      "epoch: 14 [4480/60000 (7%)]\t training loss: 0.337854\n",
      "epoch: 14 [4800/60000 (8%)]\t training loss: 0.003951\n",
      "epoch: 14 [5120/60000 (9%)]\t training loss: 0.000107\n",
      "epoch: 14 [5440/60000 (9%)]\t training loss: 0.001521\n",
      "epoch: 14 [5760/60000 (10%)]\t training loss: 0.000451\n",
      "epoch: 14 [6080/60000 (10%)]\t training loss: 0.001913\n",
      "epoch: 14 [6400/60000 (11%)]\t training loss: 0.010715\n",
      "epoch: 14 [6720/60000 (11%)]\t training loss: 0.053027\n",
      "epoch: 14 [7040/60000 (12%)]\t training loss: 0.051065\n",
      "epoch: 14 [7360/60000 (12%)]\t training loss: 0.006301\n",
      "epoch: 14 [7680/60000 (13%)]\t training loss: 0.117269\n",
      "epoch: 14 [8000/60000 (13%)]\t training loss: 0.171936\n",
      "epoch: 14 [8320/60000 (14%)]\t training loss: 0.028258\n",
      "epoch: 14 [8640/60000 (14%)]\t training loss: 0.000437\n",
      "epoch: 14 [8960/60000 (15%)]\t training loss: 0.012990\n",
      "epoch: 14 [9280/60000 (15%)]\t training loss: 0.104412\n",
      "epoch: 14 [9600/60000 (16%)]\t training loss: 0.061471\n",
      "epoch: 14 [9920/60000 (17%)]\t training loss: 0.016925\n",
      "epoch: 14 [10240/60000 (17%)]\t training loss: 0.000895\n",
      "epoch: 14 [10560/60000 (18%)]\t training loss: 0.043917\n",
      "epoch: 14 [10880/60000 (18%)]\t training loss: 0.009998\n",
      "epoch: 14 [11200/60000 (19%)]\t training loss: 0.001743\n",
      "epoch: 14 [11520/60000 (19%)]\t training loss: 0.000981\n",
      "epoch: 14 [11840/60000 (20%)]\t training loss: 0.088029\n",
      "epoch: 14 [12160/60000 (20%)]\t training loss: 0.001265\n",
      "epoch: 14 [12480/60000 (21%)]\t training loss: 0.007321\n",
      "epoch: 14 [12800/60000 (21%)]\t training loss: 0.007502\n",
      "epoch: 14 [13120/60000 (22%)]\t training loss: 0.021593\n",
      "epoch: 14 [13440/60000 (22%)]\t training loss: 0.019287\n",
      "epoch: 14 [13760/60000 (23%)]\t training loss: 0.055461\n",
      "epoch: 14 [14080/60000 (23%)]\t training loss: 0.019372\n",
      "epoch: 14 [14400/60000 (24%)]\t training loss: 0.083153\n",
      "epoch: 14 [14720/60000 (25%)]\t training loss: 0.000093\n",
      "epoch: 14 [15040/60000 (25%)]\t training loss: 0.046384\n",
      "epoch: 14 [15360/60000 (26%)]\t training loss: 0.000142\n",
      "epoch: 14 [15680/60000 (26%)]\t training loss: 0.123363\n",
      "epoch: 14 [16000/60000 (27%)]\t training loss: 0.002543\n",
      "epoch: 14 [16320/60000 (27%)]\t training loss: 0.000348\n",
      "epoch: 14 [16640/60000 (28%)]\t training loss: 0.000003\n",
      "epoch: 14 [16960/60000 (28%)]\t training loss: 0.000634\n",
      "epoch: 14 [17280/60000 (29%)]\t training loss: 0.006262\n",
      "epoch: 14 [17600/60000 (29%)]\t training loss: 0.001303\n",
      "epoch: 14 [17920/60000 (30%)]\t training loss: 0.022463\n",
      "epoch: 14 [18240/60000 (30%)]\t training loss: 0.000560\n",
      "epoch: 14 [18560/60000 (31%)]\t training loss: 0.030337\n",
      "epoch: 14 [18880/60000 (31%)]\t training loss: 0.000400\n",
      "epoch: 14 [19200/60000 (32%)]\t training loss: 0.000624\n",
      "epoch: 14 [19520/60000 (33%)]\t training loss: 0.100759\n",
      "epoch: 14 [19840/60000 (33%)]\t training loss: 0.001694\n",
      "epoch: 14 [20160/60000 (34%)]\t training loss: 0.001608\n",
      "epoch: 14 [20480/60000 (34%)]\t training loss: 0.014183\n",
      "epoch: 14 [20800/60000 (35%)]\t training loss: 0.000115\n",
      "epoch: 14 [21120/60000 (35%)]\t training loss: 0.047295\n",
      "epoch: 14 [21440/60000 (36%)]\t training loss: 0.013736\n",
      "epoch: 14 [21760/60000 (36%)]\t training loss: 0.000321\n",
      "epoch: 14 [22080/60000 (37%)]\t training loss: 0.000442\n",
      "epoch: 14 [22400/60000 (37%)]\t training loss: 0.000217\n",
      "epoch: 14 [22720/60000 (38%)]\t training loss: 0.000290\n",
      "epoch: 14 [23040/60000 (38%)]\t training loss: 0.027857\n",
      "epoch: 14 [23360/60000 (39%)]\t training loss: 0.088745\n",
      "epoch: 14 [23680/60000 (39%)]\t training loss: 0.000704\n",
      "epoch: 14 [24000/60000 (40%)]\t training loss: 0.079915\n",
      "epoch: 14 [24320/60000 (41%)]\t training loss: 0.024369\n",
      "epoch: 14 [24640/60000 (41%)]\t training loss: 0.000368\n",
      "epoch: 14 [24960/60000 (42%)]\t training loss: 0.013698\n",
      "epoch: 14 [25280/60000 (42%)]\t training loss: 0.031828\n",
      "epoch: 14 [25600/60000 (43%)]\t training loss: 0.000030\n",
      "epoch: 14 [25920/60000 (43%)]\t training loss: 0.000034\n",
      "epoch: 14 [26240/60000 (44%)]\t training loss: 0.000003\n",
      "epoch: 14 [26560/60000 (44%)]\t training loss: 0.629139\n",
      "epoch: 14 [26880/60000 (45%)]\t training loss: 0.008120\n",
      "epoch: 14 [27200/60000 (45%)]\t training loss: 0.004943\n",
      "epoch: 14 [27520/60000 (46%)]\t training loss: 0.004882\n",
      "epoch: 14 [27840/60000 (46%)]\t training loss: 0.021346\n",
      "epoch: 14 [28160/60000 (47%)]\t training loss: 0.044625\n",
      "epoch: 14 [28480/60000 (47%)]\t training loss: 0.008501\n",
      "epoch: 14 [28800/60000 (48%)]\t training loss: 0.273605\n",
      "epoch: 14 [29120/60000 (49%)]\t training loss: 0.000436\n",
      "epoch: 14 [29440/60000 (49%)]\t training loss: 0.001874\n",
      "epoch: 14 [29760/60000 (50%)]\t training loss: 0.178664\n",
      "epoch: 14 [30080/60000 (50%)]\t training loss: 0.111766\n",
      "epoch: 14 [30400/60000 (51%)]\t training loss: 0.000371\n",
      "epoch: 14 [30720/60000 (51%)]\t training loss: 0.002038\n",
      "epoch: 14 [31040/60000 (52%)]\t training loss: 0.046936\n",
      "epoch: 14 [31360/60000 (52%)]\t training loss: 0.006243\n",
      "epoch: 14 [31680/60000 (53%)]\t training loss: 0.014003\n",
      "epoch: 14 [32000/60000 (53%)]\t training loss: 0.018737\n",
      "epoch: 14 [32320/60000 (54%)]\t training loss: 0.011526\n",
      "epoch: 14 [32640/60000 (54%)]\t training loss: 0.145697\n",
      "epoch: 14 [32960/60000 (55%)]\t training loss: 0.070579\n",
      "epoch: 14 [33280/60000 (55%)]\t training loss: 0.001343\n",
      "epoch: 14 [33600/60000 (56%)]\t training loss: 0.000079\n",
      "epoch: 14 [33920/60000 (57%)]\t training loss: 0.009956\n",
      "epoch: 14 [34240/60000 (57%)]\t training loss: 0.045966\n",
      "epoch: 14 [34560/60000 (58%)]\t training loss: 0.000311\n",
      "epoch: 14 [34880/60000 (58%)]\t training loss: 0.000001\n",
      "epoch: 14 [35200/60000 (59%)]\t training loss: 0.000039\n",
      "epoch: 14 [35520/60000 (59%)]\t training loss: 0.228663\n",
      "epoch: 14 [35840/60000 (60%)]\t training loss: 0.000919\n",
      "epoch: 14 [36160/60000 (60%)]\t training loss: 0.003103\n",
      "epoch: 14 [36480/60000 (61%)]\t training loss: 0.021350\n",
      "epoch: 14 [36800/60000 (61%)]\t training loss: 0.221952\n",
      "epoch: 14 [37120/60000 (62%)]\t training loss: 0.016870\n",
      "epoch: 14 [37440/60000 (62%)]\t training loss: 0.001197\n",
      "epoch: 14 [37760/60000 (63%)]\t training loss: 0.000104\n",
      "epoch: 14 [38080/60000 (63%)]\t training loss: 0.002258\n",
      "epoch: 14 [38400/60000 (64%)]\t training loss: 0.000005\n",
      "epoch: 14 [38720/60000 (65%)]\t training loss: 0.000064\n",
      "epoch: 14 [39040/60000 (65%)]\t training loss: 0.015580\n",
      "epoch: 14 [39360/60000 (66%)]\t training loss: 0.019824\n",
      "epoch: 14 [39680/60000 (66%)]\t training loss: 0.003044\n",
      "epoch: 14 [40000/60000 (67%)]\t training loss: 0.000982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 [40320/60000 (67%)]\t training loss: 0.021639\n",
      "epoch: 14 [40640/60000 (68%)]\t training loss: 0.002966\n",
      "epoch: 14 [40960/60000 (68%)]\t training loss: 0.007757\n",
      "epoch: 14 [41280/60000 (69%)]\t training loss: 0.002163\n",
      "epoch: 14 [41600/60000 (69%)]\t training loss: 0.029250\n",
      "epoch: 14 [41920/60000 (70%)]\t training loss: 0.049484\n",
      "epoch: 14 [42240/60000 (70%)]\t training loss: 0.000030\n",
      "epoch: 14 [42560/60000 (71%)]\t training loss: 0.000092\n",
      "epoch: 14 [42880/60000 (71%)]\t training loss: 0.137855\n",
      "epoch: 14 [43200/60000 (72%)]\t training loss: 0.004490\n",
      "epoch: 14 [43520/60000 (73%)]\t training loss: 0.003624\n",
      "epoch: 14 [43840/60000 (73%)]\t training loss: 0.042385\n",
      "epoch: 14 [44160/60000 (74%)]\t training loss: 0.002559\n",
      "epoch: 14 [44480/60000 (74%)]\t training loss: 0.031025\n",
      "epoch: 14 [44800/60000 (75%)]\t training loss: 0.000239\n",
      "epoch: 14 [45120/60000 (75%)]\t training loss: 0.436764\n",
      "epoch: 14 [45440/60000 (76%)]\t training loss: 0.007362\n",
      "epoch: 14 [45760/60000 (76%)]\t training loss: 0.001524\n",
      "epoch: 14 [46080/60000 (77%)]\t training loss: 0.011160\n",
      "epoch: 14 [46400/60000 (77%)]\t training loss: 0.000022\n",
      "epoch: 14 [46720/60000 (78%)]\t training loss: 0.051415\n",
      "epoch: 14 [47040/60000 (78%)]\t training loss: 0.002727\n",
      "epoch: 14 [47360/60000 (79%)]\t training loss: 0.005272\n",
      "epoch: 14 [47680/60000 (79%)]\t training loss: 0.033647\n",
      "epoch: 14 [48000/60000 (80%)]\t training loss: 0.164675\n",
      "epoch: 14 [48320/60000 (81%)]\t training loss: 0.000983\n",
      "epoch: 14 [48640/60000 (81%)]\t training loss: 0.032289\n",
      "epoch: 14 [48960/60000 (82%)]\t training loss: 0.073112\n",
      "epoch: 14 [49280/60000 (82%)]\t training loss: 0.005658\n",
      "epoch: 14 [49600/60000 (83%)]\t training loss: 0.049480\n",
      "epoch: 14 [49920/60000 (83%)]\t training loss: 0.000503\n",
      "epoch: 14 [50240/60000 (84%)]\t training loss: 0.141682\n",
      "epoch: 14 [50560/60000 (84%)]\t training loss: 0.019214\n",
      "epoch: 14 [50880/60000 (85%)]\t training loss: 0.009628\n",
      "epoch: 14 [51200/60000 (85%)]\t training loss: 0.011202\n",
      "epoch: 14 [51520/60000 (86%)]\t training loss: 0.000525\n",
      "epoch: 14 [51840/60000 (86%)]\t training loss: 0.007805\n",
      "epoch: 14 [52160/60000 (87%)]\t training loss: 0.262596\n",
      "epoch: 14 [52480/60000 (87%)]\t training loss: 0.002927\n",
      "epoch: 14 [52800/60000 (88%)]\t training loss: 0.000431\n",
      "epoch: 14 [53120/60000 (89%)]\t training loss: 0.028613\n",
      "epoch: 14 [53440/60000 (89%)]\t training loss: 0.000139\n",
      "epoch: 14 [53760/60000 (90%)]\t training loss: 0.000539\n",
      "epoch: 14 [54080/60000 (90%)]\t training loss: 0.000666\n",
      "epoch: 14 [54400/60000 (91%)]\t training loss: 0.001736\n",
      "epoch: 14 [54720/60000 (91%)]\t training loss: 0.033785\n",
      "epoch: 14 [55040/60000 (92%)]\t training loss: 0.000058\n",
      "epoch: 14 [55360/60000 (92%)]\t training loss: 0.077622\n",
      "epoch: 14 [55680/60000 (93%)]\t training loss: 0.098146\n",
      "epoch: 14 [56000/60000 (93%)]\t training loss: 0.000283\n",
      "epoch: 14 [56320/60000 (94%)]\t training loss: 0.004017\n",
      "epoch: 14 [56640/60000 (94%)]\t training loss: 0.025712\n",
      "epoch: 14 [56960/60000 (95%)]\t training loss: 0.040310\n",
      "epoch: 14 [57280/60000 (95%)]\t training loss: 0.000505\n",
      "epoch: 14 [57600/60000 (96%)]\t training loss: 0.002795\n",
      "epoch: 14 [57920/60000 (97%)]\t training loss: 0.000301\n",
      "epoch: 14 [58240/60000 (97%)]\t training loss: 0.004030\n",
      "epoch: 14 [58560/60000 (98%)]\t training loss: 0.255352\n",
      "epoch: 14 [58880/60000 (98%)]\t training loss: 0.000060\n",
      "epoch: 14 [59200/60000 (99%)]\t training loss: 0.065508\n",
      "epoch: 14 [59520/60000 (99%)]\t training loss: 0.000426\n",
      "epoch: 14 [59840/60000 (100%)]\t training loss: 0.006963\n",
      "\n",
      "Test dataset: Overall Loss: 0.0405, Overall Accuracy: 9913/10000 (99%)\n",
      "\n",
      "epoch: 15 [0/60000 (0%)]\t training loss: 0.000040\n",
      "epoch: 15 [320/60000 (1%)]\t training loss: 0.168294\n",
      "epoch: 15 [640/60000 (1%)]\t training loss: 0.000314\n",
      "epoch: 15 [960/60000 (2%)]\t training loss: 0.001143\n",
      "epoch: 15 [1280/60000 (2%)]\t training loss: 0.070446\n",
      "epoch: 15 [1600/60000 (3%)]\t training loss: 0.001573\n",
      "epoch: 15 [1920/60000 (3%)]\t training loss: 0.011107\n",
      "epoch: 15 [2240/60000 (4%)]\t training loss: 0.006162\n",
      "epoch: 15 [2560/60000 (4%)]\t training loss: 0.001046\n",
      "epoch: 15 [2880/60000 (5%)]\t training loss: 0.000398\n",
      "epoch: 15 [3200/60000 (5%)]\t training loss: 0.000651\n",
      "epoch: 15 [3520/60000 (6%)]\t training loss: 0.075122\n",
      "epoch: 15 [3840/60000 (6%)]\t training loss: 0.000101\n",
      "epoch: 15 [4160/60000 (7%)]\t training loss: 0.000916\n",
      "epoch: 15 [4480/60000 (7%)]\t training loss: 0.001411\n",
      "epoch: 15 [4800/60000 (8%)]\t training loss: 0.010489\n",
      "epoch: 15 [5120/60000 (9%)]\t training loss: 0.417966\n",
      "epoch: 15 [5440/60000 (9%)]\t training loss: 0.048360\n",
      "epoch: 15 [5760/60000 (10%)]\t training loss: 0.023302\n",
      "epoch: 15 [6080/60000 (10%)]\t training loss: 0.016600\n",
      "epoch: 15 [6400/60000 (11%)]\t training loss: 0.016312\n",
      "epoch: 15 [6720/60000 (11%)]\t training loss: 0.007172\n",
      "epoch: 15 [7040/60000 (12%)]\t training loss: 0.003058\n",
      "epoch: 15 [7360/60000 (12%)]\t training loss: 0.046975\n",
      "epoch: 15 [7680/60000 (13%)]\t training loss: 0.000220\n",
      "epoch: 15 [8000/60000 (13%)]\t training loss: 0.001226\n",
      "epoch: 15 [8320/60000 (14%)]\t training loss: 0.021266\n",
      "epoch: 15 [8640/60000 (14%)]\t training loss: 0.000020\n",
      "epoch: 15 [8960/60000 (15%)]\t training loss: 0.000001\n",
      "epoch: 15 [9280/60000 (15%)]\t training loss: 0.001706\n",
      "epoch: 15 [9600/60000 (16%)]\t training loss: 0.001010\n",
      "epoch: 15 [9920/60000 (17%)]\t training loss: 0.001027\n",
      "epoch: 15 [10240/60000 (17%)]\t training loss: 0.000312\n",
      "epoch: 15 [10560/60000 (18%)]\t training loss: 0.000575\n",
      "epoch: 15 [10880/60000 (18%)]\t training loss: 0.001239\n",
      "epoch: 15 [11200/60000 (19%)]\t training loss: 0.596335\n",
      "epoch: 15 [11520/60000 (19%)]\t training loss: 0.003829\n",
      "epoch: 15 [11840/60000 (20%)]\t training loss: 0.000918\n",
      "epoch: 15 [12160/60000 (20%)]\t training loss: 0.000364\n",
      "epoch: 15 [12480/60000 (21%)]\t training loss: 0.007574\n",
      "epoch: 15 [12800/60000 (21%)]\t training loss: 0.001520\n",
      "epoch: 15 [13120/60000 (22%)]\t training loss: 0.000151\n",
      "epoch: 15 [13440/60000 (22%)]\t training loss: 0.141952\n",
      "epoch: 15 [13760/60000 (23%)]\t training loss: 0.001773\n",
      "epoch: 15 [14080/60000 (23%)]\t training loss: 0.030572\n",
      "epoch: 15 [14400/60000 (24%)]\t training loss: 0.016360\n",
      "epoch: 15 [14720/60000 (25%)]\t training loss: 0.000098\n",
      "epoch: 15 [15040/60000 (25%)]\t training loss: 0.000053\n",
      "epoch: 15 [15360/60000 (26%)]\t training loss: 0.000240\n",
      "epoch: 15 [15680/60000 (26%)]\t training loss: 0.001235\n",
      "epoch: 15 [16000/60000 (27%)]\t training loss: 0.000453\n",
      "epoch: 15 [16320/60000 (27%)]\t training loss: 0.001897\n",
      "epoch: 15 [16640/60000 (28%)]\t training loss: 0.016185\n",
      "epoch: 15 [16960/60000 (28%)]\t training loss: 0.015774\n",
      "epoch: 15 [17280/60000 (29%)]\t training loss: 0.001937\n",
      "epoch: 15 [17600/60000 (29%)]\t training loss: 0.000328\n",
      "epoch: 15 [17920/60000 (30%)]\t training loss: 0.000134\n",
      "epoch: 15 [18240/60000 (30%)]\t training loss: 0.002873\n",
      "epoch: 15 [18560/60000 (31%)]\t training loss: 0.007864\n",
      "epoch: 15 [18880/60000 (31%)]\t training loss: 0.001877\n",
      "epoch: 15 [19200/60000 (32%)]\t training loss: 0.064067\n",
      "epoch: 15 [19520/60000 (33%)]\t training loss: 0.000015\n",
      "epoch: 15 [19840/60000 (33%)]\t training loss: 0.001877\n",
      "epoch: 15 [20160/60000 (34%)]\t training loss: 0.017962\n",
      "epoch: 15 [20480/60000 (34%)]\t training loss: 0.000006\n",
      "epoch: 15 [20800/60000 (35%)]\t training loss: 0.047322\n",
      "epoch: 15 [21120/60000 (35%)]\t training loss: 0.000093\n",
      "epoch: 15 [21440/60000 (36%)]\t training loss: 0.000169\n",
      "epoch: 15 [21760/60000 (36%)]\t training loss: 0.000682\n",
      "epoch: 15 [22080/60000 (37%)]\t training loss: 0.006170\n",
      "epoch: 15 [22400/60000 (37%)]\t training loss: 0.007750\n",
      "epoch: 15 [22720/60000 (38%)]\t training loss: 0.008448\n",
      "epoch: 15 [23040/60000 (38%)]\t training loss: 0.001827\n",
      "epoch: 15 [23360/60000 (39%)]\t training loss: 0.000081\n",
      "epoch: 15 [23680/60000 (39%)]\t training loss: 0.000049\n",
      "epoch: 15 [24000/60000 (40%)]\t training loss: 0.001421\n",
      "epoch: 15 [24320/60000 (41%)]\t training loss: 0.000050\n",
      "epoch: 15 [24640/60000 (41%)]\t training loss: 0.139942\n",
      "epoch: 15 [24960/60000 (42%)]\t training loss: 0.002123\n",
      "epoch: 15 [25280/60000 (42%)]\t training loss: 0.013404\n",
      "epoch: 15 [25600/60000 (43%)]\t training loss: 0.067978\n",
      "epoch: 15 [25920/60000 (43%)]\t training loss: 0.163780\n",
      "epoch: 15 [26240/60000 (44%)]\t training loss: 0.057058\n",
      "epoch: 15 [26560/60000 (44%)]\t training loss: 0.018943\n",
      "epoch: 15 [26880/60000 (45%)]\t training loss: 0.001293\n",
      "epoch: 15 [27200/60000 (45%)]\t training loss: 0.142372\n",
      "epoch: 15 [27520/60000 (46%)]\t training loss: 0.038375\n",
      "epoch: 15 [27840/60000 (46%)]\t training loss: 0.000243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 [28160/60000 (47%)]\t training loss: 0.016440\n",
      "epoch: 15 [28480/60000 (47%)]\t training loss: 0.109084\n",
      "epoch: 15 [28800/60000 (48%)]\t training loss: 0.079342\n",
      "epoch: 15 [29120/60000 (49%)]\t training loss: 0.000022\n",
      "epoch: 15 [29440/60000 (49%)]\t training loss: 0.061371\n",
      "epoch: 15 [29760/60000 (50%)]\t training loss: 0.002196\n",
      "epoch: 15 [30080/60000 (50%)]\t training loss: 0.002558\n",
      "epoch: 15 [30400/60000 (51%)]\t training loss: 0.023348\n",
      "epoch: 15 [30720/60000 (51%)]\t training loss: 0.002387\n",
      "epoch: 15 [31040/60000 (52%)]\t training loss: 0.002678\n",
      "epoch: 15 [31360/60000 (52%)]\t training loss: 0.000169\n",
      "epoch: 15 [31680/60000 (53%)]\t training loss: 0.001035\n",
      "epoch: 15 [32000/60000 (53%)]\t training loss: 0.003888\n",
      "epoch: 15 [32320/60000 (54%)]\t training loss: 0.002187\n",
      "epoch: 15 [32640/60000 (54%)]\t training loss: 0.007836\n",
      "epoch: 15 [32960/60000 (55%)]\t training loss: 0.083940\n",
      "epoch: 15 [33280/60000 (55%)]\t training loss: 0.012527\n",
      "epoch: 15 [33600/60000 (56%)]\t training loss: 0.000313\n",
      "epoch: 15 [33920/60000 (57%)]\t training loss: 0.009459\n",
      "epoch: 15 [34240/60000 (57%)]\t training loss: 0.002199\n",
      "epoch: 15 [34560/60000 (58%)]\t training loss: 0.000435\n",
      "epoch: 15 [34880/60000 (58%)]\t training loss: 0.001862\n",
      "epoch: 15 [35200/60000 (59%)]\t training loss: 0.038683\n",
      "epoch: 15 [35520/60000 (59%)]\t training loss: 0.258905\n",
      "epoch: 15 [35840/60000 (60%)]\t training loss: 0.032670\n",
      "epoch: 15 [36160/60000 (60%)]\t training loss: 0.012257\n",
      "epoch: 15 [36480/60000 (61%)]\t training loss: 0.013431\n",
      "epoch: 15 [36800/60000 (61%)]\t training loss: 0.001407\n",
      "epoch: 15 [37120/60000 (62%)]\t training loss: 0.018678\n",
      "epoch: 15 [37440/60000 (62%)]\t training loss: 0.000208\n",
      "epoch: 15 [37760/60000 (63%)]\t training loss: 0.001376\n",
      "epoch: 15 [38080/60000 (63%)]\t training loss: 0.001932\n",
      "epoch: 15 [38400/60000 (64%)]\t training loss: 0.000024\n",
      "epoch: 15 [38720/60000 (65%)]\t training loss: 0.001227\n",
      "epoch: 15 [39040/60000 (65%)]\t training loss: 0.032869\n",
      "epoch: 15 [39360/60000 (66%)]\t training loss: 0.324798\n",
      "epoch: 15 [39680/60000 (66%)]\t training loss: 0.057859\n",
      "epoch: 15 [40000/60000 (67%)]\t training loss: 0.000228\n",
      "epoch: 15 [40320/60000 (67%)]\t training loss: 0.003301\n",
      "epoch: 15 [40640/60000 (68%)]\t training loss: 0.000015\n",
      "epoch: 15 [40960/60000 (68%)]\t training loss: 0.000122\n",
      "epoch: 15 [41280/60000 (69%)]\t training loss: 0.004836\n",
      "epoch: 15 [41600/60000 (69%)]\t training loss: 0.006334\n",
      "epoch: 15 [41920/60000 (70%)]\t training loss: 0.007172\n",
      "epoch: 15 [42240/60000 (70%)]\t training loss: 0.029098\n",
      "epoch: 15 [42560/60000 (71%)]\t training loss: 0.006231\n",
      "epoch: 15 [42880/60000 (71%)]\t training loss: 0.027125\n",
      "epoch: 15 [43200/60000 (72%)]\t training loss: 0.028042\n",
      "epoch: 15 [43520/60000 (73%)]\t training loss: 0.083419\n",
      "epoch: 15 [43840/60000 (73%)]\t training loss: 0.002240\n",
      "epoch: 15 [44160/60000 (74%)]\t training loss: 0.036758\n",
      "epoch: 15 [44480/60000 (74%)]\t training loss: 0.001749\n",
      "epoch: 15 [44800/60000 (75%)]\t training loss: 0.000244\n",
      "epoch: 15 [45120/60000 (75%)]\t training loss: 0.051631\n",
      "epoch: 15 [45440/60000 (76%)]\t training loss: 0.000081\n",
      "epoch: 15 [45760/60000 (76%)]\t training loss: 0.002115\n",
      "epoch: 15 [46080/60000 (77%)]\t training loss: 0.004793\n",
      "epoch: 15 [46400/60000 (77%)]\t training loss: 0.000382\n",
      "epoch: 15 [46720/60000 (78%)]\t training loss: 0.014863\n",
      "epoch: 15 [47040/60000 (78%)]\t training loss: 0.028102\n",
      "epoch: 15 [47360/60000 (79%)]\t training loss: 0.004683\n",
      "epoch: 15 [47680/60000 (79%)]\t training loss: 0.000014\n",
      "epoch: 15 [48000/60000 (80%)]\t training loss: 0.000540\n",
      "epoch: 15 [48320/60000 (81%)]\t training loss: 0.284451\n",
      "epoch: 15 [48640/60000 (81%)]\t training loss: 0.004994\n",
      "epoch: 15 [48960/60000 (82%)]\t training loss: 0.000928\n",
      "epoch: 15 [49280/60000 (82%)]\t training loss: 0.010233\n",
      "epoch: 15 [49600/60000 (83%)]\t training loss: 0.000719\n",
      "epoch: 15 [49920/60000 (83%)]\t training loss: 0.000056\n",
      "epoch: 15 [50240/60000 (84%)]\t training loss: 0.095977\n",
      "epoch: 15 [50560/60000 (84%)]\t training loss: 0.001486\n",
      "epoch: 15 [50880/60000 (85%)]\t training loss: 0.000370\n",
      "epoch: 15 [51200/60000 (85%)]\t training loss: 0.000330\n",
      "epoch: 15 [51520/60000 (86%)]\t training loss: 0.018972\n",
      "epoch: 15 [51840/60000 (86%)]\t training loss: 0.142267\n",
      "epoch: 15 [52160/60000 (87%)]\t training loss: 0.034830\n",
      "epoch: 15 [52480/60000 (87%)]\t training loss: 0.073410\n",
      "epoch: 15 [52800/60000 (88%)]\t training loss: 0.013091\n",
      "epoch: 15 [53120/60000 (89%)]\t training loss: 0.000014\n",
      "epoch: 15 [53440/60000 (89%)]\t training loss: 0.018136\n",
      "epoch: 15 [53760/60000 (90%)]\t training loss: 0.006536\n",
      "epoch: 15 [54080/60000 (90%)]\t training loss: 0.000144\n",
      "epoch: 15 [54400/60000 (91%)]\t training loss: 0.000819\n",
      "epoch: 15 [54720/60000 (91%)]\t training loss: 0.000010\n",
      "epoch: 15 [55040/60000 (92%)]\t training loss: 0.000125\n",
      "epoch: 15 [55360/60000 (92%)]\t training loss: 0.000031\n",
      "epoch: 15 [55680/60000 (93%)]\t training loss: 0.005968\n",
      "epoch: 15 [56000/60000 (93%)]\t training loss: 0.011014\n",
      "epoch: 15 [56320/60000 (94%)]\t training loss: 0.005867\n",
      "epoch: 15 [56640/60000 (94%)]\t training loss: 0.000450\n",
      "epoch: 15 [56960/60000 (95%)]\t training loss: 0.002438\n",
      "epoch: 15 [57280/60000 (95%)]\t training loss: 0.010259\n",
      "epoch: 15 [57600/60000 (96%)]\t training loss: 0.000701\n",
      "epoch: 15 [57920/60000 (97%)]\t training loss: 0.001001\n",
      "epoch: 15 [58240/60000 (97%)]\t training loss: 0.063988\n",
      "epoch: 15 [58560/60000 (98%)]\t training loss: 0.000220\n",
      "epoch: 15 [58880/60000 (98%)]\t training loss: 0.063388\n",
      "epoch: 15 [59200/60000 (99%)]\t training loss: 0.101931\n",
      "epoch: 15 [59520/60000 (99%)]\t training loss: 0.008968\n",
      "epoch: 15 [59840/60000 (100%)]\t training loss: 0.003786\n",
      "\n",
      "Test dataset: Overall Loss: 0.0305, Overall Accuracy: 9912/10000 (99%)\n",
      "\n",
      "epoch: 16 [0/60000 (0%)]\t training loss: 0.010511\n",
      "epoch: 16 [320/60000 (1%)]\t training loss: 0.039953\n",
      "epoch: 16 [640/60000 (1%)]\t training loss: 0.000418\n",
      "epoch: 16 [960/60000 (2%)]\t training loss: 0.000750\n",
      "epoch: 16 [1280/60000 (2%)]\t training loss: 0.000038\n",
      "epoch: 16 [1600/60000 (3%)]\t training loss: 0.000058\n",
      "epoch: 16 [1920/60000 (3%)]\t training loss: 0.006279\n",
      "epoch: 16 [2240/60000 (4%)]\t training loss: 0.000887\n",
      "epoch: 16 [2560/60000 (4%)]\t training loss: 0.000055\n",
      "epoch: 16 [2880/60000 (5%)]\t training loss: 0.003231\n",
      "epoch: 16 [3200/60000 (5%)]\t training loss: 0.009491\n",
      "epoch: 16 [3520/60000 (6%)]\t training loss: 0.000121\n",
      "epoch: 16 [3840/60000 (6%)]\t training loss: 0.017352\n",
      "epoch: 16 [4160/60000 (7%)]\t training loss: 0.000339\n",
      "epoch: 16 [4480/60000 (7%)]\t training loss: 0.002524\n",
      "epoch: 16 [4800/60000 (8%)]\t training loss: 0.000263\n",
      "epoch: 16 [5120/60000 (9%)]\t training loss: 0.000706\n",
      "epoch: 16 [5440/60000 (9%)]\t training loss: 0.003165\n",
      "epoch: 16 [5760/60000 (10%)]\t training loss: 0.010340\n",
      "epoch: 16 [6080/60000 (10%)]\t training loss: 0.001250\n",
      "epoch: 16 [6400/60000 (11%)]\t training loss: 0.000187\n",
      "epoch: 16 [6720/60000 (11%)]\t training loss: 0.000627\n",
      "epoch: 16 [7040/60000 (12%)]\t training loss: 0.006077\n",
      "epoch: 16 [7360/60000 (12%)]\t training loss: 0.211843\n",
      "epoch: 16 [7680/60000 (13%)]\t training loss: 0.000409\n",
      "epoch: 16 [8000/60000 (13%)]\t training loss: 0.089772\n",
      "epoch: 16 [8320/60000 (14%)]\t training loss: 0.000075\n",
      "epoch: 16 [8640/60000 (14%)]\t training loss: 0.168192\n",
      "epoch: 16 [8960/60000 (15%)]\t training loss: 0.025308\n",
      "epoch: 16 [9280/60000 (15%)]\t training loss: 0.006641\n",
      "epoch: 16 [9600/60000 (16%)]\t training loss: 0.077345\n",
      "epoch: 16 [9920/60000 (17%)]\t training loss: 0.010840\n",
      "epoch: 16 [10240/60000 (17%)]\t training loss: 0.007155\n",
      "epoch: 16 [10560/60000 (18%)]\t training loss: 0.330912\n",
      "epoch: 16 [10880/60000 (18%)]\t training loss: 0.040444\n",
      "epoch: 16 [11200/60000 (19%)]\t training loss: 0.005372\n",
      "epoch: 16 [11520/60000 (19%)]\t training loss: 0.000858\n",
      "epoch: 16 [11840/60000 (20%)]\t training loss: 0.038862\n",
      "epoch: 16 [12160/60000 (20%)]\t training loss: 0.000576\n",
      "epoch: 16 [12480/60000 (21%)]\t training loss: 0.012765\n",
      "epoch: 16 [12800/60000 (21%)]\t training loss: 0.000022\n",
      "epoch: 16 [13120/60000 (22%)]\t training loss: 0.003941\n",
      "epoch: 16 [13440/60000 (22%)]\t training loss: 0.000437\n",
      "epoch: 16 [13760/60000 (23%)]\t training loss: 0.001558\n",
      "epoch: 16 [14080/60000 (23%)]\t training loss: 0.001095\n",
      "epoch: 16 [14400/60000 (24%)]\t training loss: 0.003276\n",
      "epoch: 16 [14720/60000 (25%)]\t training loss: 0.060042\n",
      "epoch: 16 [15040/60000 (25%)]\t training loss: 0.002952\n",
      "epoch: 16 [15360/60000 (26%)]\t training loss: 0.049230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 [15680/60000 (26%)]\t training loss: 0.000421\n",
      "epoch: 16 [16000/60000 (27%)]\t training loss: 0.000206\n",
      "epoch: 16 [16320/60000 (27%)]\t training loss: 0.000862\n",
      "epoch: 16 [16640/60000 (28%)]\t training loss: 0.001722\n",
      "epoch: 16 [16960/60000 (28%)]\t training loss: 0.000014\n",
      "epoch: 16 [17280/60000 (29%)]\t training loss: 0.025071\n",
      "epoch: 16 [17600/60000 (29%)]\t training loss: 0.000786\n",
      "epoch: 16 [17920/60000 (30%)]\t training loss: 0.000176\n",
      "epoch: 16 [18240/60000 (30%)]\t training loss: 0.006509\n",
      "epoch: 16 [18560/60000 (31%)]\t training loss: 0.392423\n",
      "epoch: 16 [18880/60000 (31%)]\t training loss: 0.001598\n",
      "epoch: 16 [19200/60000 (32%)]\t training loss: 0.122854\n",
      "epoch: 16 [19520/60000 (33%)]\t training loss: 0.065816\n",
      "epoch: 16 [19840/60000 (33%)]\t training loss: 0.001760\n",
      "epoch: 16 [20160/60000 (34%)]\t training loss: 0.007275\n",
      "epoch: 16 [20480/60000 (34%)]\t training loss: 0.013118\n",
      "epoch: 16 [20800/60000 (35%)]\t training loss: 0.000760\n",
      "epoch: 16 [21120/60000 (35%)]\t training loss: 0.107477\n",
      "epoch: 16 [21440/60000 (36%)]\t training loss: 0.139356\n",
      "epoch: 16 [21760/60000 (36%)]\t training loss: 0.000806\n",
      "epoch: 16 [22080/60000 (37%)]\t training loss: 0.000253\n",
      "epoch: 16 [22400/60000 (37%)]\t training loss: 0.003253\n",
      "epoch: 16 [22720/60000 (38%)]\t training loss: 0.000018\n",
      "epoch: 16 [23040/60000 (38%)]\t training loss: 0.020033\n",
      "epoch: 16 [23360/60000 (39%)]\t training loss: 0.000882\n",
      "epoch: 16 [23680/60000 (39%)]\t training loss: 0.000353\n",
      "epoch: 16 [24000/60000 (40%)]\t training loss: 0.001737\n",
      "epoch: 16 [24320/60000 (41%)]\t training loss: 0.365901\n",
      "epoch: 16 [24640/60000 (41%)]\t training loss: 0.000414\n",
      "epoch: 16 [24960/60000 (42%)]\t training loss: 0.116879\n",
      "epoch: 16 [25280/60000 (42%)]\t training loss: 0.000015\n",
      "epoch: 16 [25600/60000 (43%)]\t training loss: 0.000875\n",
      "epoch: 16 [25920/60000 (43%)]\t training loss: 0.000417\n",
      "epoch: 16 [26240/60000 (44%)]\t training loss: 0.000113\n",
      "epoch: 16 [26560/60000 (44%)]\t training loss: 0.005994\n",
      "epoch: 16 [26880/60000 (45%)]\t training loss: 0.000135\n",
      "epoch: 16 [27200/60000 (45%)]\t training loss: 0.000107\n",
      "epoch: 16 [27520/60000 (46%)]\t training loss: 0.010460\n",
      "epoch: 16 [27840/60000 (46%)]\t training loss: 0.041539\n",
      "epoch: 16 [28160/60000 (47%)]\t training loss: 0.245179\n",
      "epoch: 16 [28480/60000 (47%)]\t training loss: 0.064006\n",
      "epoch: 16 [28800/60000 (48%)]\t training loss: 0.000218\n",
      "epoch: 16 [29120/60000 (49%)]\t training loss: 0.000117\n",
      "epoch: 16 [29440/60000 (49%)]\t training loss: 0.032668\n",
      "epoch: 16 [29760/60000 (50%)]\t training loss: 0.013704\n",
      "epoch: 16 [30080/60000 (50%)]\t training loss: 0.000507\n",
      "epoch: 16 [30400/60000 (51%)]\t training loss: 0.033166\n",
      "epoch: 16 [30720/60000 (51%)]\t training loss: 0.002011\n",
      "epoch: 16 [31040/60000 (52%)]\t training loss: 0.037186\n",
      "epoch: 16 [31360/60000 (52%)]\t training loss: 0.020728\n",
      "epoch: 16 [31680/60000 (53%)]\t training loss: 0.001635\n",
      "epoch: 16 [32000/60000 (53%)]\t training loss: 0.000056\n",
      "epoch: 16 [32320/60000 (54%)]\t training loss: 0.006055\n",
      "epoch: 16 [32640/60000 (54%)]\t training loss: 0.031459\n",
      "epoch: 16 [32960/60000 (55%)]\t training loss: 0.021949\n",
      "epoch: 16 [33280/60000 (55%)]\t training loss: 0.001085\n",
      "epoch: 16 [33600/60000 (56%)]\t training loss: 0.023742\n",
      "epoch: 16 [33920/60000 (57%)]\t training loss: 0.109421\n",
      "epoch: 16 [34240/60000 (57%)]\t training loss: 0.010891\n",
      "epoch: 16 [34560/60000 (58%)]\t training loss: 0.048600\n",
      "epoch: 16 [34880/60000 (58%)]\t training loss: 0.013030\n",
      "epoch: 16 [35200/60000 (59%)]\t training loss: 0.000015\n",
      "epoch: 16 [35520/60000 (59%)]\t training loss: 0.108239\n",
      "epoch: 16 [35840/60000 (60%)]\t training loss: 0.000006\n",
      "epoch: 16 [36160/60000 (60%)]\t training loss: 0.000258\n",
      "epoch: 16 [36480/60000 (61%)]\t training loss: 0.000299\n",
      "epoch: 16 [36800/60000 (61%)]\t training loss: 0.003308\n",
      "epoch: 16 [37120/60000 (62%)]\t training loss: 0.002147\n",
      "epoch: 16 [37440/60000 (62%)]\t training loss: 0.077375\n",
      "epoch: 16 [37760/60000 (63%)]\t training loss: 0.000778\n",
      "epoch: 16 [38080/60000 (63%)]\t training loss: 0.000029\n",
      "epoch: 16 [38400/60000 (64%)]\t training loss: 0.000001\n",
      "epoch: 16 [38720/60000 (65%)]\t training loss: 0.000047\n",
      "epoch: 16 [39040/60000 (65%)]\t training loss: 0.000421\n",
      "epoch: 16 [39360/60000 (66%)]\t training loss: 0.001164\n",
      "epoch: 16 [39680/60000 (66%)]\t training loss: 0.000253\n",
      "epoch: 16 [40000/60000 (67%)]\t training loss: 0.000001\n",
      "epoch: 16 [40320/60000 (67%)]\t training loss: 0.001432\n",
      "epoch: 16 [40640/60000 (68%)]\t training loss: 0.004012\n",
      "epoch: 16 [40960/60000 (68%)]\t training loss: 0.001621\n",
      "epoch: 16 [41280/60000 (69%)]\t training loss: 0.117426\n",
      "epoch: 16 [41600/60000 (69%)]\t training loss: 0.000209\n",
      "epoch: 16 [41920/60000 (70%)]\t training loss: 0.001466\n",
      "epoch: 16 [42240/60000 (70%)]\t training loss: 0.000037\n",
      "epoch: 16 [42560/60000 (71%)]\t training loss: 0.002388\n",
      "epoch: 16 [42880/60000 (71%)]\t training loss: 0.004630\n",
      "epoch: 16 [43200/60000 (72%)]\t training loss: 0.089698\n",
      "epoch: 16 [43520/60000 (73%)]\t training loss: 0.000071\n",
      "epoch: 16 [43840/60000 (73%)]\t training loss: 0.015932\n",
      "epoch: 16 [44160/60000 (74%)]\t training loss: 0.000009\n",
      "epoch: 16 [44480/60000 (74%)]\t training loss: 0.000880\n",
      "epoch: 16 [44800/60000 (75%)]\t training loss: 0.000318\n",
      "epoch: 16 [45120/60000 (75%)]\t training loss: 0.001301\n",
      "epoch: 16 [45440/60000 (76%)]\t training loss: 0.000721\n",
      "epoch: 16 [45760/60000 (76%)]\t training loss: 0.011307\n",
      "epoch: 16 [46080/60000 (77%)]\t training loss: 0.032258\n",
      "epoch: 16 [46400/60000 (77%)]\t training loss: 0.006690\n",
      "epoch: 16 [46720/60000 (78%)]\t training loss: 0.000002\n",
      "epoch: 16 [47040/60000 (78%)]\t training loss: 0.000274\n",
      "epoch: 16 [47360/60000 (79%)]\t training loss: 0.001721\n",
      "epoch: 16 [47680/60000 (79%)]\t training loss: 0.015726\n",
      "epoch: 16 [48000/60000 (80%)]\t training loss: 0.000523\n",
      "epoch: 16 [48320/60000 (81%)]\t training loss: 0.000937\n",
      "epoch: 16 [48640/60000 (81%)]\t training loss: 0.000168\n",
      "epoch: 16 [48960/60000 (82%)]\t training loss: 0.000284\n",
      "epoch: 16 [49280/60000 (82%)]\t training loss: 0.006930\n",
      "epoch: 16 [49600/60000 (83%)]\t training loss: 0.013902\n",
      "epoch: 16 [49920/60000 (83%)]\t training loss: 0.017280\n",
      "epoch: 16 [50240/60000 (84%)]\t training loss: 0.011633\n",
      "epoch: 16 [50560/60000 (84%)]\t training loss: 0.078482\n",
      "epoch: 16 [50880/60000 (85%)]\t training loss: 0.000320\n",
      "epoch: 16 [51200/60000 (85%)]\t training loss: 0.015576\n",
      "epoch: 16 [51520/60000 (86%)]\t training loss: 0.093708\n",
      "epoch: 16 [51840/60000 (86%)]\t training loss: 0.016533\n",
      "epoch: 16 [52160/60000 (87%)]\t training loss: 0.141786\n",
      "epoch: 16 [52480/60000 (87%)]\t training loss: 0.000240\n",
      "epoch: 16 [52800/60000 (88%)]\t training loss: 0.000074\n",
      "epoch: 16 [53120/60000 (89%)]\t training loss: 0.000077\n",
      "epoch: 16 [53440/60000 (89%)]\t training loss: 0.000582\n",
      "epoch: 16 [53760/60000 (90%)]\t training loss: 0.000338\n",
      "epoch: 16 [54080/60000 (90%)]\t training loss: 0.000022\n",
      "epoch: 16 [54400/60000 (91%)]\t training loss: 0.260694\n",
      "epoch: 16 [54720/60000 (91%)]\t training loss: 0.012162\n",
      "epoch: 16 [55040/60000 (92%)]\t training loss: 0.086784\n",
      "epoch: 16 [55360/60000 (92%)]\t training loss: 0.007584\n",
      "epoch: 16 [55680/60000 (93%)]\t training loss: 0.021159\n",
      "epoch: 16 [56000/60000 (93%)]\t training loss: 0.001089\n",
      "epoch: 16 [56320/60000 (94%)]\t training loss: 0.074925\n",
      "epoch: 16 [56640/60000 (94%)]\t training loss: 0.056466\n",
      "epoch: 16 [56960/60000 (95%)]\t training loss: 0.002263\n",
      "epoch: 16 [57280/60000 (95%)]\t training loss: 0.002352\n",
      "epoch: 16 [57600/60000 (96%)]\t training loss: 0.036596\n",
      "epoch: 16 [57920/60000 (97%)]\t training loss: 0.033504\n",
      "epoch: 16 [58240/60000 (97%)]\t training loss: 0.059629\n",
      "epoch: 16 [58560/60000 (98%)]\t training loss: 0.007107\n",
      "epoch: 16 [58880/60000 (98%)]\t training loss: 0.001377\n",
      "epoch: 16 [59200/60000 (99%)]\t training loss: 0.002635\n",
      "epoch: 16 [59520/60000 (99%)]\t training loss: 0.043879\n",
      "epoch: 16 [59840/60000 (100%)]\t training loss: 0.002214\n",
      "\n",
      "Test dataset: Overall Loss: 0.0365, Overall Accuracy: 9915/10000 (99%)\n",
      "\n",
      "epoch: 17 [0/60000 (0%)]\t training loss: 0.023838\n",
      "epoch: 17 [320/60000 (1%)]\t training loss: 0.000427\n",
      "epoch: 17 [640/60000 (1%)]\t training loss: 0.019976\n",
      "epoch: 17 [960/60000 (2%)]\t training loss: 0.008126\n",
      "epoch: 17 [1280/60000 (2%)]\t training loss: 0.003070\n",
      "epoch: 17 [1600/60000 (3%)]\t training loss: 0.012155\n",
      "epoch: 17 [1920/60000 (3%)]\t training loss: 0.000020\n",
      "epoch: 17 [2240/60000 (4%)]\t training loss: 0.010795\n",
      "epoch: 17 [2560/60000 (4%)]\t training loss: 0.000223\n",
      "epoch: 17 [2880/60000 (5%)]\t training loss: 0.000102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 [3200/60000 (5%)]\t training loss: 0.005832\n",
      "epoch: 17 [3520/60000 (6%)]\t training loss: 0.000369\n",
      "epoch: 17 [3840/60000 (6%)]\t training loss: 0.038796\n",
      "epoch: 17 [4160/60000 (7%)]\t training loss: 0.002477\n",
      "epoch: 17 [4480/60000 (7%)]\t training loss: 0.111183\n",
      "epoch: 17 [4800/60000 (8%)]\t training loss: 0.003587\n",
      "epoch: 17 [5120/60000 (9%)]\t training loss: 0.005276\n",
      "epoch: 17 [5440/60000 (9%)]\t training loss: 0.010138\n",
      "epoch: 17 [5760/60000 (10%)]\t training loss: 0.000631\n",
      "epoch: 17 [6080/60000 (10%)]\t training loss: 0.000040\n",
      "epoch: 17 [6400/60000 (11%)]\t training loss: 0.000165\n",
      "epoch: 17 [6720/60000 (11%)]\t training loss: 0.168261\n",
      "epoch: 17 [7040/60000 (12%)]\t training loss: 0.032778\n",
      "epoch: 17 [7360/60000 (12%)]\t training loss: 0.023042\n",
      "epoch: 17 [7680/60000 (13%)]\t training loss: 0.100290\n",
      "epoch: 17 [8000/60000 (13%)]\t training loss: 0.062186\n",
      "epoch: 17 [8320/60000 (14%)]\t training loss: 0.107635\n",
      "epoch: 17 [8640/60000 (14%)]\t training loss: 0.002756\n",
      "epoch: 17 [8960/60000 (15%)]\t training loss: 0.026701\n",
      "epoch: 17 [9280/60000 (15%)]\t training loss: 0.000153\n",
      "epoch: 17 [9600/60000 (16%)]\t training loss: 0.000353\n",
      "epoch: 17 [9920/60000 (17%)]\t training loss: 0.016539\n",
      "epoch: 17 [10240/60000 (17%)]\t training loss: 0.001235\n",
      "epoch: 17 [10560/60000 (18%)]\t training loss: 0.009878\n",
      "epoch: 17 [10880/60000 (18%)]\t training loss: 0.109676\n",
      "epoch: 17 [11200/60000 (19%)]\t training loss: 0.095164\n",
      "epoch: 17 [11520/60000 (19%)]\t training loss: 0.000602\n",
      "epoch: 17 [11840/60000 (20%)]\t training loss: 0.043024\n",
      "epoch: 17 [12160/60000 (20%)]\t training loss: 0.000770\n",
      "epoch: 17 [12480/60000 (21%)]\t training loss: 0.000044\n",
      "epoch: 17 [12800/60000 (21%)]\t training loss: 0.011432\n",
      "epoch: 17 [13120/60000 (22%)]\t training loss: 0.001671\n",
      "epoch: 17 [13440/60000 (22%)]\t training loss: 0.001335\n",
      "epoch: 17 [13760/60000 (23%)]\t training loss: 0.009394\n",
      "epoch: 17 [14080/60000 (23%)]\t training loss: 0.000173\n",
      "epoch: 17 [14400/60000 (24%)]\t training loss: 0.000381\n",
      "epoch: 17 [14720/60000 (25%)]\t training loss: 0.017232\n",
      "epoch: 17 [15040/60000 (25%)]\t training loss: 0.044561\n",
      "epoch: 17 [15360/60000 (26%)]\t training loss: 0.032814\n",
      "epoch: 17 [15680/60000 (26%)]\t training loss: 0.046658\n",
      "epoch: 17 [16000/60000 (27%)]\t training loss: 0.001787\n",
      "epoch: 17 [16320/60000 (27%)]\t training loss: 0.010587\n",
      "epoch: 17 [16640/60000 (28%)]\t training loss: 0.372599\n",
      "epoch: 17 [16960/60000 (28%)]\t training loss: 0.001248\n",
      "epoch: 17 [17280/60000 (29%)]\t training loss: 0.000119\n",
      "epoch: 17 [17600/60000 (29%)]\t training loss: 0.043537\n",
      "epoch: 17 [17920/60000 (30%)]\t training loss: 0.085916\n",
      "epoch: 17 [18240/60000 (30%)]\t training loss: 0.007947\n",
      "epoch: 17 [18560/60000 (31%)]\t training loss: 0.001393\n",
      "epoch: 17 [18880/60000 (31%)]\t training loss: 0.000166\n",
      "epoch: 17 [19200/60000 (32%)]\t training loss: 0.000200\n",
      "epoch: 17 [19520/60000 (33%)]\t training loss: 0.001120\n",
      "epoch: 17 [19840/60000 (33%)]\t training loss: 0.000043\n",
      "epoch: 17 [20160/60000 (34%)]\t training loss: 0.000006\n",
      "epoch: 17 [20480/60000 (34%)]\t training loss: 0.000593\n",
      "epoch: 17 [20800/60000 (35%)]\t training loss: 0.073967\n",
      "epoch: 17 [21120/60000 (35%)]\t training loss: 0.169667\n",
      "epoch: 17 [21440/60000 (36%)]\t training loss: 0.015761\n",
      "epoch: 17 [21760/60000 (36%)]\t training loss: 0.000446\n",
      "epoch: 17 [22080/60000 (37%)]\t training loss: 0.000257\n",
      "epoch: 17 [22400/60000 (37%)]\t training loss: 0.011770\n",
      "epoch: 17 [22720/60000 (38%)]\t training loss: 0.012109\n",
      "epoch: 17 [23040/60000 (38%)]\t training loss: 0.000152\n",
      "epoch: 17 [23360/60000 (39%)]\t training loss: 0.002984\n",
      "epoch: 17 [23680/60000 (39%)]\t training loss: 0.001025\n",
      "epoch: 17 [24000/60000 (40%)]\t training loss: 0.010295\n",
      "epoch: 17 [24320/60000 (41%)]\t training loss: 0.002142\n",
      "epoch: 17 [24640/60000 (41%)]\t training loss: 0.000900\n",
      "epoch: 17 [24960/60000 (42%)]\t training loss: 0.001991\n",
      "epoch: 17 [25280/60000 (42%)]\t training loss: 0.006170\n",
      "epoch: 17 [25600/60000 (43%)]\t training loss: 0.000036\n",
      "epoch: 17 [25920/60000 (43%)]\t training loss: 0.000559\n",
      "epoch: 17 [26240/60000 (44%)]\t training loss: 0.000253\n",
      "epoch: 17 [26560/60000 (44%)]\t training loss: 0.025550\n",
      "epoch: 17 [26880/60000 (45%)]\t training loss: 0.000380\n",
      "epoch: 17 [27200/60000 (45%)]\t training loss: 0.001438\n",
      "epoch: 17 [27520/60000 (46%)]\t training loss: 0.000101\n",
      "epoch: 17 [27840/60000 (46%)]\t training loss: 0.011132\n",
      "epoch: 17 [28160/60000 (47%)]\t training loss: 0.013895\n",
      "epoch: 17 [28480/60000 (47%)]\t training loss: 0.007056\n",
      "epoch: 17 [28800/60000 (48%)]\t training loss: 0.035419\n",
      "epoch: 17 [29120/60000 (49%)]\t training loss: 0.011622\n",
      "epoch: 17 [29440/60000 (49%)]\t training loss: 0.063179\n",
      "epoch: 17 [29760/60000 (50%)]\t training loss: 0.027359\n",
      "epoch: 17 [30080/60000 (50%)]\t training loss: 0.004284\n",
      "epoch: 17 [30400/60000 (51%)]\t training loss: 0.000139\n",
      "epoch: 17 [30720/60000 (51%)]\t training loss: 0.117154\n",
      "epoch: 17 [31040/60000 (52%)]\t training loss: 0.000346\n",
      "epoch: 17 [31360/60000 (52%)]\t training loss: 0.215181\n",
      "epoch: 17 [31680/60000 (53%)]\t training loss: 0.000036\n",
      "epoch: 17 [32000/60000 (53%)]\t training loss: 0.011744\n",
      "epoch: 17 [32320/60000 (54%)]\t training loss: 0.000959\n",
      "epoch: 17 [32640/60000 (54%)]\t training loss: 0.001621\n",
      "epoch: 17 [32960/60000 (55%)]\t training loss: 0.052511\n",
      "epoch: 17 [33280/60000 (55%)]\t training loss: 0.006970\n",
      "epoch: 17 [33600/60000 (56%)]\t training loss: 0.021486\n",
      "epoch: 17 [33920/60000 (57%)]\t training loss: 0.015938\n",
      "epoch: 17 [34240/60000 (57%)]\t training loss: 0.018558\n",
      "epoch: 17 [34560/60000 (58%)]\t training loss: 0.019798\n",
      "epoch: 17 [34880/60000 (58%)]\t training loss: 0.007551\n",
      "epoch: 17 [35200/60000 (59%)]\t training loss: 0.007211\n",
      "epoch: 17 [35520/60000 (59%)]\t training loss: 0.000147\n",
      "epoch: 17 [35840/60000 (60%)]\t training loss: 0.010886\n",
      "epoch: 17 [36160/60000 (60%)]\t training loss: 0.056911\n",
      "epoch: 17 [36480/60000 (61%)]\t training loss: 0.007858\n",
      "epoch: 17 [36800/60000 (61%)]\t training loss: 0.007985\n",
      "epoch: 17 [37120/60000 (62%)]\t training loss: 0.000789\n",
      "epoch: 17 [37440/60000 (62%)]\t training loss: 0.003042\n",
      "epoch: 17 [37760/60000 (63%)]\t training loss: 0.033685\n",
      "epoch: 17 [38080/60000 (63%)]\t training loss: 0.000510\n",
      "epoch: 17 [38400/60000 (64%)]\t training loss: 0.000029\n",
      "epoch: 17 [38720/60000 (65%)]\t training loss: 0.003501\n",
      "epoch: 17 [39040/60000 (65%)]\t training loss: 0.001717\n",
      "epoch: 17 [39360/60000 (66%)]\t training loss: 0.002932\n",
      "epoch: 17 [39680/60000 (66%)]\t training loss: 0.002364\n",
      "epoch: 17 [40000/60000 (67%)]\t training loss: 0.000300\n",
      "epoch: 17 [40320/60000 (67%)]\t training loss: 0.046779\n",
      "epoch: 17 [40640/60000 (68%)]\t training loss: 0.001160\n",
      "epoch: 17 [40960/60000 (68%)]\t training loss: 0.003521\n",
      "epoch: 17 [41280/60000 (69%)]\t training loss: 0.001759\n",
      "epoch: 17 [41600/60000 (69%)]\t training loss: 0.001145\n",
      "epoch: 17 [41920/60000 (70%)]\t training loss: 0.000035\n",
      "epoch: 17 [42240/60000 (70%)]\t training loss: 0.002541\n",
      "epoch: 17 [42560/60000 (71%)]\t training loss: 0.000547\n",
      "epoch: 17 [42880/60000 (71%)]\t training loss: 0.002057\n",
      "epoch: 17 [43200/60000 (72%)]\t training loss: 0.002231\n",
      "epoch: 17 [43520/60000 (73%)]\t training loss: 0.044513\n",
      "epoch: 17 [43840/60000 (73%)]\t training loss: 0.137636\n",
      "epoch: 17 [44160/60000 (74%)]\t training loss: 0.016219\n",
      "epoch: 17 [44480/60000 (74%)]\t training loss: 0.000802\n",
      "epoch: 17 [44800/60000 (75%)]\t training loss: 0.001958\n",
      "epoch: 17 [45120/60000 (75%)]\t training loss: 0.065538\n",
      "epoch: 17 [45440/60000 (76%)]\t training loss: 0.122769\n",
      "epoch: 17 [45760/60000 (76%)]\t training loss: 0.002897\n",
      "epoch: 17 [46080/60000 (77%)]\t training loss: 0.003747\n",
      "epoch: 17 [46400/60000 (77%)]\t training loss: 0.030176\n",
      "epoch: 17 [46720/60000 (78%)]\t training loss: 0.162850\n",
      "epoch: 17 [47040/60000 (78%)]\t training loss: 0.147716\n",
      "epoch: 17 [47360/60000 (79%)]\t training loss: 0.004309\n",
      "epoch: 17 [47680/60000 (79%)]\t training loss: 0.010267\n",
      "epoch: 17 [48000/60000 (80%)]\t training loss: 0.000052\n",
      "epoch: 17 [48320/60000 (81%)]\t training loss: 0.021387\n",
      "epoch: 17 [48640/60000 (81%)]\t training loss: 0.031379\n",
      "epoch: 17 [48960/60000 (82%)]\t training loss: 0.001859\n",
      "epoch: 17 [49280/60000 (82%)]\t training loss: 0.004103\n",
      "epoch: 17 [49600/60000 (83%)]\t training loss: 0.046602\n",
      "epoch: 17 [49920/60000 (83%)]\t training loss: 0.102675\n",
      "epoch: 17 [50240/60000 (84%)]\t training loss: 0.042494\n",
      "epoch: 17 [50560/60000 (84%)]\t training loss: 0.002271\n",
      "epoch: 17 [50880/60000 (85%)]\t training loss: 0.006091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 [51200/60000 (85%)]\t training loss: 0.070297\n",
      "epoch: 17 [51520/60000 (86%)]\t training loss: 0.000202\n",
      "epoch: 17 [51840/60000 (86%)]\t training loss: 0.001157\n",
      "epoch: 17 [52160/60000 (87%)]\t training loss: 0.000169\n",
      "epoch: 17 [52480/60000 (87%)]\t training loss: 0.000103\n",
      "epoch: 17 [52800/60000 (88%)]\t training loss: 0.000020\n",
      "epoch: 17 [53120/60000 (89%)]\t training loss: 0.000130\n",
      "epoch: 17 [53440/60000 (89%)]\t training loss: 0.011921\n",
      "epoch: 17 [53760/60000 (90%)]\t training loss: 0.003096\n",
      "epoch: 17 [54080/60000 (90%)]\t training loss: 0.000181\n",
      "epoch: 17 [54400/60000 (91%)]\t training loss: 0.045832\n",
      "epoch: 17 [54720/60000 (91%)]\t training loss: 0.000399\n",
      "epoch: 17 [55040/60000 (92%)]\t training loss: 0.000600\n",
      "epoch: 17 [55360/60000 (92%)]\t training loss: 0.092749\n",
      "epoch: 17 [55680/60000 (93%)]\t training loss: 0.000370\n",
      "epoch: 17 [56000/60000 (93%)]\t training loss: 0.000009\n",
      "epoch: 17 [56320/60000 (94%)]\t training loss: 0.020051\n",
      "epoch: 17 [56640/60000 (94%)]\t training loss: 0.000141\n",
      "epoch: 17 [56960/60000 (95%)]\t training loss: 0.123791\n",
      "epoch: 17 [57280/60000 (95%)]\t training loss: 0.010588\n",
      "epoch: 17 [57600/60000 (96%)]\t training loss: 0.051027\n",
      "epoch: 17 [57920/60000 (97%)]\t training loss: 0.004775\n",
      "epoch: 17 [58240/60000 (97%)]\t training loss: 0.153711\n",
      "epoch: 17 [58560/60000 (98%)]\t training loss: 0.000929\n",
      "epoch: 17 [58880/60000 (98%)]\t training loss: 0.001421\n",
      "epoch: 17 [59200/60000 (99%)]\t training loss: 0.001654\n",
      "epoch: 17 [59520/60000 (99%)]\t training loss: 0.006344\n",
      "epoch: 17 [59840/60000 (100%)]\t training loss: 0.011141\n",
      "\n",
      "Test dataset: Overall Loss: 0.0342, Overall Accuracy: 9912/10000 (99%)\n",
      "\n",
      "epoch: 18 [0/60000 (0%)]\t training loss: 0.002132\n",
      "epoch: 18 [320/60000 (1%)]\t training loss: 0.005215\n",
      "epoch: 18 [640/60000 (1%)]\t training loss: 0.023040\n",
      "epoch: 18 [960/60000 (2%)]\t training loss: 0.000046\n",
      "epoch: 18 [1280/60000 (2%)]\t training loss: 0.061875\n",
      "epoch: 18 [1600/60000 (3%)]\t training loss: 0.000073\n",
      "epoch: 18 [1920/60000 (3%)]\t training loss: 0.014192\n",
      "epoch: 18 [2240/60000 (4%)]\t training loss: 0.034281\n",
      "epoch: 18 [2560/60000 (4%)]\t training loss: 0.014835\n",
      "epoch: 18 [2880/60000 (5%)]\t training loss: 0.000513\n",
      "epoch: 18 [3200/60000 (5%)]\t training loss: 0.003787\n",
      "epoch: 18 [3520/60000 (6%)]\t training loss: 0.002475\n",
      "epoch: 18 [3840/60000 (6%)]\t training loss: 0.088092\n",
      "epoch: 18 [4160/60000 (7%)]\t training loss: 0.084826\n",
      "epoch: 18 [4480/60000 (7%)]\t training loss: 0.024873\n",
      "epoch: 18 [4800/60000 (8%)]\t training loss: 0.004605\n",
      "epoch: 18 [5120/60000 (9%)]\t training loss: 0.000750\n",
      "epoch: 18 [5440/60000 (9%)]\t training loss: 0.000024\n",
      "epoch: 18 [5760/60000 (10%)]\t training loss: 0.000522\n",
      "epoch: 18 [6080/60000 (10%)]\t training loss: 0.002375\n",
      "epoch: 18 [6400/60000 (11%)]\t training loss: 0.000792\n",
      "epoch: 18 [6720/60000 (11%)]\t training loss: 0.224357\n",
      "epoch: 18 [7040/60000 (12%)]\t training loss: 0.000892\n",
      "epoch: 18 [7360/60000 (12%)]\t training loss: 0.004934\n",
      "epoch: 18 [7680/60000 (13%)]\t training loss: 0.013406\n",
      "epoch: 18 [8000/60000 (13%)]\t training loss: 0.011842\n",
      "epoch: 18 [8320/60000 (14%)]\t training loss: 0.002874\n",
      "epoch: 18 [8640/60000 (14%)]\t training loss: 0.028261\n",
      "epoch: 18 [8960/60000 (15%)]\t training loss: 0.005934\n",
      "epoch: 18 [9280/60000 (15%)]\t training loss: 0.000249\n",
      "epoch: 18 [9600/60000 (16%)]\t training loss: 0.000949\n",
      "epoch: 18 [9920/60000 (17%)]\t training loss: 0.000048\n",
      "epoch: 18 [10240/60000 (17%)]\t training loss: 0.004489\n",
      "epoch: 18 [10560/60000 (18%)]\t training loss: 0.000099\n",
      "epoch: 18 [10880/60000 (18%)]\t training loss: 0.001526\n",
      "epoch: 18 [11200/60000 (19%)]\t training loss: 0.009478\n",
      "epoch: 18 [11520/60000 (19%)]\t training loss: 0.002938\n",
      "epoch: 18 [11840/60000 (20%)]\t training loss: 0.005023\n",
      "epoch: 18 [12160/60000 (20%)]\t training loss: 0.192390\n",
      "epoch: 18 [12480/60000 (21%)]\t training loss: 0.116930\n",
      "epoch: 18 [12800/60000 (21%)]\t training loss: 0.001775\n",
      "epoch: 18 [13120/60000 (22%)]\t training loss: 0.074075\n",
      "epoch: 18 [13440/60000 (22%)]\t training loss: 0.000404\n",
      "epoch: 18 [13760/60000 (23%)]\t training loss: 0.000031\n",
      "epoch: 18 [14080/60000 (23%)]\t training loss: 0.021066\n",
      "epoch: 18 [14400/60000 (24%)]\t training loss: 0.000214\n",
      "epoch: 18 [14720/60000 (25%)]\t training loss: 0.000381\n",
      "epoch: 18 [15040/60000 (25%)]\t training loss: 0.133757\n",
      "epoch: 18 [15360/60000 (26%)]\t training loss: 0.000669\n",
      "epoch: 18 [15680/60000 (26%)]\t training loss: 0.000111\n",
      "epoch: 18 [16000/60000 (27%)]\t training loss: 0.023227\n",
      "epoch: 18 [16320/60000 (27%)]\t training loss: 0.010297\n",
      "epoch: 18 [16640/60000 (28%)]\t training loss: 0.001117\n",
      "epoch: 18 [16960/60000 (28%)]\t training loss: 0.001579\n",
      "epoch: 18 [17280/60000 (29%)]\t training loss: 0.000532\n",
      "epoch: 18 [17600/60000 (29%)]\t training loss: 0.011654\n",
      "epoch: 18 [17920/60000 (30%)]\t training loss: 0.000076\n",
      "epoch: 18 [18240/60000 (30%)]\t training loss: 0.001595\n",
      "epoch: 18 [18560/60000 (31%)]\t training loss: 0.010837\n",
      "epoch: 18 [18880/60000 (31%)]\t training loss: 0.017245\n",
      "epoch: 18 [19200/60000 (32%)]\t training loss: 0.003623\n",
      "epoch: 18 [19520/60000 (33%)]\t training loss: 0.000016\n",
      "epoch: 18 [19840/60000 (33%)]\t training loss: 0.002891\n",
      "epoch: 18 [20160/60000 (34%)]\t training loss: 0.000970\n",
      "epoch: 18 [20480/60000 (34%)]\t training loss: 0.002557\n",
      "epoch: 18 [20800/60000 (35%)]\t training loss: 0.340838\n",
      "epoch: 18 [21120/60000 (35%)]\t training loss: 0.012474\n",
      "epoch: 18 [21440/60000 (36%)]\t training loss: 0.000289\n",
      "epoch: 18 [21760/60000 (36%)]\t training loss: 0.001257\n",
      "epoch: 18 [22080/60000 (37%)]\t training loss: 0.000852\n",
      "epoch: 18 [22400/60000 (37%)]\t training loss: 0.001999\n",
      "epoch: 18 [22720/60000 (38%)]\t training loss: 0.017108\n",
      "epoch: 18 [23040/60000 (38%)]\t training loss: 0.000238\n",
      "epoch: 18 [23360/60000 (39%)]\t training loss: 0.004924\n",
      "epoch: 18 [23680/60000 (39%)]\t training loss: 0.000070\n",
      "epoch: 18 [24000/60000 (40%)]\t training loss: 0.000528\n",
      "epoch: 18 [24320/60000 (41%)]\t training loss: 0.024716\n",
      "epoch: 18 [24640/60000 (41%)]\t training loss: 0.000242\n",
      "epoch: 18 [24960/60000 (42%)]\t training loss: 0.002482\n",
      "epoch: 18 [25280/60000 (42%)]\t training loss: 0.000519\n",
      "epoch: 18 [25600/60000 (43%)]\t training loss: 0.006795\n",
      "epoch: 18 [25920/60000 (43%)]\t training loss: 0.000371\n",
      "epoch: 18 [26240/60000 (44%)]\t training loss: 0.010987\n",
      "epoch: 18 [26560/60000 (44%)]\t training loss: 0.000034\n",
      "epoch: 18 [26880/60000 (45%)]\t training loss: 0.007584\n",
      "epoch: 18 [27200/60000 (45%)]\t training loss: 0.000053\n",
      "epoch: 18 [27520/60000 (46%)]\t training loss: 0.000005\n",
      "epoch: 18 [27840/60000 (46%)]\t training loss: 0.000377\n",
      "epoch: 18 [28160/60000 (47%)]\t training loss: 0.000646\n",
      "epoch: 18 [28480/60000 (47%)]\t training loss: 0.003108\n",
      "epoch: 18 [28800/60000 (48%)]\t training loss: 0.006351\n",
      "epoch: 18 [29120/60000 (49%)]\t training loss: 0.004965\n",
      "epoch: 18 [29440/60000 (49%)]\t training loss: 0.000008\n",
      "epoch: 18 [29760/60000 (50%)]\t training loss: 0.015799\n",
      "epoch: 18 [30080/60000 (50%)]\t training loss: 0.000397\n",
      "epoch: 18 [30400/60000 (51%)]\t training loss: 0.000031\n",
      "epoch: 18 [30720/60000 (51%)]\t training loss: 0.015129\n",
      "epoch: 18 [31040/60000 (52%)]\t training loss: 0.003075\n",
      "epoch: 18 [31360/60000 (52%)]\t training loss: 0.003178\n",
      "epoch: 18 [31680/60000 (53%)]\t training loss: 0.000847\n",
      "epoch: 18 [32000/60000 (53%)]\t training loss: 0.001181\n",
      "epoch: 18 [32320/60000 (54%)]\t training loss: 0.042662\n",
      "epoch: 18 [32640/60000 (54%)]\t training loss: 0.000211\n",
      "epoch: 18 [32960/60000 (55%)]\t training loss: 0.004496\n",
      "epoch: 18 [33280/60000 (55%)]\t training loss: 0.002171\n",
      "epoch: 18 [33600/60000 (56%)]\t training loss: 0.000316\n",
      "epoch: 18 [33920/60000 (57%)]\t training loss: 0.114964\n",
      "epoch: 18 [34240/60000 (57%)]\t training loss: 0.026994\n",
      "epoch: 18 [34560/60000 (58%)]\t training loss: 0.004162\n",
      "epoch: 18 [34880/60000 (58%)]\t training loss: 0.001630\n",
      "epoch: 18 [35200/60000 (59%)]\t training loss: 0.001786\n",
      "epoch: 18 [35520/60000 (59%)]\t training loss: 0.021496\n",
      "epoch: 18 [35840/60000 (60%)]\t training loss: 0.002634\n",
      "epoch: 18 [36160/60000 (60%)]\t training loss: 0.024832\n",
      "epoch: 18 [36480/60000 (61%)]\t training loss: 0.004417\n",
      "epoch: 18 [36800/60000 (61%)]\t training loss: 0.000009\n",
      "epoch: 18 [37120/60000 (62%)]\t training loss: 0.000003\n",
      "epoch: 18 [37440/60000 (62%)]\t training loss: 0.000522\n",
      "epoch: 18 [37760/60000 (63%)]\t training loss: 0.001519\n",
      "epoch: 18 [38080/60000 (63%)]\t training loss: 0.059156\n",
      "epoch: 18 [38400/60000 (64%)]\t training loss: 0.000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 [38720/60000 (65%)]\t training loss: 0.028883\n",
      "epoch: 18 [39040/60000 (65%)]\t training loss: 0.000872\n",
      "epoch: 18 [39360/60000 (66%)]\t training loss: 0.001147\n",
      "epoch: 18 [39680/60000 (66%)]\t training loss: 0.056013\n",
      "epoch: 18 [40000/60000 (67%)]\t training loss: 0.159671\n",
      "epoch: 18 [40320/60000 (67%)]\t training loss: 0.017505\n",
      "epoch: 18 [40640/60000 (68%)]\t training loss: 0.004328\n",
      "epoch: 18 [40960/60000 (68%)]\t training loss: 0.003501\n",
      "epoch: 18 [41280/60000 (69%)]\t training loss: 0.014652\n",
      "epoch: 18 [41600/60000 (69%)]\t training loss: 0.003805\n",
      "epoch: 18 [41920/60000 (70%)]\t training loss: 0.000783\n",
      "epoch: 18 [42240/60000 (70%)]\t training loss: 0.000116\n",
      "epoch: 18 [42560/60000 (71%)]\t training loss: 0.011686\n",
      "epoch: 18 [42880/60000 (71%)]\t training loss: 0.001366\n",
      "epoch: 18 [43200/60000 (72%)]\t training loss: 0.000139\n",
      "epoch: 18 [43520/60000 (73%)]\t training loss: 0.073795\n",
      "epoch: 18 [43840/60000 (73%)]\t training loss: 0.000104\n",
      "epoch: 18 [44160/60000 (74%)]\t training loss: 0.047211\n",
      "epoch: 18 [44480/60000 (74%)]\t training loss: 0.000524\n",
      "epoch: 18 [44800/60000 (75%)]\t training loss: 0.002181\n",
      "epoch: 18 [45120/60000 (75%)]\t training loss: 0.000640\n",
      "epoch: 18 [45440/60000 (76%)]\t training loss: 0.000016\n",
      "epoch: 18 [45760/60000 (76%)]\t training loss: 0.000000\n",
      "epoch: 18 [46080/60000 (77%)]\t training loss: 0.010970\n",
      "epoch: 18 [46400/60000 (77%)]\t training loss: 0.002223\n",
      "epoch: 18 [46720/60000 (78%)]\t training loss: 0.001777\n",
      "epoch: 18 [47040/60000 (78%)]\t training loss: 0.009859\n",
      "epoch: 18 [47360/60000 (79%)]\t training loss: 0.013774\n",
      "epoch: 18 [47680/60000 (79%)]\t training loss: 0.050233\n",
      "epoch: 18 [48000/60000 (80%)]\t training loss: 0.000081\n",
      "epoch: 18 [48320/60000 (81%)]\t training loss: 0.002081\n",
      "epoch: 18 [48640/60000 (81%)]\t training loss: 0.023651\n",
      "epoch: 18 [48960/60000 (82%)]\t training loss: 0.004118\n",
      "epoch: 18 [49280/60000 (82%)]\t training loss: 0.000219\n",
      "epoch: 18 [49600/60000 (83%)]\t training loss: 0.008006\n",
      "epoch: 18 [49920/60000 (83%)]\t training loss: 0.000003\n",
      "epoch: 18 [50240/60000 (84%)]\t training loss: 0.124653\n",
      "epoch: 18 [50560/60000 (84%)]\t training loss: 0.027051\n",
      "epoch: 18 [50880/60000 (85%)]\t training loss: 0.001369\n",
      "epoch: 18 [51200/60000 (85%)]\t training loss: 0.004655\n",
      "epoch: 18 [51520/60000 (86%)]\t training loss: 0.000023\n",
      "epoch: 18 [51840/60000 (86%)]\t training loss: 0.044828\n",
      "epoch: 18 [52160/60000 (87%)]\t training loss: 0.002267\n",
      "epoch: 18 [52480/60000 (87%)]\t training loss: 0.001160\n",
      "epoch: 18 [52800/60000 (88%)]\t training loss: 0.000802\n",
      "epoch: 18 [53120/60000 (89%)]\t training loss: 0.004308\n",
      "epoch: 18 [53440/60000 (89%)]\t training loss: 0.000359\n",
      "epoch: 18 [53760/60000 (90%)]\t training loss: 0.007840\n",
      "epoch: 18 [54080/60000 (90%)]\t training loss: 0.000869\n",
      "epoch: 18 [54400/60000 (91%)]\t training loss: 0.023095\n",
      "epoch: 18 [54720/60000 (91%)]\t training loss: 0.046214\n",
      "epoch: 18 [55040/60000 (92%)]\t training loss: 0.000785\n",
      "epoch: 18 [55360/60000 (92%)]\t training loss: 0.011757\n",
      "epoch: 18 [55680/60000 (93%)]\t training loss: 0.013662\n",
      "epoch: 18 [56000/60000 (93%)]\t training loss: 0.057215\n",
      "epoch: 18 [56320/60000 (94%)]\t training loss: 0.001564\n",
      "epoch: 18 [56640/60000 (94%)]\t training loss: 0.000041\n",
      "epoch: 18 [56960/60000 (95%)]\t training loss: 0.008636\n",
      "epoch: 18 [57280/60000 (95%)]\t training loss: 0.001642\n",
      "epoch: 18 [57600/60000 (96%)]\t training loss: 0.000043\n",
      "epoch: 18 [57920/60000 (97%)]\t training loss: 0.026907\n",
      "epoch: 18 [58240/60000 (97%)]\t training loss: 0.000239\n",
      "epoch: 18 [58560/60000 (98%)]\t training loss: 0.000002\n",
      "epoch: 18 [58880/60000 (98%)]\t training loss: 0.001156\n",
      "epoch: 18 [59200/60000 (99%)]\t training loss: 0.097804\n",
      "epoch: 18 [59520/60000 (99%)]\t training loss: 0.000312\n",
      "epoch: 18 [59840/60000 (100%)]\t training loss: 0.000602\n",
      "\n",
      "Test dataset: Overall Loss: 0.0395, Overall Accuracy: 9913/10000 (99%)\n",
      "\n",
      "epoch: 19 [0/60000 (0%)]\t training loss: 0.003474\n",
      "epoch: 19 [320/60000 (1%)]\t training loss: 0.002045\n",
      "epoch: 19 [640/60000 (1%)]\t training loss: 0.000024\n",
      "epoch: 19 [960/60000 (2%)]\t training loss: 0.000442\n",
      "epoch: 19 [1280/60000 (2%)]\t training loss: 0.002659\n",
      "epoch: 19 [1600/60000 (3%)]\t training loss: 0.000196\n",
      "epoch: 19 [1920/60000 (3%)]\t training loss: 0.009509\n",
      "epoch: 19 [2240/60000 (4%)]\t training loss: 0.000642\n",
      "epoch: 19 [2560/60000 (4%)]\t training loss: 0.000058\n",
      "epoch: 19 [2880/60000 (5%)]\t training loss: 0.001529\n",
      "epoch: 19 [3200/60000 (5%)]\t training loss: 0.003634\n",
      "epoch: 19 [3520/60000 (6%)]\t training loss: 0.000125\n",
      "epoch: 19 [3840/60000 (6%)]\t training loss: 0.000217\n",
      "epoch: 19 [4160/60000 (7%)]\t training loss: 0.000716\n",
      "epoch: 19 [4480/60000 (7%)]\t training loss: 0.001295\n",
      "epoch: 19 [4800/60000 (8%)]\t training loss: 0.001146\n",
      "epoch: 19 [5120/60000 (9%)]\t training loss: 0.015305\n",
      "epoch: 19 [5440/60000 (9%)]\t training loss: 0.019459\n",
      "epoch: 19 [5760/60000 (10%)]\t training loss: 0.000334\n",
      "epoch: 19 [6080/60000 (10%)]\t training loss: 0.051489\n",
      "epoch: 19 [6400/60000 (11%)]\t training loss: 0.000322\n",
      "epoch: 19 [6720/60000 (11%)]\t training loss: 0.000028\n",
      "epoch: 19 [7040/60000 (12%)]\t training loss: 0.001296\n",
      "epoch: 19 [7360/60000 (12%)]\t training loss: 0.000009\n",
      "epoch: 19 [7680/60000 (13%)]\t training loss: 0.001832\n",
      "epoch: 19 [8000/60000 (13%)]\t training loss: 0.142499\n",
      "epoch: 19 [8320/60000 (14%)]\t training loss: 0.000064\n",
      "epoch: 19 [8640/60000 (14%)]\t training loss: 0.003294\n",
      "epoch: 19 [8960/60000 (15%)]\t training loss: 0.000017\n",
      "epoch: 19 [9280/60000 (15%)]\t training loss: 0.000007\n",
      "epoch: 19 [9600/60000 (16%)]\t training loss: 0.000033\n",
      "epoch: 19 [9920/60000 (17%)]\t training loss: 0.010063\n",
      "epoch: 19 [10240/60000 (17%)]\t training loss: 0.000387\n",
      "epoch: 19 [10560/60000 (18%)]\t training loss: 0.000085\n",
      "epoch: 19 [10880/60000 (18%)]\t training loss: 0.005844\n",
      "epoch: 19 [11200/60000 (19%)]\t training loss: 0.000527\n",
      "epoch: 19 [11520/60000 (19%)]\t training loss: 0.006608\n",
      "epoch: 19 [11840/60000 (20%)]\t training loss: 0.001144\n",
      "epoch: 19 [12160/60000 (20%)]\t training loss: 0.021943\n",
      "epoch: 19 [12480/60000 (21%)]\t training loss: 0.000025\n",
      "epoch: 19 [12800/60000 (21%)]\t training loss: 0.000173\n",
      "epoch: 19 [13120/60000 (22%)]\t training loss: 0.011473\n",
      "epoch: 19 [13440/60000 (22%)]\t training loss: 0.003427\n",
      "epoch: 19 [13760/60000 (23%)]\t training loss: 0.000024\n",
      "epoch: 19 [14080/60000 (23%)]\t training loss: 0.059782\n",
      "epoch: 19 [14400/60000 (24%)]\t training loss: 0.043043\n",
      "epoch: 19 [14720/60000 (25%)]\t training loss: 0.009629\n",
      "epoch: 19 [15040/60000 (25%)]\t training loss: 0.100112\n",
      "epoch: 19 [15360/60000 (26%)]\t training loss: 0.000140\n",
      "epoch: 19 [15680/60000 (26%)]\t training loss: 0.191096\n",
      "epoch: 19 [16000/60000 (27%)]\t training loss: 0.000159\n",
      "epoch: 19 [16320/60000 (27%)]\t training loss: 0.028123\n",
      "epoch: 19 [16640/60000 (28%)]\t training loss: 0.000341\n",
      "epoch: 19 [16960/60000 (28%)]\t training loss: 0.243305\n",
      "epoch: 19 [17280/60000 (29%)]\t training loss: 0.002031\n",
      "epoch: 19 [17600/60000 (29%)]\t training loss: 0.002807\n",
      "epoch: 19 [17920/60000 (30%)]\t training loss: 0.000423\n",
      "epoch: 19 [18240/60000 (30%)]\t training loss: 0.011355\n",
      "epoch: 19 [18560/60000 (31%)]\t training loss: 0.000774\n",
      "epoch: 19 [18880/60000 (31%)]\t training loss: 0.009595\n",
      "epoch: 19 [19200/60000 (32%)]\t training loss: 0.068743\n",
      "epoch: 19 [19520/60000 (33%)]\t training loss: 0.039070\n",
      "epoch: 19 [19840/60000 (33%)]\t training loss: 0.011417\n",
      "epoch: 19 [20160/60000 (34%)]\t training loss: 0.016089\n",
      "epoch: 19 [20480/60000 (34%)]\t training loss: 0.105069\n",
      "epoch: 19 [20800/60000 (35%)]\t training loss: 0.000187\n",
      "epoch: 19 [21120/60000 (35%)]\t training loss: 0.000290\n",
      "epoch: 19 [21440/60000 (36%)]\t training loss: 0.000520\n",
      "epoch: 19 [21760/60000 (36%)]\t training loss: 0.047360\n",
      "epoch: 19 [22080/60000 (37%)]\t training loss: 0.001429\n",
      "epoch: 19 [22400/60000 (37%)]\t training loss: 0.011304\n",
      "epoch: 19 [22720/60000 (38%)]\t training loss: 0.183808\n",
      "epoch: 19 [23040/60000 (38%)]\t training loss: 0.001497\n",
      "epoch: 19 [23360/60000 (39%)]\t training loss: 0.000700\n",
      "epoch: 19 [23680/60000 (39%)]\t training loss: 0.000569\n",
      "epoch: 19 [24000/60000 (40%)]\t training loss: 0.018065\n",
      "epoch: 19 [24320/60000 (41%)]\t training loss: 0.000605\n",
      "epoch: 19 [24640/60000 (41%)]\t training loss: 0.008785\n",
      "epoch: 19 [24960/60000 (42%)]\t training loss: 0.000024\n",
      "epoch: 19 [25280/60000 (42%)]\t training loss: 0.000044\n",
      "epoch: 19 [25600/60000 (43%)]\t training loss: 0.169404\n",
      "epoch: 19 [25920/60000 (43%)]\t training loss: 0.069572\n",
      "epoch: 19 [26240/60000 (44%)]\t training loss: 0.073072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 [26560/60000 (44%)]\t training loss: 0.000801\n",
      "epoch: 19 [26880/60000 (45%)]\t training loss: 0.000048\n",
      "epoch: 19 [27200/60000 (45%)]\t training loss: 0.004701\n",
      "epoch: 19 [27520/60000 (46%)]\t training loss: 0.000807\n",
      "epoch: 19 [27840/60000 (46%)]\t training loss: 0.000352\n",
      "epoch: 19 [28160/60000 (47%)]\t training loss: 0.003419\n",
      "epoch: 19 [28480/60000 (47%)]\t training loss: 0.001661\n",
      "epoch: 19 [28800/60000 (48%)]\t training loss: 0.095073\n",
      "epoch: 19 [29120/60000 (49%)]\t training loss: 0.000490\n",
      "epoch: 19 [29440/60000 (49%)]\t training loss: 0.142101\n",
      "epoch: 19 [29760/60000 (50%)]\t training loss: 0.000082\n",
      "epoch: 19 [30080/60000 (50%)]\t training loss: 0.000288\n",
      "epoch: 19 [30400/60000 (51%)]\t training loss: 0.000223\n",
      "epoch: 19 [30720/60000 (51%)]\t training loss: 0.000349\n",
      "epoch: 19 [31040/60000 (52%)]\t training loss: 0.001759\n",
      "epoch: 19 [31360/60000 (52%)]\t training loss: 0.000032\n",
      "epoch: 19 [31680/60000 (53%)]\t training loss: 0.004315\n",
      "epoch: 19 [32000/60000 (53%)]\t training loss: 0.000585\n",
      "epoch: 19 [32320/60000 (54%)]\t training loss: 0.000292\n",
      "epoch: 19 [32640/60000 (54%)]\t training loss: 0.000222\n",
      "epoch: 19 [32960/60000 (55%)]\t training loss: 0.001495\n",
      "epoch: 19 [33280/60000 (55%)]\t training loss: 0.000019\n",
      "epoch: 19 [33600/60000 (56%)]\t training loss: 0.000000\n",
      "epoch: 19 [33920/60000 (57%)]\t training loss: 0.018745\n",
      "epoch: 19 [34240/60000 (57%)]\t training loss: 0.006443\n",
      "epoch: 19 [34560/60000 (58%)]\t training loss: 0.004759\n",
      "epoch: 19 [34880/60000 (58%)]\t training loss: 0.122231\n",
      "epoch: 19 [35200/60000 (59%)]\t training loss: 0.008089\n",
      "epoch: 19 [35520/60000 (59%)]\t training loss: 0.049206\n",
      "epoch: 19 [35840/60000 (60%)]\t training loss: 0.010173\n",
      "epoch: 19 [36160/60000 (60%)]\t training loss: 0.001332\n",
      "epoch: 19 [36480/60000 (61%)]\t training loss: 0.006521\n",
      "epoch: 19 [36800/60000 (61%)]\t training loss: 0.007556\n",
      "epoch: 19 [37120/60000 (62%)]\t training loss: 0.000064\n",
      "epoch: 19 [37440/60000 (62%)]\t training loss: 0.019784\n",
      "epoch: 19 [37760/60000 (63%)]\t training loss: 0.000026\n",
      "epoch: 19 [38080/60000 (63%)]\t training loss: 0.000692\n",
      "epoch: 19 [38400/60000 (64%)]\t training loss: 0.012124\n",
      "epoch: 19 [38720/60000 (65%)]\t training loss: 0.012860\n",
      "epoch: 19 [39040/60000 (65%)]\t training loss: 0.000283\n",
      "epoch: 19 [39360/60000 (66%)]\t training loss: 0.001008\n",
      "epoch: 19 [39680/60000 (66%)]\t training loss: 0.172729\n",
      "epoch: 19 [40000/60000 (67%)]\t training loss: 0.009854\n",
      "epoch: 19 [40320/60000 (67%)]\t training loss: 0.003264\n",
      "epoch: 19 [40640/60000 (68%)]\t training loss: 0.000055\n",
      "epoch: 19 [40960/60000 (68%)]\t training loss: 0.045759\n",
      "epoch: 19 [41280/60000 (69%)]\t training loss: 0.024834\n",
      "epoch: 19 [41600/60000 (69%)]\t training loss: 0.005379\n",
      "epoch: 19 [41920/60000 (70%)]\t training loss: 0.000303\n",
      "epoch: 19 [42240/60000 (70%)]\t training loss: 0.003805\n",
      "epoch: 19 [42560/60000 (71%)]\t training loss: 0.000005\n",
      "epoch: 19 [42880/60000 (71%)]\t training loss: 0.000178\n",
      "epoch: 19 [43200/60000 (72%)]\t training loss: 0.000060\n",
      "epoch: 19 [43520/60000 (73%)]\t training loss: 0.065864\n",
      "epoch: 19 [43840/60000 (73%)]\t training loss: 0.003818\n",
      "epoch: 19 [44160/60000 (74%)]\t training loss: 0.003394\n",
      "epoch: 19 [44480/60000 (74%)]\t training loss: 0.006633\n",
      "epoch: 19 [44800/60000 (75%)]\t training loss: 0.006246\n",
      "epoch: 19 [45120/60000 (75%)]\t training loss: 0.011585\n",
      "epoch: 19 [45440/60000 (76%)]\t training loss: 0.000018\n",
      "epoch: 19 [45760/60000 (76%)]\t training loss: 0.000091\n",
      "epoch: 19 [46080/60000 (77%)]\t training loss: 0.000802\n",
      "epoch: 19 [46400/60000 (77%)]\t training loss: 0.002282\n",
      "epoch: 19 [46720/60000 (78%)]\t training loss: 0.033781\n",
      "epoch: 19 [47040/60000 (78%)]\t training loss: 0.001844\n",
      "epoch: 19 [47360/60000 (79%)]\t training loss: 0.000099\n",
      "epoch: 19 [47680/60000 (79%)]\t training loss: 0.026245\n",
      "epoch: 19 [48000/60000 (80%)]\t training loss: 0.001427\n",
      "epoch: 19 [48320/60000 (81%)]\t training loss: 0.000974\n",
      "epoch: 19 [48640/60000 (81%)]\t training loss: 0.002588\n",
      "epoch: 19 [48960/60000 (82%)]\t training loss: 0.000429\n",
      "epoch: 19 [49280/60000 (82%)]\t training loss: 0.000002\n",
      "epoch: 19 [49600/60000 (83%)]\t training loss: 0.000886\n",
      "epoch: 19 [49920/60000 (83%)]\t training loss: 0.000345\n",
      "epoch: 19 [50240/60000 (84%)]\t training loss: 0.000177\n",
      "epoch: 19 [50560/60000 (84%)]\t training loss: 0.000076\n",
      "epoch: 19 [50880/60000 (85%)]\t training loss: 0.039650\n",
      "epoch: 19 [51200/60000 (85%)]\t training loss: 0.094577\n",
      "epoch: 19 [51520/60000 (86%)]\t training loss: 0.038831\n",
      "epoch: 19 [51840/60000 (86%)]\t training loss: 0.003462\n",
      "epoch: 19 [52160/60000 (87%)]\t training loss: 0.009550\n",
      "epoch: 19 [52480/60000 (87%)]\t training loss: 0.000208\n",
      "epoch: 19 [52800/60000 (88%)]\t training loss: 0.001457\n",
      "epoch: 19 [53120/60000 (89%)]\t training loss: 0.007606\n",
      "epoch: 19 [53440/60000 (89%)]\t training loss: 0.004778\n",
      "epoch: 19 [53760/60000 (90%)]\t training loss: 0.013266\n",
      "epoch: 19 [54080/60000 (90%)]\t training loss: 0.001230\n",
      "epoch: 19 [54400/60000 (91%)]\t training loss: 0.013338\n",
      "epoch: 19 [54720/60000 (91%)]\t training loss: 0.001555\n",
      "epoch: 19 [55040/60000 (92%)]\t training loss: 0.002375\n",
      "epoch: 19 [55360/60000 (92%)]\t training loss: 0.017376\n",
      "epoch: 19 [55680/60000 (93%)]\t training loss: 0.009846\n",
      "epoch: 19 [56000/60000 (93%)]\t training loss: 0.000995\n",
      "epoch: 19 [56320/60000 (94%)]\t training loss: 0.000692\n",
      "epoch: 19 [56640/60000 (94%)]\t training loss: 0.026130\n",
      "epoch: 19 [56960/60000 (95%)]\t training loss: 0.180695\n",
      "epoch: 19 [57280/60000 (95%)]\t training loss: 0.092631\n",
      "epoch: 19 [57600/60000 (96%)]\t training loss: 0.090547\n",
      "epoch: 19 [57920/60000 (97%)]\t training loss: 0.000047\n",
      "epoch: 19 [58240/60000 (97%)]\t training loss: 0.004159\n",
      "epoch: 19 [58560/60000 (98%)]\t training loss: 0.059880\n",
      "epoch: 19 [58880/60000 (98%)]\t training loss: 0.002505\n",
      "epoch: 19 [59200/60000 (99%)]\t training loss: 0.001843\n",
      "epoch: 19 [59520/60000 (99%)]\t training loss: 0.015596\n",
      "epoch: 19 [59840/60000 (100%)]\t training loss: 0.001828\n",
      "\n",
      "Test dataset: Overall Loss: 0.0388, Overall Accuracy: 9891/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 20):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAANx0lEQVR4nO3dXahd9ZnH8d9Pp16YFt/ihGjVtFFBGRg7BB2YKI61voHESCiVEDOMkCoVGpmLiY5QUSo6TDsoSOUUtZnBsRSimIgvtVLMzE3JUYx5G+trSEJMDF7UkouOyTMXZ6Uc9ez/Ou611l47eb4fOOy913P2Xk928stae/33Wn9HhAAc+47ruwEAo0HYgSQIO5AEYQeSIOxAEn8xypXZ5tA/0LGI8EzLG23ZbV9j+y3b79he0+S1AHTLw46z2z5e0u8lfUfSbkmbJN0UEdsLz2HLDnSsiy37xZLeiYj3IuJPkn4paUmD1wPQoSZhP1PSrmmPd1fLPsP2KtuTticbrAtAQ50foIuICUkTErvxQJ+abNn3SDpr2uOvV8sAjKEmYd8k6Tzb37B9gqTvSVrfTlsA2jb0bnxEfGr7dkkvSTpe0uMRsa21zgC0auiht6FWxmd2oHOdfKkGwNGDsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkRjplM2a2ZEl5iryzzz67WH/44YcH1g4fPjxUT7N13HHl7UWT9a9bt65Yf+SRR4r1V199deh1H4vYsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoyzj8Btt91WrD/44IPF+oknnlisl8ayu56lt24cvcn6b7zxxmL9hBNOKNY3bdo0sHbw4MGhejqaNQq77Q8kfSLpkKRPI2JRG00BaF8bW/a/j4gDLbwOgA7xmR1IomnYQ9Kvbb9me9VMv2B7le1J25MN1wWggaa78YsjYo/tv5T0su3/jYiN038hIiYkTUiS7W6PFgEYqNGWPSL2VLf7JT0j6eI2mgLQvqHDbnuO7a8duS/pKklb22oMQLs87Dio7W9qamsuTX0c+K+I+HHNc1Luxr/11lvF+sKFCxu9vu2Bta7H2Uvr7nr9des+//zzB9befffdttsZGxEx4xsz9Gf2iHhP0l8P3RGAkWLoDUiCsANJEHYgCcIOJEHYgSQ4xRXHrGXLlg2s1Z1WfCxiyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDOPgJ1Uwc3PcUVM1u8ePHAGuPsAI5ZhB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsIzAxMVGsz58/v9Hrr169utHzS+6+++5i/eabb+5s3U3t2LGj7xbGClt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUhi6Cmbh1pZ0imbu3byyScPrM2dO7f43FtvvbVYX7p0abG+YMGCYr3Lf18bNmwo1pcvXz6wdvDgwbbbGRuDpmyu3bLbftz2fttbpy071fbLtt+ubk9ps1kA7ZvNbvwvJF3zuWVrJL0SEedJeqV6DGCM1YY9IjZK+vhzi5dIWlvdXyvphnbbAtC2Yb8bPy8i9lb3P5Q0b9Av2l4ladWQ6wHQksYnwkRElA68RcSEpAmJA3RAn4Ydettne74kVbf722sJQBeGDft6SSur+yslPdtOOwC6Ursbb/spSZdLmmt7t6QfSXpA0q9s3yJpp6Tvdtkkyp555pmBtUsvvXSEnYzWzp07i/VjeSx9GLVhj4ibBpS+3XIvADrE12WBJAg7kARhB5Ig7EAShB1IgktJj4Hnn3++WL/66quL9eOOG/x/9uHDh4fqabZK6+56/faMZ3JiALbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+wjcPrppxfrp512WrFedznm0lh215cKrxtH73L9K1asKNZfeOGFgbUXX3yx7XbGHlt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcXYctU466aRi/YknnhhYu/7664vPnZycHKqnccaWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9BD766KNi/cCBAyPqpH0bN24s1i+44IKBtblz57bdzmeUriNQdw2BY1Htlt3247b32946bdk9tvfYfqP6ua7bNgE0NZvd+F9IumaG5f8eERdVP+UpTQD0rjbsEbFR0scj6AVAh5ocoLvd9pvVbv4pg37J9irbk7aPvS8bA0eRYcP+M0kLJV0kaa+knwz6xYiYiIhFEbFoyHUBaMFQYY+IfRFxKCIOS/q5pIvbbQtA24YKu+350x4ulbR10O8CGA+14+y2n5J0uaS5tndL+pGky21fJCkkfSDp+9212I4FCxYU63XnN5fGkzdv3jxMS39WN894Xb00R/r7779ffG7pnG9Juu+++4r1Otdee+3A2nPPPdfotZvMDZ9xbvfasEfETTMsfqyDXgB0iK/LAkkQdiAJwg4kQdiBJAg7kESaU1yffPLJYv2SSy4p1kunoV5xxRXF527fvr1YX7NmTbF+6NChYr3kzjvvLNa3bm32FYm6Ic37779/YK3pdM5NpovueirrccSWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSSDPOXjcmW6d02eMNGzYUn7t8+fJive5S0nfccUex3sS5555brC9btqxYr/uzlS4ljdFiyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXiU5/Xa7u0k4nPOOadYr7us8TiPF5cui9z132/dJZm7XH/durdt2zawVnfp8J07dw7V0ziIiBnfGLbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEmvPZ68ZNd+3aVaxfeOGFbbbTqtLUxU3P42+y7q7Xv2XLlmL9yiuvHFiru4bAsah2y277LNu/tb3d9jbbP6yWn2r7ZdtvV7endN8ugGHNZjf+U0n/FBEXSvpbST+wfaGkNZJeiYjzJL1SPQYwpmrDHhF7I+L16v4nknZIOlPSEklrq19bK+mGjnoE0IIv9Znd9gJJ35L0O0nzImJvVfpQ0rwBz1klaVWDHgG0YNZH421/VdI6Sasj4g/TazF1tsOMZzxExERELIqIRY06BdDIrMJu+yuaCvqTEfF0tXif7flVfb6k/d20CKANtbvxnjqP8DFJOyLip9NK6yWtlPRAdftsJx2OyL333lusX3XVVSPq5MsrDW91fYprk2mTm3r00UeL9YzDayWz+cz+d5JWSNpi+41q2V2aCvmvbN8iaaek73bSIYBW1IY9Iv5H0qCrBHy73XYAdIWvywJJEHYgCcIOJEHYgSQIO5BEmktJ15kzZ06xvnjx4oG1yy67rPjcFStWFOtnnHFGsV7naL2U9ObNm4vPveuuu4r1l156qVjPiktJA8kRdiAJwg4kQdiBJAg7kARhB5Ig7EASjLOPQN100XXTB9d56KGHBta6/vtdvXr10M/dsGFDsX40T5vcJ8bZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtmBYwzj7EByhB1IgrADSRB2IAnCDiRB2IEkCDuQRG3YbZ9l+7e2t9veZvuH1fJ7bO+x/Ub1c1337QIYVu2XamzPlzQ/Il63/TVJr0m6QVPzsf8xIv5t1ivjSzVA5wZ9qWY287PvlbS3uv+J7R2Szmy3PQBd+1Kf2W0vkPQtSb+rFt1u+03bj9s+ZcBzVtmetD3ZrFUATcz6u/G2vyrpVUk/joinbc+TdEBSSLpPU7v6/1jzGuzGAx0btBs/q7Db/oqk5yS9FBE/naG+QNJzEfFXNa9D2IGODX0ijKem6XxM0o7pQa8O3B2xVNLWpk0C6M5sjsYvlvTfkrZIOlwtvkvSTZIu0tRu/AeSvl8dzCu9Flt2oGONduPbQtiB7nE+O5AcYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IInaC0627ICkndMez62WjaNx7W1c+5LobVht9nbOoMJIz2f/wsrtyYhY1FsDBePa27j2JdHbsEbVG7vxQBKEHUii77BP9Lz+knHtbVz7kuhtWCPprdfP7ABGp+8tO4ARIexAEr2E3fY1tt+y/Y7tNX30MIjtD2xvqaah7nV+umoOvf22t05bdqrtl22/Xd3OOMdeT72NxTTehWnGe33v+p7+fOSf2W0fL+n3kr4jabekTZJuiojtI21kANsfSFoUEb1/AcP2ZZL+KOk/jkytZftfJX0cEQ9U/1GeEhH/PCa93aMvOY13R70Nmmb8H9Tje9fm9OfD6GPLfrGkdyLivYj4k6RfSlrSQx9jLyI2Svr4c4uXSFpb3V+rqX8sIzegt7EQEXsj4vXq/ieSjkwz3ut7V+hrJPoI+5mSdk17vFvjNd97SPq17ddsr+q7mRnMmzbN1oeS5vXZzAxqp/Eepc9NMz42790w0583xQG6L1ocEX8j6VpJP6h2V8dSTH0GG6ex059JWqipOQD3SvpJn81U04yvk7Q6Iv4wvdbnezdDXyN53/oI+x5JZ017/PVq2ViIiD3V7X5Jz2jqY8c42XdkBt3qdn/P/fxZROyLiEMRcVjSz9Xje1dNM75O0pMR8XS1uPf3bqa+RvW+9RH2TZLOs/0N2ydI+p6k9T308QW251QHTmR7jqSrNH5TUa+XtLK6v1LSsz328hnjMo33oGnG1fN71/v05xEx8h9J12nqiPy7kv6ljx4G9PVNSZurn2199ybpKU3t1v2fpo5t3CLpNEmvSHpb0m8knTpGvf2npqb2flNTwZrfU2+LNbWL/qakN6qf6/p+7wp9jeR94+uyQBIcoAOSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJP4ffIV2Q5pdRYcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 3\n",
      "Ground truth is : 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###visualize filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1)),\n",
       " Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
       " Dropout2d(p=0.1, inplace=False),\n",
       " Dropout2d(p=0.25, inplace=False),\n",
       " Linear(in_features=4608, out_features=64, bias=True),\n",
       " Linear(in_features=64, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_children_list = list(model.children())\n",
    "convolutional_layers = []\n",
    "model_parameters = []\n",
    "model_children_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_children_list)):\n",
    "    if type(model_children_list[i]) == nn.Conv2d:\n",
    "        model_parameters.append(model_children_list[i].weight)\n",
    "        convolutional_layers.append(model_children_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "for i, flt in enumerate(model_parameters[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(flt[0, :, :].detach(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x576 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 8))\n",
    "for i, flt in enumerate(model_parameters[1]):\n",
    "    plt.subplot(8, 4, i+1)\n",
    "    plt.imshow(flt[0, :, :].detach(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###visualize feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_layer_results = [convolutional_layers[0](sample_data)]\n",
    "for i in range(1, len(convolutional_layers)):\n",
    "    per_layer_results.append(convolutional_layers[i](per_layer_results[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 26, 26])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "layer_visualisation = per_layer_results[0][0, :, :, :]\n",
    "layer_visualisation = layer_visualisation.data\n",
    "print(layer_visualisation.size())\n",
    "for i, flt in enumerate(layer_visualisation):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(flt, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24, 24])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARcAAAHBCAYAAABDtP/mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOy9SXNca3Le/9QAoOa5MJG8vPf2cLvVLclh2eGFv4pDdnjptTfeyGFtvPHaS4et8OfwVguHZMktdfftO3HCWPNchZr+C/5/iTyHIAgQl6xDR2VEBUigUDjnPe+bw5NPZsbW67W2spWtbOXHlvimL2ArW9nK/5uyVS5b2cpWPohslctWtrKVDyJb5bKVrWzlg8hWuWxlK1v5ILJVLlvZylY+iCRv++H//t//+9Y89Xq91nq91mq10nK51NXVlcbjsbrdriaTiXq9ngaDgdrttmazmcbjsVarlVar1RufFYvFFIvFlEwmlUwmVSgUlEqldHBwoFwup3q9rkwmo0wmo2QyqXg8rlgs9tZr++f//J+//Yc/ovzlX/7lnXL5rNN8Ptd8PtdkMrH1mk6nGo1Gms/nms1m9v6bJBaLKR6PKx6PK51Oa3d319Yql8tpb29POzs7SiQSt66PJP3FX/zFR1kjSe9co9VqpcVioeVyqfl8rul0qn6/r+l0quFwqPF4rF6vp6urK02nU83nc11dXb3xOYlEQvF4XLu7u0okEsrn89rb21OxWFQ6nVa5XFY6nVYmk9Hu7q52dnbetU4fZY3+/M///J1rxHOXXq8XZyYejyuRSCiRSGhnZ0epVMrub29v743P8WdwvV7b565WK63Xa41GI11dXanX62mxWOjq6urGM4v81V/91Y1rdKtyuU04LIvFwg5Lu91Wt9vVq1ev1O/31el0NB6PNRgMNJ/PtVgstFgsAhfKv1EWbI58Pq9UKqVWq6V8Pq/5fK5SqaR6va5UKqXd3V1JeucBioL4dRqNRhqPx+p0OppOpxoMBppMJur3+6ZsWNu3CfecSqW0s7Ojer2ufD6vx48fK5fLqVAo2OH6FNYHhTIcDm0fdTodvXjxQt1uV+fn5xqNRup2u7aPZrPZjcqF+y6VSspkMqrVasrlcqrVaspmszo6OlI+n9f+/r7S6bRyuZySyfc+Bh9FvAJJJpOmYP3PY7GYUqmUKRkMTSaTkRQ0VjgFOzs7SiaTpohYh2azqclkopcvX2o4HNrevK88aFW5YDwXrPJisXj94cmkUqmU3cxyubz14PgF2NvbUyKRsN/jc5fLpWnYT+Hg+HvH2l5dXWk2m2k2m2m5XCoejyuXy+nq6krJZPJWxYLlkq431Xq9ts+ez+e2Pp/CGq3Xa7OOo9FIg8FAnU5H7XZbzWZT/X5f3W5X0+lU0+nU9tpt5E/WyFt2fg8lP5/PtbOz88nsJf+sV6uVJpOJYrGYKQi8FLz7dDqtVCqlVColKWjEuWd+Fw94b2/P9t9oNFKr1dJ8PjfDf1/C7YM8l+VyaQ96uVyaS5/NZlUsFvX06VPF43FTMul0WslkUolE4o3PQ3n0+31NJhONx2NdXV3Zg+cAzedzJZNJ7ezsKB6PR35j+LUhVOSgSFI6nValUtHR0ZH9DhuJfyNYLx5ys9nUcDhUu90OeD2ZTMY8wKivz2KxMOv4/PlzdbtdvXz5Ur1eT6enp3ZfOzs7KhaLdpBYi7Ds7Oxod3dXmUxGqVTKPBj23Xq91tXVlSaTiSTZz6K8RvF4XHt7e1qtVppOp+r1enrx4oUymYyq1aoymYyOj4+Vz+dVqVSUyWSUz+eVTCa1t7f3hlLg3OLZEgkAQdRqNY3HY00mEyWTSQ2Hw4DRuqs8yHPBQsTjcS0WC6XTacViMVMAuFqJREJ7e3sBt+2NC0kmzbLEYjFTWovF4g1sh5gx6geHa/YPJZFI2Ob3FsSHMHz1Xgr/93F3LpdTPB7X1dWVhsOhKWDv3UVdVquVKV7CoqurK9v8/p5ZF/aGv7/w2uHi4+16LxjvBc/F4xdRFOCC1WplXi8Hn5+l02nzPCQZTuKxFT6L/cH9cl7L5bLW67X29vYUi8VMQXOGb/Oob5L3Vi54JDs7O0qn05pOp0qn0wa88R7vouP+3iR+s/hN5ZUKm+Lq6kqpVEqr1epGRRUV8Yc9Fotpb29PlUrFNgTgdywWU6fTCfwuD94rUNaHGDmfz6tcLmtnZ0eDwcDwm6urKwMr3wV8b1rm87lhLK1Wyzw6wPzRaBQIa6bTqYWTy+Xyjc9j3wBsf/7556pUKgZ681nT6dQUM/stqusUj8eVSqW0WCzUbrdtvfBMstmsCoWCKQC8Y68wvXLxxhqFK10r5Gq1ah7McrnU+fn5rWv+NnmQcsEFTafTkmQxnHRtMcAauKF3gZU+Y4AyQTH5z/wULLN/gNL1JpFeW2wyYz77hbJ820Zfr9cBb5BsUSKR0HQ6DazTp4C9+HCXfZFMJu37WGsME2DmbcolFouZcu12u9rZ2dHOzo79nKydX6uoC/vCnwuunxfGGMMP3hQ+J/yfvTGbzQJJB/6ezzrt7OwEQOS7yIOUSzab1d7enkqlkpLJpHq9nqVWh8OhOp2OJpOJRqOR/V5YKbChCAvwhrD43Ph4PFY8HtdsNrMQyseNURQOCLK3t6dCoaDJZKJutyvpNT7FPe/u7hqecNs9YZFIr5JNI4XIuqXT6cgrYMBDPNpkMqlcLmdZxvF4rHa7bdmi8MEKrxNr57E98DvWHuAzkUhoNpvZQQqHoVERjy+NRiMLH8fjsYbDoTKZjHq9nmWI0um0GRyP0b1NSPkvl0uNx2PztMvlsmKxmAqFgtEl7iPvrVw8Eo/4sIYDw4P2uXS0pN8ouGc+rr4ptvaYQ1SVSli8t5ZKpQyXwtVHsezs7AR4CWGMySvmWCxm64jHmM1m7bBEXal48dgd+8qHxuwlQmHvvfIKJxckWRhFKBXGo7xnF2W56fowXOPx2AywT35wnx6zDNM9/BlmP2GcJNma413f97w9CNDlj3kClPSaa4BSIWza3d1VKpWyTdDr9cy76fV6lpoNbwAWjEXgIEYZa/ESj8eNB5ROp1UsFrVarZTL5eznWBuUMgdmMploOp3aV9bYu7PL5dK4P8fHx0qlUppMJvcG3zYlbHYUCOHifD63vZPL5bS7u6tsNmvvZU+gPLDmk8nEDttyuTQOUaVSMZDTH74oh4xeuFaPRU6nU11cXCiTydi+iMVixgPCeLOfUBIeqPUKh3VEIWWzWQN28ajvIw8m0fnYd2dnR5lMJoDKY43ZFLBQOTQ+E+A9G0kWO3OD2WxWqVTKPi/KIZGkN6zCer22bBHKRZIpXtaIteN3PTbgATrPXdjZ2VGhUJAkS02HsyxRFLgaZBOl6xB5vV5bFsTzp8Dg4GCQXp7P52+9V5TX3t5egMX8KewjzxEjdOawh88L67ZYLAKUjuVyadwXjH4YewkLyRNv1O4jD1IuxMl8zefzxkXw4Us8HrcDMxwOLZb2tOWwguHhsxFqtZqKxaIqlYptwihvCOnaKqMgyG6VSiU9fvzYPD4UDvRt2Kf9fl+DwSBAngpbW1LZqVRKjx8/1tXVlb799lsNBgPLFkVZYJNCZ+B78XhcT548CWQKOTDD4VDdblfdblftdjtAIvR8Fun6GaTTaWWzWZXLZWWzWWUyGVNoGLCoCgA0HKarqyvl83lJMsXhvXvCwU6no0ajYZyVcrls+CgYE4YvrKCk1/gOmafZbPZxlQuAGmCQp/ZzcCAssSnI0Q8GA3Nlb0P/fQzOpnufnPumxFvG5XKp4XBofCAeLMAcawCXgf+jbHDlEa/AJVm4kE6nTWnxN6IqhIUe/OY5Q3XncPR6PbXbbV1eXqrdbqvRaGg0Gmk0Gpk3TPhIWA5505M4Pd4Qxg2jKoQqhIrFYtF+xjqNRiMDqYfDoYbDoa1HOIvkjRPfRwFjDAeDgbGj3yer9t7KBfITaUTPY6HQDO7C6empms2mzs7O7IZ8GhbqsSfd8T7p+hDxfn4e5UMjKXDNsVhMV1dXuri4sLooFMmrV6/0j//4j+r1eup0OqZMyuWyyuWy0bm91+KxChQ5a1gsFm2T3QSKR0l8HQweBAo5mUyq3+9rPB6r3+/rm2++0fPnz/W73/1O5+fnOjk5sXCHFwoVhQIXKJ/PWx1ROBSPMoFOkgGtq9XKcKdEImHGJpPJBDJrGCegB49XEgkQTnqyYqlUUrVatazs+fm5Li8vTcF8VM8FlxUlQ5qs0WhoMBio0Wio3++r2WxqOp1qPB7bYZOus0r+YXPDKCwODq7ZbDYL5O6jvjF4sLihw+FQ5+fn+s1vfiPp9ca5uLgwgHsymQTCnpsyRf5AAOwB8OIhfQrWWLrGiySZ5wsQy176/vvvdXp6qu+++06Xl5dqtVqG1/lMmy9A9IZrZ2fHwnjPJt/Z2fkkgF2Pr3mGN0YjkUhoNBoFFG25XLbf5XwVCgXlcjlLHvC57CcSC5LMUcBAEbLeRx5cWzSfzzUYDDQajXR+fq5ms6lvv/1W7XZbr169sotMpVLKZrMG0BHvsilQMoQCAFFwaQaDgfETQL4BA6O6KaTXhwei22w2U6vVUqPR0O9+9zu7H1jNWBvWh/CPA+fdeA9CwogO0+Wl6Ht3bGoPzI5GI11eXurrr7/W8+fP9X/+z//R+fm5fvjhB1uLePx1ywnaJ7AmZBsByXlRdT2dTi1tD59qsViYNxlF8RAE5wWKfjwe13Q6VbvdtjN1eHiozz77TNlsVvl8PlBn5vcDGSUUMDVJGPLxeGwcJEKr+8iDeS7+YOPSplIpiwnRrsRzHBz/O/4rCwAbEw0LNkNNSNQR/rAQIu3t7QXSy8PhUPF4XJlMxqywJ015MJPPQFh/z4SWZBsvqqGQF495sC5Q3JvNpnq9XqD2BSvrjVIikQhkNLDGXsFeXV2ZIpMUCC+j7v0i4EjwW6Tr+8jlclYwXKlULH3vSyfe5u2z33yFNWeWLOb7rM+DeS5hUg41L4VCQY8ePQrUH+VyOdtM9DWZTqcWT+Kiepo3YdF4PLb34xp+Cq4/Dx+lkM1mzYridtbrdX322WeBzIjPxPkGSOENgnIhW7JcLq0i1hOooirsHwxIr9fT+fm5zs7O9PLlS3W7XV1dXSmdTuvx48eWhsVKg09RV8Ua8tkIVjpckvEpKBXpGn/c3d0NNHJarVYqFAo6PDxUtVrV0dGReSwI54n9gnFGMFo+xU2jLcKt9zlrD1YuOzs7yufz5lINh0OVy+UAY5IKTOLdMGHuJhZuGNGGDJXNZs3t5b1Rjpel61QiLmk+n9eXX35pZQAcEp9u5oGjSG/qvufXlM9CyfA7HrSMqnDfeHNgI9Vq1VLF4G6eW4WCJiPpsxoeD0Tp+GSAZ+4CdEbdg/HZHp4zOCZ7CyLiaDSyM+U5Vj5s9hlLfy4BjDnTmUzGzmu42dtt8mDlQktKGJQg1rPZTKPRyGoSqDHyVhvlEgaLwq7qarXSYDDQarXS7u6uZrOZstmsAXrh0CpqQmZNkvFc6vW69SwhvpWCbSx9vREHI+zOxuNxO1yXl5cGCqfTaZVKJfsMPjuqEvZOk8mkDg8PtVgsrDoa0JsaNqgNZCzDDFzCh729PS0WC1PQ6/Va4/FYu7u71rrA4zZRFb/fwaa63a4pXyIHkisoF0Be9lCY+o/xIyyl7ooGZvl83vbYfUomHoS5SLJYDe1Plmc2m6nX69kCEOaEORssCBcMuEcTHxYVDIGXb5kZZcvsMzw8RJQJSLwUrMfi3/zfZ9i4TwBer9wvLi4Ui8Ws+Aw+zdsadEVB/DO9qdIdr6zf76vVahkJk4MQ5lf5MAsyGbiWr0nj53g+8IKiupfC14Ty5P4uLy+1t7enarUaaKiFwmQPsI+898G/e72eYrGYtQCFEFutVs3A36ftwoObRfn8Oe49lpq6oWazacogrPU4PP4Q3tTwxoNzvvDxpg0ZJQkrF1x+vDpP4cf78532/AbxYZNPTRaLReVyOSuKJLQgU3CXSutNi783/z28j36/r8vLS6M7hBtH49WFXf3xeKxYLGaZSSl4UDF6eDYeMI+ieOOCN9Hr9XR5ealUKqV6va5CoWDZML+PwgkAr9jX67VxrACDS6VSoK/LxcWFRRwP9lzCMZuvtJRk9HwYlvF4XN1uV81mU91u1ywMygECDylE4umdnR27aEhTfI+/zfv4XUhXUTk0Yevrtbsv7PSHgawa+ADpU74S0rDp+QyfHhyNRpaFo3Cx0+loPp+/szfMxxbvUvt6GV+9zH35UIdwaTKZWNbMTzvwVtobH/YkhwEvmUJarHlUMo+eu+QPL9imL1fA2OLB+q7/1A+hZPlc9g8pefYinwlc0e/3lUgkVKlUtLOzo1wuZ8DufXCpd6ppb1F48PQfhYTkqzJxX+mMRgiEh1IsFlUqlVSpVFQul03hgBVcXFyo2+0apdl7JigzlEuUOq35cM2nhWFS8j3W6SY3FSUCdwNmLm4u+AJkPAA4DtnBwYHy+bxWq5WGw2FkAEqen1cmcFrCysVjKD6EBLDlsJARyefzxloF3EbpEjqxj/BgSPuHlcsm18pjIGHjhDePcQV8lmTvg89DzZRnsfNvQPFut6t+v2+fX61WTfnE43H1+32r54Ofls1mAxMC7iK3vpNNAE7SaDQs3JFk6SrIbrVaTbFYTPl8PoAxkEqkHV+pVLILxuIQP/uuZCwyCxhmq0YhHApjQP1+3+oypGuANvxQfHaCz8D6cHDYSPwdSGLj8Thg3ekQBjCZTqe1XC7f4MRsSlCq4/HYwEbcetau3++r0WiYp4I1Bpj2XIxUKqVyuaxKpWJGBuVDShqmc7jwkX/T5pLmY5sOrz0mBuDsw7n9/X1ls1nV63Utl0tr4MQ5896dJ2ECOXCOO52OXr16pYuLC/tsb6i9MSSED3OC7irvVC7gJ5PJxLrNX1xc2OYtFova3d1VuVw24lwulwtoy/l8rnw+r1KpZF8Jczym4i18OBQL4w2b3gxeOCAg+BDBcDtpscDhQJGE66iwPFSX014C5e5Jij4jQBqbBs38PCoAJd4rYTJhm88W0tsHJeA7+BcKBTsEuVxOpVJJtVpNBwcH9jdarZY6nU6AzSwFDRL7ixQ1+NTu7u7G9xP7wmNFeOq7u7uBvraTyUS5XE6DwSCgXFCq+Xw+wAfyRqjf7+v8/FzPnz83r69Wq5lDIF2fe99m9kdXLvyxcCrPz8dZrVY6OzszABEWJGQ6Dh2HYDKZ6Pz8PDC/B1IQdGOIcigcT/n2zaS8YtrUAQorO6zF7u6ujYGIxWLq9Xo2+a9QKJgn55nMeDiLxcIqfgEzh8NhoFrYe3t+fWgv4MsFNq1cGKp1enpqbjnPjXCbZ4+VRbnk83kdHR3ZwePQgCe0Wi21Wi09f/5cJycnFl6jpAiJ6vW6pGvGM8ra18/wtzexXjs7O1osFup2uxqPxzo7OzMIIB6P6/DwUKVSScfHx9rd3dXp6akymYza7bathc+crVYrCzMBwU9PT3VycmLtQkkgeC4R+8ZngTGE9y3yvFW54NJ7YJEDjyZcr9dqt9sWqwG0+dQ0DEpivV6vp16vZ+DtZDJ5g6AD2u3BP+LCcMZkkxbHe1VeQXi+AWEd6XXo7EwE9J+1Wq3U6XRs0uDZ2Zn9n02wv7+vYrEYKPTzhDkAYYD3TSsYDAaT/Pr9vrn8HALfQQ5vjtorJhyQOFiv17af2u22nj17pq+//lrff/+9MXFx9Vl730qV/XuTodyU0CSM3sGdTsfqhOLxuMrlsqrVqg4PDyVJT58+1Xw+1w8//GBFwWEPHwVL75tGo2FGC2fBJ1W8cvFhKLiUdL/w+lblAo08l8tpsVioXC6r3++rUCio3+/r5OTEFuTi4sIaQHkcAavEAvCSXh8mNoF0TRLyWhNg1N9suDXDJg8ONH0s7fHxsTGKR6ORqtWqut2ufvjhBwO7O52Orq6u9OjRI/3yl780SzEajaxZMoxbyt19yhq3nnv3nCO/djyHTXsuvtlQIpGwGiE8LMBn4n2GcE2nU2vw5Fm8NIvqdDo6OTkxj4UQB8OUSCTMcwG09d5fuFHZJoVroHgVz4qxyOfn54rH43r69KkKhYJ+8YtfKJ1Om7JuNpva3983Apyf7onHV6vVtLOzY0246vW6edJ42uH94nEr/n1XuVW58JBwKfP5vM19piM7jZ+m06lardYb/TtRKvAJyAAAPNJrA6/IF095HCZ8cLw23aSAbeDCHh0dmYdFKHN2dqbnz59bjwyszGAwsHtPJBLWOQyrQ5tCD17DU8DyenId/+eFJdv0wfFVtSgVsmJ+9jPuPV7MaDQKUM85SMztaTabajabajQati/Isnkw21tjr2DCymWTtWoeo/SFqpyvTqdjbUzpl7xardRsNtVut41MSaKAEAkjxM99J4JisRiYJY0xkq4hkbByuU8i5VblwuJz4+AJhUJBi8VClUrF8ADPUaF/C9+n2fLe3p7y+bzq9fpbGyHx1SsbSRYbEnuGu4ptSkDTB4OBHSDWK5VK6dGjR0qn09Yoivs7Pz83/CGbzSqXy5nVYh1Ye/5Pj1jCRbqGodgYNQJmRVZp09m1bDZrGUVCOA4G+6bb7eri4sIUiE/nr9dr2098xehQg5TL5ZTJZKxfCeEO2ZJCoaBCoaBqtWrrRN9h0qyb5LtwL8ViUbFYTBcXF5rP53rx4oXG47FqtZr6/b5KpZJ5+6vVSrVaze6bcBgFSzYWz9GPEfaeoE+D+x69ZOBarZba7bY5B3eVd2IufrHR+mxm+nlywMAYOp2OTRNE6/nZtbVaTbVaTdVq1RYKkA1LBobAV7SsZx+Gr20T4osxvSLwxKdYLKbHjx8rFovp5OTE6mQIGXO5XKC5D64nREOEtfSNqT3Gg6Km541vuOWt0scWAFhfA4NnR+dC31sZ75gDgVKBN0WxIV334LsUCgXLMHkqAGsJiA6bmbQ2bQU26bnwzDlTdBKkR/Dp6amSyaROT08tY7ZarUwx+jYm4Ch8H1wT2ofH6ogKPM4C7IDHyfPxWdy7yK3KhVGjkObQgAyLR/N//vnngapKT89m8/vGPlDciYl9EaN347HK3sLcdOAkbcz9T6VSurq6svaCz549Uz6f1/HxsSTZOh0cHBgzFNo2yod2FHgXhJIehCRu9geCVPVN/wYL87+/KWGeEmEuNPNer6fJZBLIjGEZ0+m0KpWKpOsyAN9X2IfJnqE7n89Ncc/nc2vGfXR0pKOjI2vQTWiAMtq0BwwmRUKElgncC2RC0vjsd9bBM7o9v8W3n/AERp9l5TOgP9Aw/fT0VP1+X69evTLsz/eFeZfcqlxwmdjkYZAI4g0ums/T+/AGTUpRIoATFdJkNfwBCGeqODQopXANyqY2BqljtDyTFAnnJpOJVZiCA/B+DgcKBuWCFQ83L8eiQJLyhY2+hsTjLngKmxR4JGAqvqE2mAJpdQ4DtHPuDQzBHwzvaZDpgNrAHuSwFAoFOzwet/NdEKXNecC+0xz73bfhYM/ggUCSQ7H6fUTEwe/7102eB2eSdWLv9no9tVotdbtd81zuWrQovUO5+I1JPMZmns/nlkr04K1PUXuQyKf8oHuzwTzPwGeLvEZmA7BgXBdh06aE0A03lQcA1wdAkhCR0MBT3z2uhRL3Fd/htYC9C/bEZmg0Grap/Gdsms0MRlQoFBSPx9Xr9cxI7O3tGQkskUhY+CPJUtaeEBc+IPzbl10sFguz/p9//rkODg50eHhonCIUu2ejbpqQyaElnNnf39fe3p7V+bTbbe3s7KjRaARqrDwYjWHx68X9+X3A+7hnvN5cLqdcLmfe0osXL9Ruty1hc991ulW5+PSnJOMZSAp4EL5zGjGd3wT+q2cL+kPgPRXPXvVWhs/GlYtKqpXwLYz0Ew4mk0kLc3ya2NPR/UPz1tl7N3iAuPK0NmTzsAGkYJHgJkMiSQbgwnvy/WUwGP4rqXf2FOI5HN69D39PuvZ8KpWKarWa4Sy+adem940Xnrcka9S0WCzMoyVUZl4ze+i2V9io+H4unF2iC983iFYp1Af6xm7S3XlltyoXn+b0B5r+K7i6sVjMal6IC32zKPqgejeP3+OggKdwgz51CdfGt+HzbuwmNwnXCK5ARfh3332nWq2mX//615Kkcrls9Gu/Rt7bC1tRz7j1kwJLpZKB22A+i8UiMBbXK7pNCwcBl9szaOfzuYrFosbjsXl51WrV1op9xFfPug3vo0wmYyUmX3zxhQ4PD/X48WOr7k0kErav8vm8WWtPEN2UeEUpyXCzcrlsBnk0Gum7776zZ+/3QCaTsTMaDpMRzgl/i/dgtDqdjvUuBgTm/d5TvKvcif7vF8DzATwhyWtSOAtwW6Tr/hMIDzudThvaDz8BYM438CEk8wvmQbgohEZ4EPP53Bi14TgaLgZoPAcHz8avEa47ILYPi8gMhb0ATwXfdL0MgpLc29vTcrlUNpsNeG3sIbxZPD3S8byX5+49YR9eZbNZq7Y/Pj7W8fGx6vW6VYpjCABO2VNRINLxnPjqizR9GQyZNCIGnr9PpftQ6SbYAAUb5vYAqJOwCZfa3Hcv3al+Gova6/UCbjYPh3w6MVu9XrebDx+csMvPZ3jXnkK3m4hOYawlKq4tIV25XNZqtdLl5aUmk4mFL/V6Xel0WrVaTev12lo4+h6wfPXenRRMo+LaU9lLepaU4Ww2s+/5lhWbVDKeQ8Hh9tQDjJPHiPCESddTl4bH4z+T4We5XE7ValWFQkHlclm5XM4+jz3lewoBgEaBL4V4wDSZTAbOEgb66upKrVbLlIHHXBKJhHUfIATnXEqy7C/rDgtYUmC9fXbO17D9aJiL50Z4YIiHi+Xx7RlRGNyw723qi+w8cYcbxqX3B8K/J8xriZJyka5Hk3IwJpOJWq2WtZyQpFKpFEgVUgNEWIVy8S8wF5QL9UqwVn2I4VsxbLpexgvenSS7Z54tyjGVStkzJzs2n89tPK0Ph3wKlvAmk8lYVz56tqC0vVHz2bSoML2lYPiBl8WaoHA5hz575I0Rnp73mDmTkiyiYM3j8esGWygXzqA/t/fJEiF38ly8osBCeuIYrhvuKQ+XGhI2gAc9OXykDlutVsAd48H7UQd8jaJika6Viw/fSON1u13t7e3pxYsXgY5zZH0ODg4CymRnZ8eUbbvdNjxFkq1bOEPCQfKcoagImx3PBYCXMNB32vOKkjAQXA7FDDfIg7n+AMDiJQQgHAVrIcu2SZzlJvFgszes4Jz5fN6MCj/nPjkPhNu+Xmk0Gtk6oVj4GzgG/kyRyfQA+H3lzsolnBLkIrEy0vXhCvMy0JpoVbwZXw3LQHEOhr9Z77lEUal44bpxOf2B4SDhhUCoQ+n6+/Nu7mq1MjBckilgDpYHbzlMmw6FbhLuy2MD0uvwerlcmkfD82YfeMXg/09BLYaO0If19lk4vOvbpilEQbhejyXikUgK7Inw+zzm5mn6rLOPPHz0QMaIPevB5YfsoXcqlzDPhQdCShHiEnUKw+FQrVbLFsSHUWwY/worDlB/LAxkMTyXTZOd3iU8ZD9u0z9E+t/6wrIffvhBf//3f2+/z1fSz37SJKGFd5PJongGa9QUi6Q3rDCALNk2lAWKwntii8XCmkl5nMHfZywWM4+QwwIITp9hMJZNZ4feJZ6Bi8Jgb5Ax9HR+ziCKhRG/7DkSLkQSEFK9EWNfek7aQ4zUnRtiekvDhXochgcOeCYFG/N4vgtak8/CqvCCgOYtTFQyQ3eRMGIvXVsP1gnvDc+NtoYe5/JFnR6v8p/lwwho4VFULIhXCtyvB1S9t4LLzryccDmEt/IcED4XBcJe4qvfT5+C+D2BkvFJDjyaML8pXGDo+WA3tZpgXdlLPwZed+ewCIDJP0Q2PpscjQcegAUiHe0/zysScArSrnhInoH4qSgWxIeQ/qt03QNGUqDXKW67z4j5NCQKCc/Ee0BRyArdVfzBXq/XgYJPvpINQ6H6e/fV0p405wmYnk3O+kYh5XxfCR9+Qm3pukuBdB0uUczJfXpF4ktzpGvj7/dXmAH9ELmX5+L/jbLxLF6vXSUFeCrhz+Ag+WbCfoPchLF8SpvibeJZs2EPh80Sxl48huK9xZuo3Z+a3BTeYI0pG/DsVe+53aRc8Fh8aOQTAp/6HvJ7wXu5UnCOGGfIt02hmt7DEjcxnn+sfRT7FDfkVraylejLpxF4bmUrW/nkZKtctrKVrXwQ2SqXrWxlKx9EtsplK1vZygeRrXLZyla28kFkq1y2spWtfBDZKpetbGUrH0S2ymUrW9nKB5FbGbr/8A//8E6GHX0ffH+RXq8XKNeGqXtT6z0ERmUmk7Eh9hSd+Z4T0JffVffw61//+qNQMX/zm9/caY18MSGlEr51IO+hmtePhEA8izdcGuFbGvh1vo2R+sd//McfZY3+7M/+7E5MzXAdlmcp+30Eg/emewv31Q1/Pt+7K6P5b/7mbz7KGv3n//yf33kxns3sWdo0DWPmk+/JclOb03CrWRpnMSwun8+/0VL2tn30H/7Df7jxh3em/98k/lBQOMch8gfH04xZpLDQkZ3qX9oMeAUTpjt/CuJp+/7f4ToOrzzf1pzHF+ZJwaI2Ptf/zU9JfPW9Nz5Q+GkL4H8evkdfKuAbmvEzv6bhsoJPRbgPKuBpMMaM8XCF/NuUi3S91ihrGqRT0f/QlhTvrVw4/DQO9l3RfEUmPTkpYESr3vR5vlKWUa/7+/s2Atb32P0UBGVBZzqviP3cbG+dvUVCwhXk9IJhRg0eop/ESI1SVLqs3SZsct8ljmunKZQU7OHMPgpvfg4ORbO0yKQI8qZ+MlHfT759Cfd0cXGh0WikRqOhwWCgZrNp72fP0Wzsps+iHaavFu90OkqlUnbWKpWKNf9+n7qs91Yu4SK6m5rLcBM0r36bxeF7/lAxhMm3OpRe95Plc6NehEavWJQJysY35h6NRoFitLe59NK1cikUCjZyws/Z9h6M92iivE7eCwtXMPuRKl7YR77HyU3iuydyMOl4h5GikVnUhec6mUw0Go1sxKrv6Ihh9vvhpn3k3+dDJ4wTXf8ZSve+Ht6DlAsPDXc93IrSl4F7TTqdTt/YFCzIeDy2LvAsIM2by+WyqtWqNauOulVeLpc2TZBGR7ix3W5XvV5PFxcXGo/HGg6HkmSDum4S2lJUq1Xl83n98pe/1OHhoY0nxUph5YiZo6xcJAWUCS1A6dZHgyfpem3CnQq9hI0OnhDtRiVZwyoUTNS9Yc7acDjUq1evNBgMdH5+HuiVlMlkLPSmfYcfPI9wLlGsfmon7Rza7XZg3DKTUu+7lx6EuUjBh+m7YtGPg3GdbAqaA4cFTRv+OZp1PB5bf160adSVy2q1sgfsO6rTsY85Twy78q0qbrI4HISrqyv1+30Vi0Wt12tVKhVls1mz5PT4CDc6j6r4VpR+rni1Wn0DwCas9MCuVwxhzIZWrKlUyhpQg8e8zbJHTTxORziNIadFKh7Yen3d0J3QMCwoZR9ZeKcAb282m5lhfx8P772VCzdGdoK5xtPpVM1mU6enp/rtb3+rs7Mzfffdd6Yh35XpAbk+ODhQoVCwEKDT6VhT59VqZfhLlA/NYrFQv983S+NxJ5SLnzqJB/Mu7w7r8+rVKx0cHOgXv/iF9vf3baog65ROp9+anYuK0DUNpZjNZrW/v69qtaovvvjCGo9zCDzW8q4+P+v1WtVq1VqLJhIJnZ6e2pozbM83XIqi0MqUcCgej2t/f1+pVErFYtEMuc8m3qYMwv2Awut3cnJi88qvrq5UrVatidl9DPp7KxePJmNRhsOhOp2OLi8vbfIgYzDC3cfeJngqHDB6fbKp8GxYoCi7/R5H8h3kPFYQxlj8BgmLt7bL5dJSj4PBwNZJkikm30gqqmvkcTp63pbLZRtudnV1ZWuys7Nj+Fu4Q2FY8Nji8bjS6bTK5bLG47EajUYAj/D7KMoSborFxMVKpWLTKX3q/m1ho3Sdkg83ZGMNSEP7yRLL5dJmkt9VHqRcvCs2GAz0hz/8Qefn5/rmm280GAzUarUkSY8ePbLpceGpgoj3ZgivBoOBdSNDYzOkHGtG2iyqEu4Y52fI+HQhzaoLhcJbx7CihGgnymTHy8tL+3wG1C0WCxWLRfMYoyrr9drc70wmo3q9rl//+tcql8t6/PixhsOhGo2G9cP13ftu8lw4hPTbZZ0mk4my2azOz8+teTVrGYvFlM1mP/7N31FQhjTmzufzevz4sXK5nPb399Vut/XixYvAvKqbcDvWCSPHviQCAKNj3/R6PYM2/Ayku8qDMBcANt8gOh6PK5/Pmzfj04zx+Osh477RMuIth8908H/flBoPxg81j6IAmnlLgDWV9EYrRt/uMdw8WQoOpvNtDn2P2Z2dHfv6Nr5M1MR7bbj6ELl8G9RwC9DbjArZEDzsQqFgJLHd3d3AHvQjSKIoHh+hcz9TMlKplGFUeHZS8Dz5cSt+L/kWqjwDjN9q9XpImv/9+67ReysXvIbZbGYDw5fLpQqFgkqlkrnsPsQZjUaSZDEiE92k64MmBQlP/oDu7e1ZShdPJpVKRRbYZVPHYjEbJOc5KSDx4ebIeBooGBQKuIF0DX6C8DNBgPQ0adgfsyfqhxD/rHd3d5XL5fTkyRM7ROv160FgnnDojU74/nxPYp9qPjg4kCRVKhV1Oh2buTydTrVcLo1LE0WBRUu2sFgsGg+Mudvlctne7zNskjQajSxpAKjtlRBsXmgexWJR2WzWMFQ8v/uC3w/yXHy+PJlMqlgsvv5QlwqTXms+tCGxM2npt7EI/chN7wb7WDnqVhkS2HK5VD6fN9wIq8HmkPTGULObqOvEyjcpmJt4R58KUxcPxK+BBx09P8iHCH5scBh7C/OuCMuz2awymYxlMD+FdfLNxeHoECaWy2UjwvkpqPB5MEhkGjFm/gzhwfhJlIvFQplMxv7++8iPolw4KJ999pndxOXlpcW20+n0jYFUeDOQgOC3IHgy3i2WgtMesfZRlWQyqVKppFQqpVgspul0qm63a2EMTNPBYGAWhEFgKFyvVDg8bB74IHg3fgyq7/Ae5TXy7GSfmpeurSqs1H6/b4ogl8upUCjYEDS4Gtw7EyoxQHt7e8rn86pWq+p2u7q4uNB0OjVML8qCB8YeALwvlUr6yU9+Yvfe7XYDWUlKAvgMBgxivFDk7DFCR5TPZDK5c33RTfLg2iI/+8RbTg7/eDxWr9ez9BiKgrCK30XD+uFOnj4vXWcTyONHNRxCKCaMxWIqlUpmVTgs8HpQEqSicWF9kaYH6cgGEG8z98ljVf4VdWHjsl/a7XaATcs+8AcFpck6cMBQKj4b55nPGD9fUuFnRUVRwFzIfHmF6nE5KA79fl/tdjuwj/zUUylYcsFZYh/hIYK/hNfsrvJg+j8P3WMoq9XKUqTNZlMnJyfmkhYKBbO8UNclmSeDFWNDUEwFml2pVAIAVpRTiFiZeDyuUqlksetwOFSz2bQRpngyvV7PUvjtdlv9fl+9Xs82B2sLBpXP521+sserfBnAp+C5cO0wmb///nvV63XLLrIPBoOBhdXsHQbV4+kS7oRDRTw66Zr4GYtdjw+OuqHigBNKU1/mM2Ldblenp6d6+fKlvvnmG/tdlBGGiM+Do+aroVOplHnOhJJkie67Rg/yXLxFmM/nAdByvV7r+PhYlUpFT548MSvOQQizU/2sW5QL7i2bAWo4iP9NXJCoCq4t94KFBVik6BNrkc1mLbMEPuXJX36j+apxX3f1roxKVMSzvOfzuVEQGo2GecLwpfwm9yEgB8KHy2GmLiE4RgyswnsAURUMBqUdk8lEl5eXdnYGg4EajYZevHihVqul8/Nzu3+U0U0tPTxLF8xmNBoZkAue+j50hgdVRfvXdDpVq9WyrND+/r5+/etfq1AoqFKpGJ7Q6/XUarX0/PlzXV5eqtvtqtvtvpGa9nVLkowHkcvlrK4o6uxTKVje7qnt0mug++zsTOPx2NYgkUiYJc1kMqa0cXdHo5GFl/l8XqVSKYC7AHZ+KuNLeX5eYTSbTasvY/NjtPDa1uu1vYeaKv89PpMQajQa2V4jTOfQpdPpSKejqS3Cy1uv1+p2uxoMBjo9PVWj0dD3339vRY38DlKpVFSpVJTP55XL5d6g/0PMQzl78qs36h8tFS1dV6WmUikVCgUdHx+b5iuXyyoUCkomk9ZvotPp6OLiQufn5zo/P9fFxUWgaRLkL+/KZzIZlUolFQqFN6jan4LngrUApNzd3TVLMxqNTOFeXl5Kut4UuLuekewZvuGMkO/1Agj+qVhl6doLJj3MAQBfk2QKlD0DJQGPmP2DUfK4y3A4tOLFyWRiniThU5TFr02/3zfvjrKSXq+nbrdrCnhvby/QJoEQij3CPmGN/Fx3CheHw6Htn/elMzzodJLGoqalUCiY+5lOp1UsFg3Qvbi40IsXL/T8+XN98803Bl5CBPJWxlusSqWi/f191Wo1lctlw2R8GjaqgsWZzWaWJfJKAuvTbDb1/PlzO1C0GcCFxRpjXQmfCAvgcpC2x8MDKI66d+dDIt+OYjgcWnU0mN1yuTSXnec/n88NG+BA4bkBhJMhYs0BMaFSRFl45mCZ3W5Xz54903A4VKvVMoOOJ09WjHDGs2v9vWKcMV5kl05OTjQajVStVpXJZMzgf/SqaOk6K0KufW9vT1dXVzo/P9fJyYm+/fZbdTodNRoN9ft9jcdj477gDvt8O1/JOl1dXanVaikWiymXyxnZKuql8pICFse3AMWbAXPZ29szZbFYLKxK2mfeeMgAk6PRKBD+eM5LOGyNOv7CfZIcQJlQF4RrTuqa/eZbJ4TZu2El4z1Bb7Wjvoek6wQK6wPfpVKpvMGLyufzljjh5cmFiPeEwfZYo+l0quFwaD1kyPbeR8H8KMqFh53JZGyTnJ2d6cWLF/r7v/97/a//9b8sG8SmIKXmEegwKY4swWg00tnZmUajkY6Pj5XP523jsFhRtD4elO52u+p0Onr27JmVMEjXoWAmkzEgDd6Pvyc+i6/xeNy8IfoOsx6efMbveFZrFAXLHI/HTWlSClCr1cwCk/mBwYvHElYw0jWHxisX9qFnN/u0bBSF50eqnszs3t6eqtWqeXe+HUW4NWyYLIg3RAqb0hySC2A3nD+UtKQ7K5gHk+iwmLie3W5XL1++1MXFhb777judnZ1Zys8DsVysz9NzIKRgw+50Om3AJrElGptUb1QPDeLXigPge3Tgkk4mkwBt/ya2Lt4IhWWkZWu1moVBfP6nUj0uya4Tb+Wzzz5TPp9XrVaz96IwUTYevAZT4H2eiJjP51WpVJTL5dTv9yVdl6EsFgsjpkVVweB94sFB0T84OFAqlVIulzMIwoffUpDsiqBUPIeKlDbJAhQQpTb3DbEfpFw8HX0+n6vdbuv58+f667/+a7Xbbb169Uqz2cyskO+F6mnq0nWPCQ4TVgm6tndnZ7OZbTi09KfAU4CoREoVF3c6nVq/Gg9we2XrQUpCI7yfTCaj9XqtcrkcsFi+4jrKoKVXpGBG1WpVn3/+ubLZrPUsof2id/H9wQE/8S/pusaLDn6ZTMYwrPF4HCgz4fOjKCgXX0/05MkTpdNpFQoFtdttrddr41LhtXpDTrYJ5cJX32Afb7DX6xkxT5IlaACD3yW3KhcuIpymw4UMWws0If0mHj9+bJkiz871BKdwnxb+FmzM8Xgc+F3p9aFjU9BiYFOWOcyEZWN6tizf9+QvFAz3wr351Luv/wC4JYxar9fWGH0wGBgLmJAJRUXXvk0rl/CzCXtkHiPBqNBhr1AoaDKZ2J6hdML/PnvAe3s+E0ST9/39fU2nUzUaDQPKr66urNhzk1X23ouHq+Pr7nyY50NgT+VHQUiycBmiIGvEe8icUVYhKQBvUJ4C1YS/cdcs7a3v8oAiDxKtxR/wBYVYSTyJQqFgyDw3BgjHAcHl8m6sTzdOJhNLq3rlQj8XDtCmMkeeDRteD34uvalc2Py0R/CpZCyNB7Wl61CRvzeZTGyTLJdLtVotw2DYGNTObFK5eAwE8SlRL9D5US5kHX3DMMJG/1neKkvXSp6/k8vltFqtVK1WrU4JjgyHGS94E+LXiLPkex/5khn2kq+3w/PwbTx3d3dVLpdVLpdVKpXsb4GtgH+iYDH84cZjvjr6piLjt8mtyoUyazbxZDKxi4Z0s7OzYyxRT/H3sS9WJ+wJEf4wkIm/4auHuQZmGHk2oafEb0qm06m5lpJMCUI68mxID6p6Gns+n7eCMQBGTxwL93vhINEjhk0iyZTNarWyA+VLLjahgMN4E2vhQzZf/cz3PU/FF2YS0vjGSChkMkzUGxEacHgODw+1WCx0dnZmjcc2rXyla4iB/Q+26BurgXtIr6+ZBuacO84UxjaXy6lUKplBBwz2GaB8Pq/xeGyGbrVaKZ/PW+cCwPD3OWO3Khco+YCMVDkTkwHU5vN5c2eJ/3xZAAWKXqmwqVAuaGY4Dp1Ox3rM8vdBx9mM4UzIJoQ1gvjFw2UchgdVkXAIkMlkAmuGIsGycsg8QcxXyvrCRajyi8XCWh+ydpsSLCLXiHX24ZsUbGrkM4fsGa9cyPp4V53SCTg+tLtgHyYSCdXrdavS996NtNmMo1cuYHB4I9w/94chJ0rwpDmY3bBxC4VC4Kzg2fiBg/l8XpIse5bJZKyQVpI5F9L9FPGtyoW4P5vNWu6bAqnVaqWTkxNlMhkdHBwYYSeVSuno6MhugCwS2R7cWt7rCT7x+HXzJ5SMbz+AR5DJZN7IhmxKwVBPRdzqe2t4ngFehrcEeB+8x2fKvLsK8csPlaM9Iy4yCh9cRrru2rfpNZKura4vb9jb29NkMtHp6amurq7U6/UUi8V0dnamer2u4XCo9Xpt64mCIT06HA4NqyMBAAmRw4WBYv0pmfCkO7ykTQK5NHG6vLw0gypdj4lBsXa7Xe3s7CiXy9n6lUolPXr0yD6L969WKyt+7ff7Oj091eXlpRkvsmTS6z3nm8KHMcP3kXcqF48285CHw6Gm06na7bYymYwmk4kKhYIODw+1s7OjarVqCzQajexz/EAn3/nLZzh8iXcYi2FDYoXDWYFNCCNDqA1CuXCYaPnp06WSzJW/KZWeTqcDGTKwFngweIA+PPJpWT4L8DPsOX1s8WDh7u6uCoWC9vb2VCgU1O/3dXl5qdXqursgleGA9r65OXgcJK9wR0NJZsnz+bwdSkIkKBEoujC/ZVPpep819DiLr3a/urpSu922hk8w4hOJhPb39y3xAYu33+9bBf7FxYWeP3+u09NTlctlq6bnfn227KY18N/7UUh0FDl5MM0zBeG1XFxcGIaA25vJZFQsFtXtdq0KtVgs6rPPPlO5XFa9XjePJ5lM6uLiQu1228hmTFyUrsFQ7wlIwRTmpgSXGxo2HgkHqFQqmVKkW5+3LF5x4okNBgP1er1Atghrj8WFmEfzZF8JfVOKdpNCxo8Q7tGjRwbY8rzj8bguLi4Ui8U0GAx0eXmp7777TvV63fq0EDbQptJzOmjRKF1nW5iZBQYlBXuj+Pfzs02J59ukUinDPfL5vKbTqc7Pz20NYawnk0k9e/bMfg/l3Ww2dXZ2prOzM52cnNg5xVhNJhPDOFHA4cZRYQ6RdHfyHHKrciE9l0qlNJvNlE6nLf6az+cWumC9p9OpKZlaraanT5+aW0oB1ePHj/WTn/xET5480fHxsWlsevGitLhZ6Xoin6e6S8E08KYUDMqWtN5oNFIikbAs2Wg0MrITShelclM3PfAb4l1J1pEPnCWZTAaATc9ORW7K0GxKfIfBZDJpfJN6vW4ZDXCQWCxmZRLn5+dmlX1fF2b4+ApgMByfbeEgjcfjN/oS+1cUhP1OmAzztlgsajQaBfAzwp3d3V1TyOCeu7u7Oj8/17Nnz/T8+XN9++231juXlDRZNTpD4t164N23YvCZzB9NufgmRoPBQF9//bUVIfoOavl8XpPJRM1m04hJl5eXRvenqJFK6fl8rrOzM3U6HbVaLbXbbX3zzTd6+fKlpbtyuZz14QX3gdOCRME6M6y7Wq0aBoAns1wu9c0331hDHjw3vDoszWAwsELObrdrYKUkwxw80cu3FSBt6F1cFLwHyzeJJ3gQn4wNA+92d3f1q1/9SuVy2YD75fJ1w+zRaGQtOliTVqulXq9nmQ+eP+EPaWYMHXOUUT6eoOjDy017eOVyWev12jxdWmmUSiVNJpPACGC8sE6no//7f/+vfvjhB/3www/mlXU6HQstO52OeSDgTeVyWcViMVD8SpiVTqeNAsAcc4povRdzF3kniW6xWFhJ98XFhZV6r9frQD+IeDxubiox8mw2M5oyF00amboY+lF8++23Ojk5sW5YpLglWTGkJ+FJwZEjmxIUXi6XszVhk4/HYwsZ0+m06vW6IfNsong8rvF4bAzUfr8fCEGla4CX+8e9vQnL4RDxs5u8mo8tKBeuDcVCOcjx8bHi8bjOzs6sHxDFr7BN8ZDxEEkMhENkPGiUr091e+IeIacUnJ+8KSFJIcnIoeBSGCdgAmQymajVaimVSqndbtv3UcrwXvL5vKWs6YdEdsmD/dQoUXIDfojXfF8j9U7lAoMU3CSdTuvx48eazWZmnclcPHnyRNJ1t3/csEKhYN9nvGmv11Ov11Oj0TBcBqQffgYNayRZsSMVn2hjn4bdhJTLZWWzWf3sZz8z13U4HAauZzgc6uXLl3r27Jn+8Ic/qFQq6cmTJ3ZPgJMoJA6BzzwBiHMo8GioNfEPnvRkWLlsao0wFoSD0+nU0qTQ8fP5vGU2zs/PLYsxHo91cnJi+4yKehQ2L4aFgV1xvxwc1o6/wYhXz8zd5PA4Zn2jQHwDd+n1PlutXteP7e7umodMaOiVJ9XSGJhcLqdcLqdyuaxcLqdMJhNggNPo/PDwULlcTvF43LhpcIeWy6XtqbsqmDvxePFEACaz2azG47E6nY7Ftj4zAssPjZnJZMzq9Pt9NRoNtVotNZtNtdttG0eKwqDTmC87IP5DA2PFN11XRC9f5um2223F43G12207IPP5XJ1OR8vlUq9evVKxWFSv17M18qlGnymQZJ7eer02a43765s1+4JI1sWHS5v0XLgOQhHoBb4x9Gq10v7+vpLJZKABlCcFEqZL13T/dDptBwYLC1dICpaw4B2CEfoDw/ptynvxyle6Dtv4fyaT0XQ6tYLCYrFoWCYK238WmUj2GGsU7j0NkZX35PN540kRMnIe71vY+U7PBeJRqVRSpVKxB0Tc2+121Ww2dX5+bpgLqT9fBR2uJYIoBXiVz+eVTqetaxYPHwYs7y2VSqrVagEi2ibdfl/HIV2HO7j2ANSSzEOJxWK6uLgwblAYlEaRjkYj6yfrO/CxQWjQTQ0JB44D4xuZb1IBsyHJ9Hz99dcajUb6/PPP7Zmv12v9/Oc/V6fTMaXM74ZrhqifAiDnPjFQ7DlP2CMD+f333+vs7MxGyBaLxUCfkk0lBgjjpOtWI5S2kAipVCpWUFipVAI8JpSupyj4zI/P+KDEaML12WefqVar2XMiNMW782HRjwbocqE8OKoxAZWYyrZavZ45w0ODWgyp6SZkHpyAGA/AFivnaxjW6+shWDAv/UHcNJ6AVifMmc/ndmCo6QBY8xXNhJOIR+s9HZ7Qz1tYvDivkFkLT9zzHKJNrZMn9YETZLNZ64lLih1X3ltOr3T9wedewQZYo/C9ojBISzNZwZMYASzDf+Njisd/pOCsa0kWBcCsxfsjewQoi9xkcEnbsyZgMMViUaVSyThDKC3+BgZKut/6vDNbhFsEC5U2hMlkUn/yJ3+ifr+varWqdrutw8ND60LuKzQB3vzD5KCguPxi4KUkEgk9fvxY+Xxe/+yf/TMdHR1Z/t93eN+k54KCAOcgExKPx61nK6Dt1dWVqtVqgL3sLbL30HwGA8VKS9BisRjo/A/2wuGCY8M0vqjMeKKm5cWLF1osFvrd736narWqJ0+eKB6Pm6cL+1pSYH0oyPNZH/7PfvGUeZTKbDbTs2fPdHFxoZOTEwNB2Yc3Ka+PLWAsPmmBd0pD91Qqpc8//1yj0Uj5fD7Q/IpCTDAp39UQg81eyOfzymaz2t/fD3hueCzgokQu7+vRvbMq2pONUBiEPRRFAYxxk4lEwrgasClRJjzUcFxImOQ5C4nE6x669Xpdx8fHOjg4MIvuLdMmPRfWCI8BFJ5aITAVNgOWAY/P131wb+G4FkwFr82zTMOcBK+wPcV90xkjrzxJubdaLcXjccOruE+yhNJ1psm3WsC15zNJcXMIfK8gMBb+HtwsOB4+5No0GVO63s/hZlgcdK7b11nREpVSAR8i+Zok9g+p6Hq9rkKhYFier9viMx6yZ94J6PoUIpW+vntaIpEwOjEjRKBlM5Oo3+8HAEsPPhEfs4hsCtLXX331lZ48eaJyuWxKy/c2iUJY5DkchEHValWj0Uj1el2NRsNo2c1mM0BH914dn8OmQiHwtVQqmRvr5xVxKHx9lwd7N61YpOtYX5LtkR9++MHqifD86CfijcdNaXY/p8hXDvN91vLk5ESdTkevXr1Ss9k0ANODzJtWLFIQePaMbumaBc46JZNJ7e/v23oOBgPD/KRrMJizCybF3yiVSkahiMVi6vf7gSwaZ93jNO8j9+pE57MPgE4eDwArodARxiqdvwipPNX9bQg06DUzVwA+feXwphWLF89Jicfjlr0AfDw7OzNSIArFFzf6DmGEOGFyHAAurjI/YyOhXHh5YDNKgoHCOHU6HeNT4NV6nA6GaPgzfPaJtfONzKlXoik8Ri28rptWLIj3oDyQ7UNCWnDQlQBvjfYIZJ18aQzhNGEShpu1wuB7Rc2+fAjIfSflQvyLNvNYCtqWQkbpuqUjB4Fwxjf/9Y2GfQ8TXLUvv/xStVrN2IJUBePlRO3QAAjSoAmvgY58T548Ub/f18nJiRXrDYdDtdtti5vxUjzGwsuHkigONhC8BUIlvJpN9W95m2BEyDheXFxYmtkTuMbjsb0XxcuastG9J8zngWPBzZrNZjo/P7f2BYCSKB68lqgIyhHjiTLwZR6+/xEJBMJkwmLfkA2lFIvFjFw4Ho9t8BxTOigU9V4g1/S+ci/PBQtCLwn6tPgZzx5/8CCi14RkOwBmseTxeNwUytHRkWq1WuCAgLVE6cBIwdAIQNZnkMiIUAiKxYbPQfjo1yaZDM4Fhuzl8RMOCWg+Y26jqFi8eHKmJHW7XdtLZJC4T5QLhs1XwnN4oDhAlCN7yQt8y++dKBS93iR+L/F/srZ4u8AChOKeNIkHiGeCsvKfRwRBYyoUcbgc4qFyJ+XChfHAC4WCHQJ/wYeHhwEmZa/XsymLsCCxLHR1pwUf3sj+/r59viSr10FxebJY1AQL6puJcygAwGneg9WFS4D3ggX2vAfuNVz9jHgymk9XR3GNPN+JbBptOfBc2u12IKuIV+NxFjY/a017AaTZbGo4HKrX61ltjsdioqhYpGul50fPxGIxmxKJkW40GuYZ4+lCKOT/rDNrDrbCzGz2KjgmCvjHigru5bmgZNi8PjbGWkN8ww3zrjxMTLInEOLgN8Tjr9sy5HK5QM9OXwoe5UPj6ebSNcvWN8Pyo0kp0PRxr7e4vhkWmEq4tkq6ZmRGhZF7FwGkZE95hiqGiApe9g2W20/k9FieVy4wwn3PXZ/yj6JiQXwNFNcZPvhkHSUZxuaZ9J5ThUfNrHEPSfjxItL92yrcJu81WsTfuAfFcM1pRfn5559Len0IcNWwyoQL4Dd+vAH4CpjOuxrZREVQrp6nMZvNAjgRCoD/82+yJOVyOWBNPF/IWxeANlKUvvAuymskBYFL74kMBgMlk0nrVeJdfT+aRrrmt7BnaA9JmAR+4C131JWKF0+I86Cq99ggrPrQye+LcHEmxorPCZPueN+PJQ8eRC9dX5g/LFy8r+hFuVAOn0wmA02DPd/D81k+BUvsxW9gDxp69J+w0nsa/h7DSgNgDsvOe8gshL2ZT0E8ec17FZIsNe+Ba0lveLCESoCV3lh5q/wpKZaw+Ov2noxX0r5wMcz1Ce+tMO/pQ+2d2Ke64FvZylaiLdEcLbeVrWzlk5etctnKVrbyQWSrXLayla18ENkql61sZSsfRLbKZStb2coHka1y2cpWtvJBZKtctrKVrXwQ2SqXrWxlKx9EbmXo/uIXv7iVYQcFG3o+hYz0JoGyDi07XMeAeKpyuDcoLFbaCvjxBrcxCn//+99/FKrqv/k3/+adLMQwE9XTsD0FHvGM1LD4e/br7ksL7spq/u///b9/lDX6y7/8yzszNdkHvs8wxasUdXrK+02/L1332KVKnHlYDGKjoPRd6/QXf/EXH2WN/ut//a93XiP/rH1DKen1fhqPxxoMBoGB9jfdo+/W52uVaN1RKpUCkzbeJv/u3/27G9foven/4dYAlMX7Phm+ujPcPQ7xdGaa+0yn00B9Eb0sqDOSdCcFEwW5qdTfl0tI1+0owi0dw+L7kIQLJH0PWsaibnKe0/sIipaSEAbwNRoNK0Tk4LxNAXulQ03bzs6ODg4OlMvlrCdzrVYLdHv7lIT2HVRCo4yZ+UU1OAWwN/0+5TkMJqTHUK/XM4X8xRdfqFgsqlKpvFcbhgetLJqTxr/cqD8YfiwGyiH8QH0vlN3dXbNM1NL4oqwfq9fExxLfo8X3SfWFhmGF7CcJekGp+Gpx33f36urKmgShrD4F5XJTsR2Vzr6Kl7oh6e2tF329Df/3M4B43VR8G2XxdXy08sADo5qeTgTsj7CiDQtnkVlIVO8zaG08Hls3xfcZGPfeyoUbzGQyOj4+1u7urvL5fKDlQrg5lC9i9EI/GMa8MqGQMRCTycRcs/sOZtq0cM2ZTMY2+u7urjWOok0Dm+c2z4WDwWHr9/uBamD67/reuVH3XrxX51t2DAYDG+/a7/dtffL5vDX09p3pEJSPrySnWx8jW3wLBv5ulNfIt7mkHQUD5Jhv9eLFC0nXxbGcqZvG1xBSlUolZTIZPX36VNL1XPJ/+Id/UKfTUT6f19XVlXW94/PvKg8Ki2gTwBQA+nT6bnTEhygX2guEhZiP36dbPrEjGjvc/T3KoZEPHf2gL2YAc93+ELCRbvJcwt6KdL3x6IxPJbEka8oc1fVB/HX7Z8zeYcQoB8tPN0DC4TWd1fga7kv8KQnerN/rvvMABsfPJZKu9xXC98N9m8Gh6FmNEqZ5GYr6o4VFuFP1el1fffWVWWDc2XA/WA8+eUuF7OzsmGJJpVLmmnW7XRuyxuagBUEYGI6aJBIJm2rHtDy8vXK5HABzeYh+vENYUCzS9Rr6aQusDbN/M5lM5JtHYaT4N+HdfD63Tc8aMpIUvAFrKl3vJdaIESLsH+/B+D3pfzeqa8Q4FknWsS+VSpnx7fV6arVahkf55x1WCP4eaZ/6s5/9TPV6XU+fPtXBwYF++9vfqtlsqtfrablc6smTJxY5fBTPha5qTGJcr9emBHhwbAA61fv2i2F3lk2RyWSsTeZ6vVaj0dBsNguMp/xUwiKUK2vEQKp0Oq1isaj5fG6Amx+eLt280bFMXglzMFHssdh1h3g8nKhbaiwpm5csEYeIIW9MV2S+UVg5+H9z/34CAj1yojQ14i6C5yLJ2qN6DNI3HPOdDm8zKh7fo8cSM69oN4r3grK/b3uWBymXQqFgM6TpjIUbm06nVSgUlMlkVCwWA0rHT7kL3yzYS6FQ0Pn5uebzuVKplHUW85mWqG8QJiAUCgXLVhwcHJjCJRPivTEPxoUFnCCfz5vHslqtrH/sZDKxA8XG8Ac3isKgL0JFrpn5xChib5RQsjd1qg9jLn4QHdQGPDok6vsIw4GHWygUJF3jnrlcTvV63c6gbxKF+H/zc49zLhYLy6QdHx9rOp3q5OREvV5P/X7fhtXfB9h9kHK5aW4OP2MwFxvHD73yc2d95yysNxsOLQr+4uNJP0Qsqg2vvFXhXgqFgilasCTf6Z7Ocr7TmOdu0E5UUiBE8HwHr6SiNj4jLOAqhIySzNtj9hOK5iaP1WcaWScUrMfkPGhO+BXF2UVvE+6d0JmMoj9jGGciBLw2n3r2QK/vK8x+Wa1WprASiYQpoLcZvNvkQYAum30wGJgWRYmk02kbng34RmiQTqftQeMC06KQB5/JZIyPsF6vrRUmh4iFuCmPHyXBSymXyyqXyzo6OjILDEiHNxf26jyACzDnuRv1el3JZFKNRiMwPhe32bu6URW8k2w2q2KxqJ2dHZuvLckUs3f7OUgcFMhiuPDeAO3t7ZnF9wfEDxaTFGgWHjUByPZN3IfDoY1knc1mymazAb6Ln2fkG3gzjI7Jit7LBdYoFAra39/Xq1evAgTYj6ZcACNxzzlE0jVvAc3otSiWxGd8fAbENxv2oxT8wPVYLGabJR6PmyscReGe/L1ICigL7877e/dAo7c8eHAcNj8XmtjYr+unIuwTn13b29tTJpMxz8bzqWCXgs/h/ZLZAG8iGcBzQOH6aRQ3pf6jIjxrMkSLxUKtVkuSVCwWA6HwaDQybxWlBP2DczSZTN5o6i7Jpir4IYg+8rhv+PjeymW1WgX4CMRqfmOjCf2wdJQLm4AJcLcpFzbDer02Fmq329V0OjWPJqrCuAffvNwTuTgEPkTyfJ4wjuBTizCiS6WSptOpYTBgDbyi7vJLwVCFucelUskwF7wbXHaASGYcEwaF5xpBAoPlzfTKarWqTCYTmBsVVc8FHHO1ej2IcDKZ6Ouvv9bx8bENksczYS55Op02Ly6bzdrvo5A5mxjo9Xpts6IhsbL++XxemUzm46WiUS6xWMysTL/fv3GgODfOHCM/HM0DtZ4A5Md1MiIVC4ZEvaM7FnE2m2k0GimVSmk4HNp98yBHo5FNYcSVl65B65vYth6PISuF8sZyRR2olIIMWj/KVApmkfDw6PLPAUG5dLtd9ft9c/098I/nQq2RDxvgwURdwKFyuZwR57rdriU9AMQPDw8DYSFGfjAYSLqel+VLRwDL8QAZJOdH3qRSqY+nXBaLhXq9nmn80WhkYCMhUqFQMDapPwB+LCcCMCXJapImk4mGw6EGg4FNk2Mz+pksURWUSL/fV6vV0mq1UjabNdITEyl7vZ4ajYatXS6XM7zKDxzn5e8fiwyom0wmAy5+1BUMXlsikdB0OrV0sRQskwCj6vf7Nk2RCYIQLa+ursyw4flBiUin01osFpa9K5VKymaz5rlEeZ24h3g8roODA02nU3W7XfNkS6WSDg4O9OjRIxUKBTszRAbdbte8G8b5wPj1oDbseOZIP3r0SLlczpi89804PshzIRUImj8YDAKoP5yV0WhkvAMOEJYXa+vjSQ4fxYuEFTs7O4Eak6hnQgAcJ5OJOp2OkbjAqvr9vjqdjhG+AGs9xgTFHW8OK4v775nNPuMGOzjq9H+f4cAd9wPy4vG4zs/P7Z5QxB7AvimkZM0IEcNEOcbt+omMURYigEKhEOD7eKYuSZDBYGBEVJQMlc/xeNxA2vAeIVnCnsX7vake8C7yIM9lNBpZSowHiJVYrVYGNk2nU0P5id3CiD4LBTu12+2q2WwacxW3EDaq5y5EVQgdJVnlKf+HXdlsNi0k8OUSeHKAlx5H4d94dr6wz1skwLwokw6xrtI1vYA2C16JcO+dTkeNRsM8Xzy2crkcaKfAvvDFitJ1yhnFxMjXKO8jz3bP5/OmaDEwgLCVSkVPnjyxNbq4uDDlOxwO30i7e4B8Z2dHl5eXisViNhqX7CyG7r5r9KCqaGpgCE94sJDnCH9w0339ByGS92J8VSwbAs2JtQfE/RQyIZ5DAD7V7/cDBYueWEjNjE+R+iyI53TwmT50nM1mWi6XpqzfF+X/2MJzxksFI8BNb7VaAe4OVAXWihIBeDGEUfQSglzI39jb29NwODQeR9RBb891ovIdWkc2m9XV1ZWtF8rY/98nDxDP+fE4FNgK5w3j7id93lUenIrG2gDs+l4ZeCIQcQB1WRQoxmHl4q0NGpsbpAQ/XNkaRWGNJGk0Gr1RuSxdFxfCvM3lctaPRQpWSaNQAbpHo5HhNtTQzOdzY0iToo6y54LyZbPP53O7r0ajoVarpRcvXpjBKZVKqtfrqlarqtfr5hH7YjzCHfYL9TbxeNz4HXiK0+k00l4L4lnJ4bq+k5MT/f3f/70uLy91fn4eCAcxahgzL4SScNByuZw1lwKa2Nvb02w2UyqVundHggd3yiF1yM2SG5deu55oTNwrNoPnZvgOaggKiFhxOBxqsVhYaOFT0FEvBfD8DQ94o2RQqlgQj+j73i3+EPIeX/hIaBTeRFHPHHnjgqdKHZbH9qTXvI5arWagbJhpG7bOYQIiBbE+oyQpACJHTTypkvOAJ/v48WPN53OLADzPhfUMFzJ6JrPPHLHeuVzO0tLr9VrdblexWCxA/7+LkvlRlAs9No6Pj+0AYTE4TDAtoXV7gg6hgS/ay2QyKhQK6vf7lnEhw0L8Td4/6q0XIDN5kBZLUavVAvR1HjiMSJ8d4+dsBiyZT2tzkHj4UfbskDDhcnd31yrH2TOAioVCQZVKxfaPJ86Fe7R4KgAyGAzsMKLEfC1TVIVn3e12JUkXFxdaLpf6yU9+otlsZh5wv9+3tcCQ47UhKBg+k8jD84ra7bbVrO3v72u9Xtsa3TVr9CD6Px4JIU4qlbJUqXRdWEUlcDabtcPl+7v4imeUBUQplEmj0QhgLJ7tG2VhnVAo9Xrd0vSk1/E8wh3TvEX34qutAeU4LD689HUmviI9asL10deWcIXMBp4t+Ar3iUfjSZeID5MIq1E2nr3rlXA4oxQVwdPwjPb5fG7peOCJdDqtUqkUKCLm/rw36zsU4O1IUi6XUyqV0ueff65kMqlvvvlGw+HQPJdyuXyvfjgP9lxQLvSGQBuSZk6lUiqXy1aAyMP2BXv+Rd1IsViUJLXbbUuhgVnw2d7Vi9qGkK43KQeHmg0abGExx+OxZrPZG2EQSiIcMnqKvAc2wb98aQZeU9RbL9D7BtIWNWNXV1dKp9NmjHyxKvsCpen/Dd+H3/PKJRaLGebiU7lR9ly8FwtM0O/31e12zavLZrOqVCoG6IZ/F+H97C+yvqVSScViUT/5yU9UKBT08uVLtdttdTodLZdLPXr0yPbaXeRW5RKOY316DwGsxV1nk4DD4P5T2+G9GrQtLQg5bLAwifOwXlyLz82HU4ybEu+OS9eNuDnQXDNKlqbHkgwr4QDcVIvFZ3rMhbaixWLRGKtcA5kSQsz7uLMfSvzacI3cH4ec0JEMI9kjH+v7Oi3vtfk+xd1u1xQ3IYBvqEWRLVXlUcmqhc8cCgCKBkA1IR1d+qTg1A1fq+Y5ZYlEQoVCwVLakiwdDzFvZ2dHpVLJmnOxdvfN0L7Tc+EwS9fl/F5JePIWoKTnsqA8WCQ+A/7F3t6e9ejEygBOcoAg4bGAxJO4zVzPJhRMWKmEcRJPZSeFmM/nVS6XDdGHJ8TveOISStmXx/O3CoWC4vG4isWilRCwDigXSbYZowDs4p1Jsq/sA/ZaKpWykIeMB+vi9wmeG2lZ6ojw0nZ2dqwkAM+QNaWUBG/Jhx6bFm8IfAradyhkP+VyOXtfmDzoa/X43Hg8bh40Z/TZs2cW/kwmE+XzeRWLRaspwuO7bwuPW5ULYBDpTOL4RqNhJK5EIqF2u20FVfwewgEK36yvG/FuP/iDr5nBOtHaD6YvFj4KfBfWB++Dw7BcLq1cHsWMpZZk7GPEWxxia59N882VANlqtZrhU17ZQ/Mul8vv1b39xxJie9YHAptvvF0qlZTP542iLylQ9Y3R8goll8upUCgYfgXJEvrDfD43ljiYFgcPxYJXTfbxJozrY4j3dPEc2D+DwUCnp6fqdrs6OTkxZQy7G9yNtYGSQFaVtc9kMoGCRmY50QTf92W6Kfsm3S9CuFW5QHTLZrNGTBoOh3bhFCr2ej3lcrlA+MIhYkMRK/KzdDpth8UXpnlL5UMgP6KE65KiA7554la4bgqKOT/zVGtP1/feHUoH789XlHtlvLe3p2q1qul0algF6UiU9KabRvl7Q+lNJhNdXl6awr26utL+/n7AWPgyCH94crmcisWiSqWSyuWy1Qml02nLcjC3J51OazabWXMursMXL0LS3HRo7Q0qmByGgiLNi4sLJRKv5xYx/QHeC2uEJ8zvhLljQAlgf1SbM3QOEidOAF/vK+/0XHZ3d3V0dKR4/HXjZDRdPB43SjH591arZSAbYK4k08Dew+BiWUAwh263a0QnhAPmO9LhKRALbkrw7shycT/Exhz2y8tLJZNJ9Xo9K6LzlnK1el0KD4AN2k+q3xPjULQoK2LvbDZrg7BYUxT8Jt19nqWnrU8mE6Onr9evq50rlYqWy6Xq9bpRz32GEYIhFpj0KwDmYrHQxcWF2u22Tk9P1el0AlXPyWTSDBf7iOvzfJBNKBmiAJ7to0ePLE1eKBT06tUrwz6m06lOT08Vj8f19OlTPX78WH/0R39knprfJ4RNeLmElVdXV+p0OlaVv1wuAxk2xK/FfbONtyoXHmq1WlU8HtdkMjGAkDiezXt1daVer2c3gWIJx3/2h///g4O7PhqNrCfJbDYzoI3f8SM4OZR4AZtE+cGO2OiwP1EahAGdTkeZTCbQ69ZnPljDwWBgDZA8qk8ohdcDCLlYLCwlzYtn4ptpbcrdl64Pr08MzGYzdTodI4St12s1m03t7u4aNd9THUjbsw9IS3vlMhwOdXJyYnU1KHIOi+/54o1dmGi2CcGYAEFUq1XzKJLJpPb399Vut60m7fLyUnt7e/rd736n1Wqlp0+fBrgtfI73cvk3mbTBYGB1aT41j7ciXXtT77M2tyoXrMX+/r61H0yn02aN6cqezWatlDsMGvIwPTs13BOVA+iHqXm2KlaXzeZDEM9V2ISA1sMBgFPRarWsufFyuVSz2VQsFtN3332n0WgU6CN8eXmply9fqtls6vT01KxTLpczXAHXt1KpWGaFcMuHSyjtKJHoUIrUwaD0jo+P7R7APubzuU5PTwN7COyFsSC+4RZFrJAICYvwTPb3980AoOD8evlwc5NC6r3b7Wo8Huvs7EyVSkUHBwfKZDL61a9+pUwmo8vLS7169Up/8zd/I0n6wx/+YP2OULy0nkBR4Mn40Bg8Du8aIh5ni3PMGgFF/GieCxqQalMutl6vG+EGTchm56I8+89bVzJHPv1HTMkN+umK0pskKe+6bnpTYFlQtIVCIdAfhPh3OBwqmUzq/PxckiymjsViury8VLPZVLPZVKPRCGQ3yGSQPgXslK7nZd90cDa9Ll64Tt/LZrVaqVKpmHIAV6LyWQrO2YaRy76gjywVvGRSKODE/Sfl73kw3qJHZa3A5AaDgVHud3Z2VKvVtLu7q0ePHkmSfvWrX2l3d1ffffedJFm4TTnEwcGBJTy8csnlcgHMhYJOlDsgN9lJXzrgjdZ9FMw7AV3CofV6bek7FAnZCV/m75FmSYbKo1WxPAB71C+gLCgTIFb2/A3CgVgsZhmBTQsktUqlomw2q4ODA83nc8MEXrx4YQ2Rp9OpGo2GpNc1MiiDVquldrutXq8XcFF9upX0vuf1sAY+TYtijtIh4rnX63VTlIybodeNz0riuYG/tdvtNwoTfe8aOB8od0mWWSqVSma1pev6mUqlYmDmfQ/Nh1qj6XSqdrut8Xisv/mbv9HR0ZGFdbCtf/7zn1t7CUoboOxzz2EFPB6P1Ww23/ibhOW82Eu0oADvI6tEPdyPolxYcDqFwSegbSUpVujDXrASEOU8ZkJM57v++00D6AfL1LdewN1jQTbNquQQAzgDSlIfRCiAYqCRT6/Xs8NP7Mu98n3f68aHiJ6EBr7jlXvYk9l06Og77IFDAYAzndPfF8Blp9NRp9PRq1evzAB5AqUkI2yWy+VAMSwEMA4EhkmSeVHs5yh4LpwpChOfP3+u5XKpfr9vtINYLKZarWZhDhlb6XrOs8+iAjWwH33I7D05DDsJAumaN0PZCiHpfbJGtyoXXMyLiwuVSiUb60jMS6Wyn8tLlsKzLQmfuBGflkShkA0BvyFX3+121Wq1bEOwGbDyfkE2Ib6TOi4oD25nZ0e//OUvdXl5aV4WbudwODS8ihIJFBRrVqlUbCQJfYfpWkf4wP990SOeDPyH+5bKfyjhujxHB9yMn/vykMViEagS5/8oEbxDPDsfNnkSIn+T8CGZTOrw8NCmXkLm2+Q+4vk/efLEQkeKdVEsg8FAL168sJ7MnrjJ3qDWCK/fd21EiYY9WQw+nR/BYwgrUWgeHL+L3PpONi0HASITM1JwN+kvQp+V8Xhs78X6olSwttwsuAKEJsIqrBehExYJIBQsZ9NpVh4kB8f3CV4sFjo4ONB6/bqTuleEdBCTZIxR346BSZUQxMh0YN2YmuAVLBkohMO1adDSU9C9h+XjeLgUWEvpNVge9mJRKoDdPkvmQUdPAuO54GH61D1V0v4aNyEYpXK5bFM1wJFYF6qiaW9KgoNn7+cz8fJZyZvqyzyWOZ1OrZfO1dWVDg8PjVcEtnif7OytyqXb7SqVSqlYLFoMDNMPBJ8bBLTEVUNbetIO4YykAAaDAgKLubq6UrPZNDoyMXo6nbZ+LnRf23SrS1D+ZrOp9Xqtx48fBzCSX/3qVzo8PLTuZ41GwzwcPEDvZYBrecAbQhRC7RVkNNpb0Kxausa6CEnDWNjHFJpg04KDF+HL48ePbb+wn8j0gCv4MNiXAKA8fEpfurbOKFbS1oVCwdpgjkYjtVqtSIyogXV7cHBgz3M0Gunbb79VrVbTL3/5S61WKxUKBesyh6H2UYPPCIXDaxR5WFgjnAWyT9QGeqz1PnKrciG84SYmk4m5557wk81m7b1oUt/k1x9+X2Tlqf3coHfp6OPJwqdSKSuuAsz0dPlNCF4baXrSgGBNtVpNiURCvV7P2kdICngT4Zf3OKRgTRdALu68rzkJ156wvpsuWOSeuTafefTV3J635OvFwiRJX9Tpywq8J+sxP94nyVphAlySNfEV95uSeDxutTychXa7HeA3UWbjsSnOZvic+bIR1uFt5TK83ycF+LfnW0l3P2vvVC7S66pJSfruu+9UKBS0XC4DCLV3Nb17zgPDu0BJ+SwP1pSDQ/9UAM5isWgcEtxCvBfc2U16LrBgm82mlsulTk9PDVykSIwQaTgcqlQqWSjJBuH/iE8pe/HvZXoAhCr6pa7Xa5vzW6lUAoDmpgTPg0MBrlYsFgMEQeZckUQYDAYWbmOdUQqIB659mUhYqWDAUGxgdoQA4ZDyYwv7CC+rUChosVjou+++U6vVChji/f191et1gyBowO2zPuCgN+0z33AbY0XWCXCc7LBfx/sq3lt3HIoC9mC329VyuVS5XLbUGAxQtKqPd8FOvFLhRrwL52tyeB+W2jNRqTtis22yXgbhwU0mEwt9fDaNTcGB575ZA+7LHxjEr6N07cF4DAI2L2siyf4uoVYYh/jYAr5GVhAmM/hZIvG6dQJtKyWZovR0fena+8Go4JXxeYSBhAAoDQ9so6AooPSKfVOC14ZXRp1Yv99XPB7XxcWFcrmc6vW64XLL5dIKFT0/BewEz8zvGTDOm7xi4A5PMnxIIuBW5cIDpJlMMvl60hu4Cyxd6fWGzufz5o5D+PLxHw+Xtgr+QIGjdLtddTodrVYrC8EWi4VOT0/NSoM1RMGV9Ui9JL18+dJCJNJ4XnliNbLZbICK7jc4mwLxYB2ZgvF4rFarZc25B4OBudPgCngtm2agegW8Xr/ufMbGJTWN8kNBFAoF1Wq1N7KRPlwO18NAl/DV876v8HK51HA4NG8IBR8F8aTQvb09Yy+Dg7x48UKJRELff/+9MpmM6vW6GS4UaC6Xs2ZRnFnWbDabqdVqqd/vB5rAx+Nx89xQOp6c+RCm9zt9ZdysWCxmmwMwCTSfXDyHh+wQwJmvBcJq8dkIXo0vE/eeD523WKgoKBYE5QH2kkwmjYLOofZuN4fIWyt/+LHUPm0PxsIaMUnBpxuJrcMlFlHgcUgKeLAoA+l61AxrxKZn/1DoSRjgmyJxj3wengtrFM6keW5VFDzfmwTPBUoG4Rusbbxizpw/Wzx/v/f4TAioHujlLKKo/X7k6/tm0e4UiPuMTCKRUL/ft3R0NpvVcDi01DPcDfp5cmHcEHEhzFsskG8ITI2RdF1rAXPRp16jtDHAkS4uLmzKXTqdVr1etweJEgo3gQqHPyhUvuKpgUXBf2BtSLHiqcCV2WSGKCwcitVqpW63GyhaXa1W5kkkk0mzrH7wvGfgHh8f25qGsRIKQ4fDob3AJHwnN6+8oyQoTM7TZ599Zl4fxML1em0kTFLTo9FIuVxOL1++lHRdMoMnRNjJ+hIyE8KHizmRh6zRO5ULFkKS5dz5P2XuLMZ0OlUqlQqkxhC/Gfg5DxomIiQeHyLg4vmxm1HbEN7L8t4d83kJhRCPNflNgCLASyEDgiIOv3z7Cc/mfZ8isw8t7CPP+EYpeCWLR4f3S3js2cYenPVYAmEmOBThI2EQ+EpUPRa/J+D9+P7HyWTSzgFejMcipesxNp6ZzX7w5EUPfLP24WzQQ9foTp6LTwt6NH+5fD3dr9VqGciEx+LJUdJ1zh13ns3C5vA9ZMF56OsJ+OY5M1HbHCg96NtwBVgbCIKUQPgm5WHWMhwe+Cyk3fm/ByHJsiQSiUBdSVQK8rxwGPC2fCqeGjbeB24HYc5zMSh+lWR4AS9wBZSJLyngs6O2d7ywD3xIBxUDXBMMjvdyPnyU4Cn+gNm0r5Su96tvZPZjr82d85M+5YtS8BkKUsW+BQDirQ6HxLcnCLMJ1+u1hUDew/GWLqqCEibmJ+NF1annCOFx8FDDYRD4E6FguBk6a4oVirJika6fmyfM+RoXjx+wL/jKe2i6HX4fewdwm7UKZy+jLn6NUMYeH8Hjw3D7miH2jwdipevGbP59kgKlAR8iDX8v8oO/ceJk4lvcMay0B+WwPL6PK8rCD/MirAiPgPVFbVEW79ISAniaNmCb51v4plk3pe05PCgq7+7CXfHfi/oB8tbRZ0gkBRQt9zwajWy0RRjk9saI+w8TyaIWHt5FuD/uMRaLGQ7pMzmEOvF43Hru3lS/5zkupPb9zz6UPGhWNA8SK4TCCKPNoNqe8OQPoQcwPfPU10REXbF48YfHI/n+IPDVczX8Ongvza+1dF1G4Q9lVL2V28Q/UzwX6c0CQtLVWFqfAeIzCMM9e9ev7acmYQXsWcdSsHWo94R90oRz6WENf6Z4+c/7MSX2KR3arWxlK5+ObL4Ofytb2cr/k7JVLlvZylY+iGyVy1a2spUPIlvlspWtbOWDyFa5bGUrW/kgslUuW9nKVj6IbJXLVraylQ8iW+Wyla1s5YPIrQzd//E//sc7GXa+otV3B/dNgZPJZKCk+209PD11mepoP8vHd3F/F9X9X//rf/1R6Kqff/75O9fIsyp9r1LfkIc1glkZ7hsr6Q2GpR/l4hmcfo1uk2fPnn2UNfqrv/qrO60RLF3fJtVPlYC6zv7xLVK9+OI92lBks1mrTg935ruNSPrnf/7nH2WN/tN/+k93WiNP/fdzvLgnz35/WyEi+wfGc7hmDfEs4NvkP/7H/3jjGx7UWPWmtgH+YsJtLKlgvenghA9DuO2Ab733qdDc/XX6RsnhAV2+S1+42M6LVywUQIbbFvi/xXOJsoSbZNEClLYcvoeu3ze33RfrTA2Xn6YQ7q37qUh4moEfzeKVi1fQ71IuvMJtMFnb+0xXvEkeXFvExWEVvGJhJjBtIOnONpvN3rCq9JzgKzeL1eL7frLep1A3gvKg7J02Ar5RNxbIz4i+qUrVW/TFYmHT9lDavt9N+HlEWeiFQwvPy8tLDYdDG/vBHCzahubz+Tc60vM5HDjmPTGShv3CGvk+zlEXX7CIkqRTHQrAeyx0FHhXTR5dDGjujufMjCLfoPt99tGDlAsXQ1m474mKG4syGY1G1h6Trm3+gv3ITbq0Sdczb1Au9C7xM2uiLGEXlplGDAan+5ofr/K2DeFbNfpKaYo8fRf7T6nYk73EDCa6xkmyYXiZTEb5fN6mcnLgwsqF4kWmc9IYyTfs5r13DY02Lb7nja+Glq6LPH2fYOYevc1I8XkYKdp6+N44GKe7hNdvk/dWLtwEygVXbTKZqN1uazgcqt1uazAYqNPpmAXys3W8YsB1ZVBaoVDQ7u6uDUbzfT/K5XKgq31UxWNPWNzDw0OzqDTd5qDQbuFtXdfZLDSk2t3d1Wg0snX30/ZisZgNAovyGrF/hsOhXr58aY3Hs9msvvzyS9XrdX3xxRcqlUo6ODgItA+4qWs/ShUFjEWmdQHr6quFJQU6sUVRPD5Hu08GFU4mE/X7fdtPKBcPQXgl6vv/xGIxjUYjG2Y4nU6trSgtMVFo95X3Vi64lh5IZPYvEu674RtU3/R5gEt0uKOPBdPwEomENZBaLBaRPzweY8FrqVQqNhmBgeyAsL6j/U1zhvAUcXvxAPFgwj1zPR4WZfHxPyFNqVTS4eGhSqWSisWi9vb2zAv2Q+HCwl5jfVAy2Ww2ECZGvSNdWFCOKEiacKNoacJGjyUaavF7CN/z4Q7rScN3SYHmZO+7j95buVxdXanVatlm2NnZseFlw+HQMj6ESbTpCysZ36+CtnvT6dT6fdLykYNUqVQkvR7cHXVcIRaLWVvGUqmk4+Nj/epXvzIPDWXKxARJ1uflJuXCw+71ehqPx8pkMmq329rd3dVgMFC73bYZ1L4/SpR7veC5rNevp0VUKhUdHR2pWq3qiy++sHCv0Wjob//2b9Xv99XpdCTdHMoQHnjvLpFI6OnTpzbEL7y2Ufda2CcYqJ2dHeVyOVMIGFygB5TE2545DoAPn5fLpbrdrk2v2NvbC4wveR8I4kHZIrCWMI7AyEnp2nr7+M83OkJ8Gsyj4VIQ32Ex0dpRFh8W5fN5FQoFU4o0ogasRrlIeqdywTXu9XoB0BsFjuv/KVjncAfDTCZjExnpSshsZLAYb8XDQrgDxsJ7AMvphhilyQjvEgy07yucTqc1Go0sZGENSbnjESPhLn2SLIRGyfiRLWTqiE7ex3t5EKDLCMhHjx7p4OBAx8fHSqVSGgwGFrpgvWngfdvn4e0sFgvjJ1xeXgZi6Ha7rfV6rUePHhlg9yG6aP0YEo/HlcvlVC6X9eTJEz1+/FhffvmldaLzLqqfKfO2GBfsgBg5FovZcLV4PG5jOBnZ4tPUUT1IzBoCh9rb21OxWNR4PNY//MM/aDAYqNFoBKgIxWLxjbapCMqUtcIQzedzm17IBEwUDMYrqkLT9r29PVWrVVWrVe3v72u5XNpsp6urK2UyGe3v70sKcmL4fzhKYIIEZ67RaFhjeEnq9/va2dmx9fYTLO4i761cwAn29vYs1ecbRqMpfVaDTe61LZ/lUWo+G+/Fa1Tibt+xPKrCWAfCoPBIUd5DCOM3gt8Y/itrIMmsWJjLEbbuUVUs0rVn6zkcflIgNAcUMOlX38rSt3BkP7DODPTz/Wg9RySqY0a84FlACiTpwYtz5KMDT2gNhzS+7So/n8/n9rlgnGBWfnLCR/FcSPcVi0VVq1Xl83n7PheJ5ej1enZxHILwbGkaWbMwPtc+mUx0eXlpg8FisZi5zB+ia/mPJfF43MLFQqGgTCYT+JkkSwlOp1NTuGTCUNRsEhRqLPZ67nKhUNBsNlM2m9VoNFImkzFQT9IboUEUhQODZZzNZjbDqtfraWdnR7VaTZlMRoVC4Y1m5uAL4SmcPruC8VqtVhY6sMak8T8FSSQSqlQqKhaLxvVKp9N2Dnz3f84YHrE35Chb0vT8bqFQUC6X03A41Hw+D8w9x/DfR95buXBwfEoYK+AHgWFNfRqNRWH+DA/XD13z4QHf+5Q2ghfvdY3H4wCbEmLXbDYzvMQDsf69ngHt+TPeC/TWyI9Mjap15h7xSIj9U6mUEeBKpZKF1X4I2HA41Hg8DszsCU9B8KNXfNmFJyRGfU/5Z4kiAHP0Z8+PbEFZA0kw1sZPUQiLH8XsJ36+r7y3ctnZ2VGpVDK2pCQ7/NwUQ+pBuomts9msvUidcUO4XmwSFoYxE+F0dpStshSktA+HQ11eXtpB8nVXk8nECIKEgWFcwYdH/AwL5ZUIGyudTltoEFXwmzCFAfSwsxm2XiqVdHR0ZIqg1Wrp7OxMjUZDL168sJGtjBCGaMf+AeQuFArGN9rZ2QlwglizqO4lzkEikbBwkQwqhttPRiBDmc1mDSRPp9N21sDsEPYooDrZNg89fFSGLiQt3Ha/SQDMKpVKYGKep7+jNMjRY22wuhwaD9byNwF7H1r78KEFhcmG2N3dtVSfL3eQguliD/b6dKEfOxJO5/v6ENbN4w5RFn8vyWRS6XTarOxgMLDD1Gq11Ol0dHl5aWGTV0z5fN7CafYTnko6nbYMFGROvKSoJgQQMEiepR/4ls1mNZvNVKvVbN8QQqJgOKcoH69Ib/JgMO5e4X/UVLT/435T+LkppJwpyKOOxt8orMJwRW86nbZMEK5+MplUsVhUpVKxuDnKyoX6qmQyqW63a/fv3XPqRIrFYgDc9e6/d3nftrbwg2azmYWqoPtRXiOvRH1KFcbo6empvvnmG/3www/6u7/7O43HYw0GA0vtf/HFF/riiy8sixIGaMEn8K5fvXqlfr9v1rharQZoAFGUeDxutVGTycQ8mEQioVqtFqidAsskgUCYA/wAA/y20gfOdqFQsPV7H0P+IJ4LB58N7St2UR7cBLiCby1ACs3PxfUZAVxkFtJjNbwv6geHQz8ejxWLxSy9R3aNeNnXvbCWPgPiOSu+YJR1pVBNusZkooqzeIHDAUgP7aDT6ejFixc6OzvT73//ew0GAyWTSR0dHemrr77S/v6+ke2q1ap5vPAzpGuwHANGSnc6ndrhw6pzLVEUz8xFKcJOBz/K5XIBfguhs8eXPFbD73oOmudd4fXg+UCfuI88KCzCylK1ilvu6cgcCg5AmCA2Go1saDicD5QHGno4HNoQ7mKxaExLMIWoCpsZV5WKZ2J/ScbtWS6XSqfTmk6npkDBqtgsuPDe65tOpxqNRlbgSRlAMpn8JAoYfRmJJOOjPH/+XH/913+tV69e6be//a3q9bqePn2qP/mTP9G//Jf/Uj/5yU/0i1/8wvbcixcvdHp6ahlKjA60hV6vZ97QfD5XsVhULpezEJtnE1XxrGuSKShmjy/l83kbTO9/x2fWPBjszyKGHNgCxY3nd1/qx4NIdACVzWbTvAu8keFwaBRsSDoMqffgI6Qd3y4A6yVdbw4Wz/fkiLLXIsksAQ8b1z/sxuK9sEY+PcgmCH8uh4g19nOUQfjZDFHOhvjnPZ/PNRqNdH5+rlarpVgsZt7JkydP9Ed/9Ed68uSJfvaznymTyVjKGqIdBbKe44M3SLKBiup8Pm8A+qcgKIjRaKS9vT2bzw4ZbjAYGP0fjA9vjn3Q6/VMuaxWKztLnDHqtvCsOW++lOQ+8mDlMp1ONRgMlEql1O/3LaQhVQgQJ90c+3uijn8RKoFw476SWou6YpGurbIkU6JXV1cql8sqlUqB1hF4IuAwuLDh+JgDAw+h3+8HrBGbUNInUSIhye5nNBqp3W7r66+/NiX75Zdf6k//9E/11Vdf6V/8i39hCvnVq1d68eKFzs/PdXp6qslkYp6x57igbDgc9AOqVCrmbUdZ+Xqh90o8Hlen0zEjy7qRlidKkIKETL8OkgJ7DSdgMpmY4s1kMkqlUu991h7cie7q6krdbtfcS/gJWGgaQ5HWCks4A8KN+qpX8BewGw6p7zkRVWXjsRNJBnwDJOJyojzpXXITC9O3YvDrBYgHDhXuLOYJVFET72aDJbA2tKj4+c9/rlwup9PTU+v30m631Ww21e/3NRqN3qga9qEASjvMeObv31RIGzUhBBoMBpKkfD5vlA9gBd82gUgCZezDn7exdT2mx+cRYr5Pqv7Bnehms5larZa5+cVi0UhPyWRS0+lUyWTS2hVy8V4h+AeMe4+7L+kNJi/ejGegRvHgSMF2ApKMS3B0dBRgK3sF4jvt+e/7+/SMTFL8u7u72tvb02g0shBLUgA8jqL4gtVMJqOjoyPVajV99dVXqlQqevz4sZrNpr7//nu9evVK33//faAQEVazJ2v6MGs6nQaAW39QPgVcimubz+d2LuCNFQqFAPZGZfx4PLZ9BPjrjROhTvhFWI3XR7YTLO+j0P/9TSeTSWWzWeu/cXR0pMlkom63K0k3umN8n8/honFTfVMp35luMBhosVioVCoZOMrBierhoVSiUCjo4OBAh4eHOjw8DPAH+DeKBo/M41OeqcuBAr+B1k4oSXiJ4o7q2iBU169WKzsUeCVnZ2fq9/tqtVoaDAbGT/E1Qt67DdfU+L3hyYccOt+CQIpm1sgf/kQioVKpZNweOD+SbN+wjzyD24fWKBefPYJ8x5r0+33DCQmP7uPBPFi5wHHJZrN69OiR9S1pt9vW49UrF27Mg41sFlLPeC64ZDs7O7YB+v2+xuOxqtWqJKlQKAQYqlETHnQulzPFe3x8rEePHklSoOGRL3fwB4VN5RUL3gohFP9eLpfqdDpvKJcoZ9UkmbL03J1ut6ter6fLy0vzVDxR0wP8rI2/3/Ah8ERPFDJ0efZnFDEqf04AYsGOaDrG+fD1RKzl2wyvB/6pNaK2aLVaqdPpaDweWzcDPMMfRbmE42Fvadm8np3LBs/lcppMJoGKTSkY3+G28lCLxaKln7lh0rPxeFy9Xi/QzwTwGM27Kc6L/5s3LTxKwVeQQxCEa+DDQsKdsKcXi8WMZQqBsFwua7lcqtFo6Orqyrw6rsFjDJv0XG5iHnvejscG+LnPMsLY9tXRlJGgWFkzuvSRik+lUvY3vPIGk+EVXvNNrdFNRsCH1SjhUqlk7TzW67UajYZxhXwBp39xHn2o5MMlwmqSDxQrVqtV8775nbvspzt5Lv6AeJo+5CesMyQ3qnT9jBifJiV2xDOJx+MBJiBMxKurK2P9olxI6Q4GA+3s7JjW3jTLkvv0Ma2PX71y8V35sJoeQyGD5j+bTYHljcViKpVKWq1WKhaLms1mxqVBQftriUpYxJpwuL2bnUwmA0QvlAtAb7i8JJ/PK5fLKZvNmrGSZOtIvQx/13t9vpg0TIPYhHivNAy4IihHet4Ui0Xt7+9rPp8rn8/benLGwhlYFDHnLOwR49Gx9uAto9HIsko/Gs+FXhjcVDabtY3uCUnUa1D2Tz8RX5EKw4/PA0eh4Oz4+Fi1Ws0OBBsln89rtVpZJzI0qu9WvslNAQpPrIullF57XzRKZn1QqqQKPduWFCIhAA+dNfLVw4lEwjZUsVjUdDpVNps1q+3dYZ8p2ZSsVisj+kG/56BgfT2w6sMkSeYV080PDxnC5uXlpS4uLtRoNCxdi9HjGcViMUsqgNXhsWxa+eJ5Y5BpAsV+YHaTL/PAk0eRerY2uBsGDJ5YLpdTrVZTsVh8o/K5Wq1qd3dXZ2dnBk1I1xyh+56zdyoXT/Qi3ewBWd/YlxeH3ru7HkxC6cRisUAz5v39fdtkKC4sj++/wctbnE1JuNAyl8vZvU4mEw2HQy2Xy8D6sDk8iM17qHYlhY8iJjz0mZFsNmtWy2cHUC5YwXcVqn0MQblMJhM1m01JsgLFUqkUuDZvgFiDbDarUqmker2u4+Nj85AbjYaazaZ6vZ5evHihi4sLtdttq45mUgSKFk6Qr+3ya7UpJeNDE6q4PdCMR4tSCXslnoTpSZuEQ3wuoVS1WrUzNBwOjX+FVw1z3IO+9w0bb1Uuw+HQNJ53+WkGDImr1+spk8mo2+1qd3fXxhwwxKpQKLzhovP/Wq2mcrmsR48eqVarWUbAF0biEZBK9GDlppF9b2lSqZSOjo4kyTAnXEss7Hg8Vrfb1dnZmeEHeC/D4VC9Xs/eSyYojH35PsLSNYjHtfjXpvEWSWZwWq2WsUF3d3dVqVSsaBOrijIJz72iyflqtdL5+bmazaYajYZevnyply9fqtVqqdvtGqW/Xq+rUqlY9z9CLI9BoHi9YtnUfoJSALES5YKHS3V4v99XJpPR5eWlYZKk5DEqNPBGqQB+Hx0dqV6v6+DgQKVSyXhCvAfPiTYNvsDTZ9/uKrcql/F4bErAN372rhfuPsomk8lY8x5IYgBBxIS+eQ/K5eDgQNVq1ape8Xh8rI17G6WUIfcBnkIWiw71vgGUb3LUarWMMCe9vhfWEAuEl8LPEXAmAHW/pt7ah3GgTQGWnuDluTf5fF7ZbNaajtGZnzEio9HIwm6K6EajkZrNpr7++mv95je/0bfffqtvv/3W7vlnP/uZca0ODg5s3QgfpOBaoly8bGJf+SZP1NdRRS/Jzlmn01GpVDKGbqVSCVTbs2dI7edyOTN8RAf1el3FYtHgjHALEGqLwg7BfT27dyoX318D3gkeS7PZ1HA4NDyFmNpTsCk2XC6XZpWh86dSKQM8fXtDsBVaOEpBMpp3nzcNVpLFKBaLyufzqtVqkq6ZuIQ3uPGAibjnfnoiw6xQHoR/KFl4Bzf1dZHeJISFQ6JNKRcU3ueff654PG70ATzRRqNhXh4eDF3/h8Ohjcug2dbp6am63a46nY5SqZS++uorffnll3ry5Im+/PJLHR8fG07QarUsterxQ+/Z+VT0pqTT6diwM54fWB2UfhIoi8XCogXKJKrVqmVxqS3C4KHAC4WC4aZ4RZw132bWp7D5//ucsXdiLn7RQfnxWkhZwcXwrjpakBv0zW4AijzI65m3HrfxKVp/cMIkqU0JoCETFem+x2YmJMR6eMCalKu/dwBgijX5DM9w9v1P+Fs+zeuvbdMhkXQ9KoXy/f39fQPp6bHsU8exWMyUS6/XU6PRsBDo1atX+u677+wA/PSnP9WjR4/0T/7JP9Gf/umf6unTpzo8PNSzZ890cXGhTqcTqOb1yiUK+wehy2I4heyBW56tT7mzD3K5nCkWIg7CHUbhUowoyd6LYvH4ppebFMyPwnOhuOvw8NCwE1KmeDFURZO1kGS1RbFYTK1WSy9fvrRaEBQULhvcDR40cSCZKpQMRZCxWMxaFhBXbvIQgYscHBxYc514/PVIkel0qkwmY16I76JHmAQTGa/P13KguKjhmkwmNxLEPIhHlsCDzJsuzqvVakokEmaV8UZ++9vfajweq9frWU1RKpWyVot4u81m04Duw8NDPX782MZrfP755/r8888NTxgOh/r973+vZ8+e6fLy0upuGCbmB9L7tPimFQzK0hcMSq+zpQw4Y1+sVisLaTA46/XaQie+z8/AQtvttoHnknR+fv7GPChJFjWA38CL+VExF8haNEomPearJQEiY7GYuXbn5+d2Ea1WS6enp+r3+2q32wHXK5FIGE4DSEuVL3iCz6R4xi4gla+X2IR43gXxLQqPHiFYYcA6PDZcUzxB36bSu/DSdWYOwmIikQiskVdU0rVHBWC8SXyKvrVkKJglfnJyYqEge4t2Anim4cH05XJZx8fHevLkib744gs9fvxYjx49snVvt9s6OzvT+fm5zTsCh0DBkIL2ZLNNM7w5D3gbnkmMx8vhZk/hefjvsZ/waoAdMDK+syEtGLxikWTcH/Yarx8Vc6nX629U7IIoS9KXX36p3d1dPX/+XMPhUN9++60uLi50eXlpD3U0GpknQgbJZ0DG47EuLy/NUuHyA1B6Xsh8Pre2fvV6PRID6Vlwz5eAnr1arcyDabfbdo+834c3mUzGFAprVCgUrBcqIBuKBYr8crm0A9rpdEyB+54xm26ERGaPfiuXl5fq9/sqFovKZrPGAMVr2d/fN4XNOrHWeNB4rpJ0cXFheN3l5aXa7bZ1tSuVSsaN2dvb0+XlpUajkRqNhubzuWWUfOOxTaxVuVwO9Jb2tXOlUsk8/z/84Q/qdrv67rvv1Ol0NBgMTOngBfp+Lj6TSDiFMI4FaIJQC+NO5MJz+lELF2mm44EvmLiz2UyVSkX9ft82cKfTMU3IISBDAn+FTYTwszDhhzgdAJhFYHORLotCg2XiY0/RJgtEp3lPciK29q6u3wgQxmCg+hEt4ToaP0AMVxj6gC+z36TgHYCnEapQs+IZxaSmGSnC/sNDZH1Yg8FgoH6/b55Kr9fTaDSy3/P7BZbpeDw2b4jm8RywTSnh8GBBSH6QTev1eqCBGlgS68YZJNvIqBpapkqykNlznny4jGLhrHnowodFd12jW3cfm6Lf71t/DUmWrnr8+LESiYSazaaNLOCQ8PscFISDgmvqwwcWKh6PW9iANWYw9uPHj1UsFlWr1VQoFDbez4WyBG8JJVmZP+lWmMae+o5i8DUeuMZsDM9f8GAwhX1UDZ+enmo6nQZYwr6sYJOYC/uCsOTLL7+0PkC+NQQKA3zt/Pzc1uptXAtfMgGOQvaO8SSFQsEKXr/99lu1Wi3zBnnfpgUF1263DR4gy1Mul/XFF18olUqp2+1aFEBZjAfzwxR91s17HeEwmd9HUSeTSZVKJT1+/Fj5fN4Y0fflAr2zcBHg8SbXH7Dp4ODAXDIu3JPcfKGTz8dL1y5zWDnwOYBUntHLhohCnxJITvQRJhaGp8J1ewvii+bAEihixGPz4Y+/R/9MYAATEmDJCBOJuzdNNmQtKIQrlUq2pxgU59OvrB9W1Buf8KGQgt3W8G7w/KjlIivV6/XU7Xat6BEuFkp/U1XRHFrIlyQAfC3RZDJRpVKxrCIG2BuPMOnNv/gb4ZoivkfSAJ5NPp9XqVS6sRbpLvvpVuXiLQaZGzqjEcKk02kdHR1pOp2qUCjYpuBBeU+EBwevgPes1+tAQZ4kS1PSZaxeryuVSunRo0cWc2/aa5FkQCztJVqtlubzuVlGKcigpTzA4y6ssQ+vvFLwxXUUbY7HYz1//lyNRkPn5+fqdDpar9dG3UYBecLdpsT37WE9SD1DAKPvsp8Ckc1m7eDg4fj18Z/F+tJ4u1wuK5fLqdfrqd1u67vvvtPFxYXi8bgODw/19OlTw7NY+016d97riMViuri40Gg0knRNCUmn03r69Glg5IxvXeKVRdiweyJn2JMlHQ1p8eDgQLu7uzo+PjYwXlLgM+8id2q54A+vd9/DVON4PB4oyIORi9XyLpu/SR8SsRB+HARAHhkZj/pvOoXoSYC+khTgDEuEa8pXUorUYHHYwpW6vrARJUFs3Ov1DOciHifc9AB5FJjMXlkSApAg8K03pGtPhLYUfi+FlYDvrexbERQKBaXTaaM2QLpD6eTzeQvzoyAoB5Qo2BmZHDxbilUhtnqF65UIz/6mfRReQygQGAE/EZUQ+3320J2Ui7eosVjMiuPK5bI13MF9HY/HZr19SwWA3jD5jViwUqkYLkF5+GKxsElyBwcHyuVy1pphkxkiLzwwyG/Uz8DQnU6nxstJpVKGxZAxWy6X9nOUCsxd1r/f79uIkvV6beSwi4sLm7wgXZPrqKMhLNp0uQTWEzA7DFZyeABtPWaHRSVkZOwK7T0wOOwnPLUffvhBnU5Hv/3tb3V+fm7XUKlUAoPQPFlzk0rYG3LCI9/0CeWCYSKpQnThwxzWzBPxPLaHgcZggeUB3oKx8BzCEMdd5U7pBBbeb1RwF9KH9FTBe4EEhwLA4oZrOdg0AL+wEQmTsPqAW1EbhuYfIKnzeDxuGAjZNen1GuDChzNMu7u7BvD65uSev8DvAHjSH9aHHSjnsIXatPeCEkEB4MX5Desrg3Hh+d0w4xQlQ0YNSw1tod1u6/z83KgR5XLZ9hAUeA+C8neiIFwXIaIPecLZQjxej1uCxXgA16fzSRognk8Ff8xP2HgfxSLdo1mUt87n5+cW69JrBBkMBoH/Y4Vx5fk9P3rU3wgat1AoBGan+Jm3m049hwWS32q10sXFhVKplEajkVKplLFOAcZgo4bHb/Z6Pct68CCxLLQqwPuD8dztdo0HhOfov0bt0HBYYFp75cLBkWRrQLaRhlAwwPmswWCgZrOp6XSqZrOpbrdreBSlKblcTj/96U/1+eefq1wuW4YRD2fTNUVh8WdNeo3hYaDIPiLgkf7w+6QBZw3Phf/zWaPRSLPZzNYF5X4TlPGjh0XcLOAjlpXsDcQjgFguKJ/P20WiWGjvyIVzWLhRHrZPx+LR4C5HJRS6SagTAieSZA+U3i14H6wdxEQ8FdZYuh4lQfwNuE2mgBESrFv4FRWPxUs4++izDiiXt1lZqnZZI/gyw+FQw+FQjUbDyHN4f7QkKJVKqlQqKpVKtld93VxU1shnYsiaMQTN9/NBMfsOjJ51zFe8ZKAI1pG1xAj5LgVhIuFD1ubOnst6vbZwhxx72KsgF461pnkwL8Apqjp7vZ4tGH/DN4eSFEjJ3pd+/DGFMKTb7SqRSBhfgJKIfD5va8VICF/U6WdtoyBQHr5eZDqdGr/FKySAQH4/auIPjm994MscvAfrwUdmR/d6Pet5w17E26P04uDgwEJsAF5fOMse9Eo8SoJi8SOO/RnD82JwGWxmMDtfTsK5YSqlb0rOOZRkyQXvMUsPb21yrx66HjDEiqDtyHpIwWbDniWKNZauU4hh8dqZG2cDRl3C9VBhNx9PDv5GKpUy4A6cQVIA1KUamqpprBmhEO/3GQIpeodGers1hNaAYEQ8ZWGxWAS6+fmMGvsMsiJAL72EfEOuh7r6H1q4Jp4j97perw3sJlXvlTMemQd2PcALjYTf98L7PKj90TwX/ggXDlhL9gcQE8sDlgCQyYX7F4eDB+3p/+TVUTBR9VYQD6Zx4EHh6WlLmQSEOt8TN3zPYTAPr4/vYbF5Jqxh1PCDuwpGJ+xxeWoCDam94vHerMdwJAXWhL3m+UNRFp88AZilpKHdbhvJDZImdWjsJwBwhPfhDdPGAe/Ph6LvC97eJPcuPvEazcfMPFgeuicEeZKSXzgPQvmb+VSUipebHob3RKRrheMzbr5xkaQAkMa6EPJ4d9aHFp4gF/WDExa/j/zeYp1QIL4696ZKXZ8p86Ei6xjec1GXsOfgCac8b1L5GDGfug7jWb7Qla+eD8Xf+jHXJ/YpLPRWtrKVT0+iD2RsZStb+SRlq1y2spWtfBDZKpetbGUrH0S2ymUrW9nKB5GtctnKVrbyQWSrXLayla18ENkql61sZSsfRLbKZStb2coHkVsZuv/zf/7PezHsfFUrrFI/U3owGFhvkrdJmLVL5TBDy0ulkvWcuK3e6F/9q3/1Uei9X3zxxZ3XKNzyk3YL3K/vnXvbHJ0wo9lT3mGn+laGbysL+OGHHz7KGv3mN7+5dY24f5ijNNumV41nJ/s1vEn4OdX09ACiEtj/zbvIH//xH3+UNfrbv/3bO13Q29jrnpl708/vImGG7l0Zu//0n/7TG//Yg2ZPeHo/9TGetk5Vr984FOS9TXgfM2zZWIeHh8rn8/riiy9souNN4yejKH5cCkqRQrtyuRwo9PQV4GHhENLW0q/varWy+qTRaGQFlFDoo1p35OuKKNJj+iQzkmlE3uv1AvvtbUKLzJ2dHR0dHalYLFonQz9W+FNip/tqcV8SQnsJzlS4Je1Nz52yCRTv29qZeGX1Qfq53EV80yM2MzUP/mfhht9euHjfRoCiMzacJOsB87apAVEUNjvrwNdcLmfjNe9yaHyxoq++pvgs3O39UxDvqfLceeYU2PmXv//bPo+C0EQiYXU3y+XSWhZ8auK9O9pMoID56murwm0VpGvFg3fMRNVcLhcYY+N7PT9kHz3Yc8ECp1IplctlazXY7/cDxVGFQuHWPiNsClziTqej4XCoZ8+eqdPpqNPpqN/vq1wua71eW3+UKDeQ8i56vV63DnSpVMrGo5TL5UBPG+l1iwb64npBmezu7toBpJnUeDy2Tu3hZtdRttC+KRZNoLrdbqD/D4WfOzs7Fmb773vBC8Kif//990qlUvrqq690cHCgn//85yqXy7d6iFETPJXZbGZNwvzX8XhsM5/b7bZBEMwSCwsKhCbmx8fHKpVKFh0w1RQIwvfZuY882HMhHKKNAE26r66u1O/3AxW9d3HNPd7iK4ZpM0DJOJ+36Rm/twnVvL5ZFI2MGN9ZKBTsvdz32za97zQHHkXI44eLeXc46sol3D5BUqCa2VtgX90ctspIeBQJh6/RaEiSjc3w0zqjrmBQwDTe5ozRvhKDTD8h34Drpp5JKFbWaTqdWkc/6XWjNprks4ff55w9SLnwhxmZWa/Xrd3gixcv9Hd/93emRcOT9fxX6VqbEgfy8AHl6IlKhzeGkL0L/NykMNa1XC7r6dOn1nawUCjo6OjI7o2Ng/VmbERYsMp8DtaLHi8oZD//KerC7KtkMmmtHQG8cf9975+b2k6ExY++YT71bDYzSz2ZTPTo0aMABhPl9VosFtaU/ezsLLAmKBWwk2q1at/3jd4Rb2xQ0rRuoCfzbDazXkyEkR9duYStCJrUd06jVZ9vYHPThfrmwH7IVSaTMbDStzVk4aJulQmFiG9pOF0oFCwT5mNpLHT4vnCN2Ugo7aurq8BEPLAX38Ev6mvEfgjPwuZZj8djC5No43jTwfHi9whe9Gq1UqfTUTabtfEiUZmnfZv4Z88kAK4dTMnPYweHC3t3Hj/xXqD0+vzxXkJyvKL3xTffe2WxlrFYzPCVRCKhRqOhy8tLi/1isZhhAbd5GlgPDko+n7fMSi6Xs7+BazgajWyCYVSFuLZcLuvw8NBS6Wxu36KQlLS3KmFhMxBP0zuV8LPVatmsKBR0lC2ydO3doVRns5nS6bS56/1+XxcXFzYEzlvlq6urNw5MuNUqWcfpdKpOp6M//OEPGgwGKhaLtvZR91w8FhmLxWzGNZ5eq9V6A2sZDodvtIxFwiNDcrmc7RU84r29PQ0GA8XjcVUqFTu791mn91YuaNNYLKbJZKJYLKZcLqf1em2WmlEQsdj1jCM4B3xGOKaWFMALfPzHOA7eE/WWhdw3LisgGZMQJJlSDjdJ5tB4JeMB4lgspnw+r2KxaPN4GOnqPwerE2XxaXrGyfiRwGG8zmNKYatKOOmVDK0fl8ulRqORjR+BR/OuDFRUhP3EOuHBTadTUypgMXQ9fNt9hZWyP8/8jIwUipx9d1d5b+XCrB6fZ89ms4rFYjo8PDSvA0uDwmFSHg90OByaxmXu8tXVlY1sZbojKVvkU+gZy4RARtHWajU9ffrUxjv4aYLSdcf7cL9hwgTc4HQ6bZ3uY7GYTk5OdHV1pWazqdFoFHCNo26VuTeG3a3Xa1UqFUuv84xJ50tBl973Xvb3iZIh2wE2hQJuNpuWtcPLi7KwRsViUXt7e5Z9ZfLmDz/8EFgv6Xqd2F98JXng1wzviPO6s7NjI4SYJMCYkrvKg1PRZC52d3ctVtvd3bVMCMPQwFDQun4BINz5mSk+XvSejxQc7B7lgyMFmy0DuEoKWBwsN4clfFDCD5T/g+dwgMID3tlAUT44PhSEv1QulzWZTGwCJxyMdDptYCb4nscb/Np5PpVvEM/aQc4DEI2y5+JxKfgnHqPc29tTsVgMZBJvigL8V+RtXm2Yaf8++OaDAd3FYmFD1/1YBx7iYDBQq9WysCaTySiXy1kowELh0oFmg+eQ+SCMIMxg4HhUM0VSkPjkXVW4QN1uV2dnZ4FQCNyJWTuEkF7B8L5MJqNCoWATCVE0eIB+jaNKqiPNCj7l7/3i4kLJZNLGYqRSqUBJCbyOXq8XuGe8PEJIr3SxvqPRSN1uV+Px2JRzVNfIe20eqGdNKpWKvvzySwO/CSVRvvzfA/y+0bf3kDlP6/XaFDCTUu8bKTw4FS0pAL75kaJYGBQQiP9kMrHMAAO/4K6gHfFaoDWjkf0Q8nfVF21aAL2ZljgYDNTpdEy5DIdDm2E0n88DJDu8PA+Ch93Y8Awef4h8zB1lzCWcKZNk86DxfPHqqEnjUKGYfM2Rz34g8XhcmUxGkmyekfQmbyiq4o0s5wavfTqd2oA0MrU+jc+Lc8jZxHjDlwIThBIgXRtHj8PcR95buWAd/EwdRkPihq/Xa5tlzCHjoKAgAIzAb9gcbCRftJZIJGx+DexBDwRHTRaLhWW1Li8vDZhG6TDgDMypWCyqWq3acHWAXzYUaUGPc8F3QLkAgDMVM8rKV7q2kCQGyACWy2UdHx9rsVjoxYsX5q0wfJ5wgK+DwSCQGZKuAXAyjp7MKClw+KKsXAhtWRso+7VaTdVqVbPZTMPh8A3lwtmAIMcs7VarpX6/b2crk8nYQDlmSPO7KG4oAveRBysXKPie8Ud6L5FIKJ/PK5/PBwrt4CeEyWM++wNQR1zMZ/I331ebfmwBc+p2u5Jkg8Q9cAsREf4LoQ4KGAEg5/f8mnrSHWtPWBVVLAFBwfR6PSUSCQ0GA/V6vQCnA+XiWczsu729vcA+Yi+iZBEfShBCfiosXa7T4yySLG0/m83MM+Zc+XotP4/c40+sr/eYWTPWKJPJKJPJ3Pu8vbdy4aEC5uJC+aLFdDqtfD6vcrlslrrT6ajdbtuh83R+Dkg8Hjc2KsxcXwkNFybqG8LjUefn5/Zv4mZm/pK1KJfL2t/fV61WU6VSCZRUeAtLGEWo5aujPUs36liCdB3eXV1dqdvtBsI9FEU2m1Wv11O329XOzo6Gw6Fhd+wzX1PEevkhYOVy2aaA4v0wTzrqewksBGWaTCYthJFeV8H3ej1dXl6alzeZTOxMsg98mQAANizxQqGgbDZrHjKOAQYvn89/POXCDeOqsaEzmYwpAorx0Kr9fl/tdtuUi083+opNXzJPCpGfh2nNUQ2JEK8IPDiJa+4xAs8vGAwGdmD8IfHWCPyKUADljEIJ9zCJqngcybdcGI1G6vf7Wq/XSqfTqtfr9l5C73BK2nN8CBfBBPkdwnJKTd7H5f+Y4jNevg3FdDpVr9dTo9HQixcvrI6K/SJdK++bGM7gNighEirggCgxTxO4jzxIubB5ifUrlYoKhYKOj48D7MGXL1+q1Wrp9PRUnU5HzWbTDg2kOrAa7/YBWEkyV5dB7gB0URauV5L6/b7i8bjG47G5m9yTz25IMvAbz4UQ0Ft08Jper2d4A+lCwE4ycVHmBIV7hsxmM7XbbY1GI7VarQCL9LPPPrNsEUrEu/0ef0Ops/4odwBMLDKhQJSVi3Q9Q3w4HJriHQ6Hury8VKfT0cnJSSDcYW95wzwej60ejQwde476Jc4tnrVvx3DfdPSPVhUNrwU3n/jv/PxcP/zwg9rtti4uLm4E3PByOEQoK3pOSK83x3Q61cXFhZWRQ1CLMnDJw/BxPfdMESMEQ+qDOFCeu+EVBD/j/ygbrJqncoPTIFFVMhStxmIx40tB9/c4iw+jfLhIyjTs9gOCd7tdLRYLPXnyxIya71IXdeUiybKMk8lErVbL0umTySSAQaGwPei/Xq/NaIUZ3LPZTIlEwtYOY48RlDbQLIoHQjhUrVYN/Ol2uzo/P9ezZ8/029/+Vr1eT+122zwdXwrg6xYIt3ycmEgk1Ol0rLoaGjLsSv4fxQ3ilUuYCEUPHO+hoEyoVPUMVF935BUN/AaUCwAdlslvpigKz9x7Eev12jJsnkwJQRArTOaMrJlPaaNcMGb0N2EPYQgJUaMuy+XS7vvi4kLj8VjtdjtAliQbiXD/tEHxxZ6sLfgMoVSpVLJzzNnyZLy77qMHey6kmxOJhGnTyWSi8/NznZ+fq9PpaDabKR6PWxqQF5kf3zcWhUOf3EKhoL29PaNtk97t9/uSZEBdVLEFQh5AxFqtpmw2awWMuJ3pdDrgyfnqYK80schsJry3QqGgXq8XAH0B/PiMqCqX8H2zHtSqkWJmb8EPAl8Iv/By2Ge+Svjy8lLJZFL9ft8AyyivjRTsZQtgn81mrVaN0HA8Htsagdl52oJXOigMzhN0Eum6DSafR1nPRytcRKgxoiKVDX96eqqTkxO1Wq0A+kzKC/FhEb/vywTYHGjo8/NzzedzS1USBkQ1bka5oAD29/cN9CbjkU6nLSTw4R0b3isHlAveHeHoeDzW+fm5eT9Rx1oQ7plXOKODUhiNRrq8vAwU54EB+FojLLBv3QFWsVwudXFxofX6de+Scrn8Bj0+qkrG3yeGmmwj3sfFxYW63a56vZ5OT08D++Wms4EXCC5FyJRKpRSPxzUcDs3Aw3b+4LVFxMGwAIfDoSkCrHEmk1GtVrMYmodOCASb1OfSfXgT1pLe1fdxdZTdfSnYX5iWC4VCQbVa7Q2PRVJAiZAFkRQAdH1fl1wup3w+r9FopFQqZeluYmlkk2vkuUv831PNb9v8ZHrAVKjcZd/4Qwcmwd8D3Pakufl8bmUDvocx+NSmDVS4BsgbGK+APVY0mUzUaDTUaDTUarUMPkC4r5vOimc18/meGiC95rvMZjML4e+6TvdSLp6c5Fm1sP1IWcE9qNVqkmRcFhi6pJppnAQjs1AomHsGoo3gzfhFD6ceoyR+U6BECYdKpZKOjo5MwaKkw/fhuT9eqfAVpiYvlC/vfwh1+8eUcGrY11F5rM2vma/LIu0KcHnT+BVPEvN0fqwt5QOEAa1WS5lMRoeHh/Z5m8btfBgcDtX8evFernk2m+ny8tL6KBEChTNxfk948eUjXrn0ej0tl0vjEZE8uWsh7K3KBQvj2YGk/EajkV69emUFeLlczkIjTxCbTqfGMg1nLWCncvBoOwhA6YlAPmvg2aib3hDSNUDrHyS1PeBKUPl9BTMbCEXt60FIt7JG/kCxvmSaarWaFouFcrmcker8Gm26IRKbfTKZWBjtKe3FYtHCac/KZa08tuQtt59HxLpBcadfi+cR4cVQKNvv961tKkpok/spHMaGPfibcLiwEWOfcK8QXKWgAiZ0JIyWZPsSHJCfzWYz7ezsmFFDL7xL7qVcfFuAyWSis7MzjUYjtdttwxDy+XwAZxiNRspkMvY7PpRBGREyQMaDj0A4FSZJ+cWPgsD5CYc0PGgf/nkwW7r2TkjRk9EYDAb2+Rw0GKV8Li0y6X9CK00UlQ8/Nnlo2NC0PeXeCAtpAkaGTFKA/5RKpazHD+sJTkWGCVImP4etKymgoNnLgJ6DwUB7e3sql8sbJx1iOMMZRp5duDiT30E8cZI1QmGv12vbY76rHZ4KREU/SM5n54go7tN87J1hEfRrekZw4Il9Z7OZLi4uLHszn8+1v79vlgbCkq96RqkkEglLrYGlgFw3Gg178B7sJdXmqe2bPDiEIvV6PcAV4HADktGak+I7vEDaL56dnVkbARQMm6ter9shBADmIK5WKxWLRY1GI6tL8sVrUQgbfRn/fD7X5eWlWdFUKqV2u22tFGnhyKZPJBIG5g8GgwBb1SsPKp/n87n9Pd+tzYdlksxL9H1dNsnW5ZkNBoOAZ5FOp23vw+eRgmESXiyCt0w0wJoRXrIfUDK0nvClIqS94/G4isWiRR/3OWu3Khc0G2FLvV631go86MVioXa7bWSbnZ0dnZ+fq1wum6XhAHrl4ntO0PGKDUQTJSwcN407TCFjFIrO8EoqlYoSiURg1g5WmnS9rxDHywEQbzabGg6H6vf79l7usVAoWKhABSsWablcWj8XXqzLppUKQuZKen2o2czD4VC7u7uG3T169MgsJwqClgLQHaRr7M+XgODVgQusVq97t3Q6HcsuISgaLDIZqU1m1sh+tdttS6mTcvZTIqB1AKyiXHy2kXAIFjJnBM8FY01rTCICH4J5drkvKL6P3BnQpd1BNpu1JtBffvmldnd3dXJyoslkotPTU9OGlUpFR0dHpkSIu0kzYmVxf2EQdrtds/JhN8xXtRIqeDBuE4InUa1WTQnO53MNBoMAf6DZbCqRSOjFixcWwvg+OHgrnU7HlEY+n1epVFKlUrEXiD29cDg0WDEPdIbTl5sShuX5MpF4PK5Go6Fer6dXr16pUCjo9PRUjx490p/92Z8Z4J9IJFQoFDQYDMy7xaISjvr2l5JMyVBucXV1ZX1uvOdzU6i9KeE6e71eoModygUHPR5/3T7i8ePHKhQKevTokSaTiQ4PD42xize2WCzUaDTMWHGv7FnPcEZRs04oaOkDKxdid/gpVDkfHBxoOp1aP09ubDabqVQqqd1u24bCVSNMKpfLqlQqphQg/2DR/OAz//B9KwHf7mFTQrhCk2y6pUmvD3y73TZ3d29vT61Wy8IoD+aCuxAOcXBov0DKOZ/Pm2dCCwL+FjF5mBcjbTYVncvlAgAzQD9K+OzsTLu7u2q1Wmo2m6rVaiqXyzo6OpJ03QcXz9BnHRH2EbgWCscD4T7lj0SFzkCI5in6KEC6CSQSCSMWSq/33v7+vubzuYrFogaDgVWQt9tt47x0u111u11bAw/w+tosP2bFJ07ed31uVS6MJyCPvlgszKLE43EdHR0pFoup2Wyq3W5bMaEfagb2AmALrXg+n1udEMg92pv6hkQiYe6r9DrkgpFYr9etteMmq1pJkWMtqV1Jp9PqdDpW0yLJhp2RbfNpVWLkcrksSabMfUZpPB4HPDU2IpYHI+CL9khZbzodnUgkbMrkz3/+cxWLRWvx+ezZM/X7fZ2dnVlxa7Va1dOnT03pNptN9Xo9A73JvLG/vALCM8FjoTcv+wQPOBxSb1LBoBSPj4+1Xq8t/G+1Wup2u3rx4oVms5meP3+uQqGgTqejg4MDNZtNu3c8l2azacmW4XBonisAL5gdiQOyZr542K+LN/A/GkMX8JaqW7yO4+NjxWKva3vm87mOj4+NaIPWBRzb3d01YKlYLJpVIfShByrKhcNDDO1p7Hw/FotZv142yabEM4x3d3dVrVYtDFyv18pms7YmkLn4uR8C5tcKS+EzUGAM9DNFifiWDAgWLyqALrwWntfBwYFWq5Xy+bzNXBqPx2o2m1YvVK1WNRgMLAz20wBp2u2LXDmc3D/r5deWfeI9vKjI3t6eVquVSqWSYrGYta30dUGDwUDNZtNmaQ8GA/NmSqWSxuOxtWA4Pz83jMWz3fnqEwrw0Dz/6MdgLN+qXNi0vV7PeAWLxcKUCSNEGBCfyWRsI2PJyXBIMoSfwfJwEgaDgWlGwieyBXg2WCWKI2u1WqCMwB+ujymsESl1gN18Pm8MRyrE2fxsDEobeLAoKFKJVJmDSeGR4MkByHU6HRuIhsX2XJdNE+l6vZ4dcEnmxRYKBQ2HQ0sDc53gUOfn55Yh9C1QoSrgnZHijsfjZuAIrXH9YUJzuLyns+mMoyTby3QBAATHeGUyGSMBcm5evHihr7/+2vaLHxxHcSbtKzFuyeTrUcjr9VqNRkP9ft8chXK5rHw+bzgpDGjS/t4Y3kXeyXPBLcWV9+M9cPNLpZIBQZ6ngOsa1sK4aygWskXeA+AGfJtCQhCyI3t7e4FNtwlBuQCoAjbDvyDuJXVPpoP78gVlWFk/+MqPHMUjQYF7XgwhEh6L57lsWrnQqMlX7xLSENawFlw/GUN6AnPfHjPgvWTfJAW6G3pg1E/yZH95UuKmlQuGxz9r72n5xAe1dbFYTBcXF+apUdTJ/vGlDbBsfYaMs4cTQCbS93shRPOK+K5yq3LJ5XJ2MWxQsjrQhSHLYUXI+PDQWRy/EQBrOXCMk/TtF8KeCAqlXq9bpXQUXFtSzZ1Ox1KfxWJRBwcHymazVjSXzWbNypLN8Vk0rAIEJpQy4R+ZEzYXFIBut2vjPAFE8YTAvTbd6hKG9Xg8Dmz8n//858pkMnr27Jl2dnYM2JYUwIlQJmx2FMJ8Ple/39doNApkxvwLLIrxwGHlT62Xx2s2Id6Yeo+dbCCKAkOFZwHO6UPncCaNc0rCxDfDJxtH43sUvSfkHRwcGF72o3kufpwB4pvL+NQffxTyDQxIj3qHU6Pk631fXK8d8Qg81wZA2DMpN4knELfST4VOc4R2kMNID3pPAjwlXGLhlQtYgi8XQEkzNcBn2khBStcs100rYTwtwjnuq1araTKZqFAoBLhLkt7w1vz1exwKD46/43/u/773VAB5/YiaTSoWf80oYrwL6RprhKXsvTFGJofvN0w/QPF4jIW9xTp478R3KLiJP3UXuZNy2dvbMyYfljebzWp/f98O/sHBgQqFghWFoVw4BICRuGZsDFLKvlGyf8i0aXjy5EmgiTCMxvfNwf9YQknDxcWFFouFvv/+e/X7fbOMAG6fffaZptNpIBtElki6HqLO9wC2fUYJbAHg88WLF2q1WvaV78Ps9N3tNymeEMdBz2az+ulPf6pyuazZbKaTkxPt7u5qMBjo8vIyQN9H+F2UMArBkyn5GYaKr+GmUMVi0XhE2Wx2454Lh5Zrr1QqWq/XqtVqGg6Hdl25XM6iA4yLNx5hxUrmzAP7sJl9yU4qlTK4gvYUn332mY3x+dEbdOOGE6bgwkP0weuAReiBn+l0arR/LCqNl31YxMP32IK37jz8SqXyxg2yYJvEXFgXel/QoX4wGFjDI0/j9tfrC8u8ckGZ+Pd6Tky/37cpCmA6gLsAm/5zNi08M1in1PoUCgWt12s9efJEsVhMz58/VyKRsGwI7r8Pc3x2DnzKKx2vkH12xP8OHrOvTdo0X8orB67T/zuXy2k6nSqTyRicAM7i19hnecJ8J/4fLgaFzoHxB68jKePxnvsY8jvVFhGGDIdDLRYLG6rkm8nQFUt67W3QXoAbYsIg4YFnB8JH8AuEJ1Or1Sxe9mXzxOe+0noTQphCqcLp6altAvq3+I0bLjSTrjEJsk5gJr7fsCTLEtAU6MWLF2q32zo5ObFU/nK5DLS4jJIAZDebTauGjsfj+ulPf6pqtWodB3/44QejKdCMmho0Kdi5jhCSkgjWEZyBfYRXUqlUlM1mdXBwoHw+Hwm8RbpWCnghXBPXmMvl1O12VavV1O129fLlSw2HQytvoD2CdL3HUEwekEWpZzIZVSoVwztZr6urKx0eHiqVSuknP/mJisVioHXmj6pcwrRyYjf+GAASxYTepeeCublkMqnpdGptFfBq+Bs+JYhbD/EOt5F0bBQUi3SN6gN8g9bj3fm14D69Z+azO3we2REwHCxGp9NRv983b4Uar3B7AR8yREnIPqI8qWIm68imxhOk2hmelXf5PSPXW2HmFbHuHtCEOo8iQglHaa185oqzA1yQSCQ0Go0MpsCgJZPJQDTAfvHnDw8Orhp0AGqxpOuMFfADX28yjHeROykXbq5cLpvXwM8Wi4V6vZ4V3XGzvg+uL7IjtYWSomARzQqwVCqVAriBx2u8couC4GWs12u9evXKrAiEQx5u2OLyf+9uwrxlhAQg3Gw2s7orQiAGsIN7hcl3URPu1XutrMdqtTI8DXYq72PyA2viU/AIxg9Wsw+P4vG4eb9UmBeLxUAiIgrigX7fdUC67qp3fHysR48e6Y//+I81mUzUbrcNpwKDYX04c+w/lCoD4ThzMJ9RJMfHx/aeMMXkPnKn2qLwgfBcC+ka+8CFRyHdxhL1VtZ7N9x0JpMx4hPvx6JHpdgMwSL7kvh2u635fG6ZEVoB4MXgwt4UukDJxqMBsyL0wVMhbRnmsUTlsLxNeHZU+Ho+BxXAVDrDxp3P54Y7wJVij3myICAuhxTlwsFiX92XEPaxxGMmhMvgK8AFHoMBlkilUpZEge/icU0wUaIBlLAUHEToz95D1+fOygVKMvwJX/DFjQP8Ugnc6XSsMIqvvhcuvA6YqL4ZkvR6E5Jp8jORN8XGvU0I2VAAk8lEqVRKjUbDWjJQBk8dEWuGsBmwynhpKBIAW3q++FG4myaB3UW4PsBqSbYvfGrZGxJJgcPBOtBbCO/Ocz+8gsE646n49P6mU/Q3iVcu8F34N+uSSCQCaWrGAOOd+FDbp5+Hw6F9HmEpSpvMGQrYNzN732zsnVsucBDCLElu3OMJvimPR/q96w/YBjPQE8XoJEbo5GfcRlGxSNfxqGfc+mbS0mteAsWfy+XSvDYUAwfEj9wEm+KFwvEsXCn63grir9PvIZ8ZC3NawpXeXpnShsE3Lfe4H1gfVtun56O+Ztyr/+qxTr82fhyLV0Qw6zH8PgNHK1ZP4MRjAUf9YLVFYfHW0W8Mn0YjvkNA832rBKwJN+wXjmyQ93aiGAp5AdT1ZDHCmeFwqGQyafNywJRI33tA0rvCYA6EAXgtvnl5uGDxU5HwPvKhL99DaaCEPYCPt+fX3LNSCbM9kOvTzVHLot0kYQXraSGs33K5tDCcOjbgifF4bGcMg8T9QyAkCwTY6zOMPwZ/7EGzosP/DnMNPJ8ApcJXFogb912vfKVv1BULctP1kUXCCwNrQfkCbLNppOvwyt87jNxwFfQmyYM/lniP5CaPxYfd4d/xL/YdaWeUi2ecfiqK5W3iz5ln9LKHMMZgLiRWUMyeqIoy9kY/7Dg8+Hr/X9igW9nKVqInn64a38pWthJp2SqXrWxlKx9EtsplK1vZygeRrXLZyla28kFkq1y2spWtfBDZKpetbGUrH0S2ymUrW9nKB5GtctnKVrbyQWSrXLayla18ELmV/v/v//2/vzN91zcBChdb+Z4SbxPfgoH6B2pEqEFi7tFd6h7+y3/5Lx+lKu0Xv/jFO9fI1wxR2MjaUDxGXQxjSG8S6Nm0vvAd5+j9Qbe3uzCvf//733+UNfq3//bfvvNiuG5f/sHsJWqMfHUw7wuLbxJFqYlvCEU1NF/fVQ7w3/7bf/soa/Tq1atb14g9RJsNKuLp39Lv9wMD1KjRu2mNfO+b3d1d66HLpAmma1DE+K7WC48fP77xh+9dWyQFO5azCRi5WigUdHBwEBhyFo/HrZFNWKiXoSEQbQWoCfGD0rwi+hSErnKz2cwaalHTQX8NZj/RDcwXkPl+J75uxk8QWK1W1qOYg/eplHZQWzYYDKwx92w2s7YTGC2qzKXrwxYWX+HLgcGoUcSH8Aw+hepo1qXX69lXmkQxiXEwGFgTfVrSvk0B+0rxWq1mjc3y+bwePXqkfD5vgwfZd/eVBxUu+k3PK5PJqFqtqlQq6ejo6I0LY2RrWHx1NOILGqmspgr4ps+ImnjFS9sIuu7Ru5R2g+Vy2dYqrBTC/VV9A2/+jm9c/akoXem6UJWCO7w7rK9vKYGBue3Zo1wYmkcPWNbQ7zE84E9FubBGKI9+v692ux3opYtRvq3HNMqFz2bkDa0ZRqOR9dOlE937tKh4kHLxrRRwr/b39/XLX/5SuVxOlUrFLBA3OhgM1Ov13vg8qodpP0DrPdw9WhRwiIrFYiSbUHvh4HAf0uueLqxRNptVsVi0kJHGPm+zyoQDuMZ8ZR28cvkx+nF8DMFbHQ6HOj8/D4Q89MKlzwiNpQuFQuCAeCFkpKOh79I2n8/VaDSs0VbUK+0RmkYNBgOdnZ3ZaJFWq6UffvjBuhRiyCRZC4+bWnIANYRHz+Dt7O3t2Ywk+hG9T5/h91Yu9MigLWEqlVKpVFKlUlGpVLLeuL6LHDOMwvNopOuwKCz8Xjwet+HtdEWXgmFDlIX1KhaLqlarqtVq1lTa9x2hf8lN4ptleRzipvYDKJeoC1idn3TglSSzxlkrPxTvJlcd5eIHy/H++Xxundj87Kyoey8YFBqH+R5HGA+mUfrm7CiX8H7yPW5oiwLWB7yBp/2QdXpv5cKIAsY05HI51Wo1lUol1et19Xo9PXv2TN1uV5eXl+8EmcLd2HCDfdtH6Xrmb7FY/GTiZdaqUqnoF7/4hWq1mr788ss3MCtJFjYxtN0LfWGGw6EdRN+rZDweS7o+YIwaibIsl0tr8D4YDLSzs2PjPmi/WK1WA+AsM3SQm/rASLJGSLRQ5UAy+0e6Dl2jMN/pbbJcLm2UDx6Y7zfMPfp7Cc+98oLxYa3w7thfYKLD4dAM+fvgUw8CLrhBWlQy3xc0mzk74/HYYum3tar0rQzBFfy8Gdxl2vuhpPwUvagJ+ACtO/P5vMrlsvL5fKCHK1/DDZXDwhow/6nX6xlO4Wf1+ObfUVcu9HIFiMY6AjjSMY39gOJ9W3tP394STCufz5tiwXPZ2dm5c+Zx0xKeayVdwxKE0z6TFu65G5awcqFhOcqFkS/T6VSpVMrO7H3P2oPCIm6ObuIArr1eT41GQ8+ePTNswHeTu8lKoFTYGMzAbTQaNlrCLyIjNaIMYOJ60q394OBAX3zxRWBMbi6Xs2yRHxzuNxGCa0rT8pOTE3W7XZ2entroXBQ+Hl/Us0ZcJ43GUS6E2eAt4FKER+GJBwj7gd95+vSpyuWy7b9YLKZut2vTGRjHG2Wh/etqtVIulwucIxpvk5qm678fwxMWlAv7zIPqvkcvrTOr1ar1vL6PvLdyCWtKrA1ehvTaPfc9UMOtG31DZjYFDxrAiQ0F58Gn5QD7oiwMPc/n8zaMiofKmFqAXTwcKTjjN4ypYHlpxoybT09ULFbUQ0YkjB1J12E3mTTffxkPGQm7/r49JvuJg5HNZjWfz83TvisnaNMCnsJ9gBuhbAiJOWPMfkJheJ6QdD2qxzfQD7cW9Rwj/uZ95L2Vi+9ST/zKjYMbVKvVwEXyfuLG0WgU6LVLLO3jyXK5rFgsZpsAC0e+n/g8igJRKZ/P6/DwULVaTeVy2WLiWq2mL774whSotzYAd7PZLDDvyM94KpfLgSmWIP1YoU9NfKYrm83qyZMnqlQqOj4+lnTNq8LAeKCRf5N1ZBwLY27AFabTqa0doXrUxfcIxphyxvCKJVnCRHqtYH1aH9Ac7BOIQpIpGU+Ww4Bj1N8nvH6Q5+Ivem9v741ZNAzvGg6HhkB7y0oc7UdBeFSaw4m1R8kQaoGQR1X8psBScr8oFBSqV7zM0+bQeEvN2qzXa1tzMAUIVCgl6fWBjcLY27cJ7rnHVHjmhULBXoh39QmT/SssPpuCJ3x1daV0Om3rH/Um8JwFvHl/NthXnAk/W9yPTGZsMgo4/Plk5jyJk736vo3NH+S5ML+31WpZ6pOQZzQa6fLyUpeXl3r27Jm5V36wNtmATCZj7hsWm5uD03B2diZJhr+k0+lACBZVAXchLGJTEw7t7OzYkPXhcGgjW4fDYWCYvQcxpdcbglm/q9XrUaitVku9Xk+j0Ui9Xs+mCkR5UgCcFDAm+FH1el1HR0eqVCo6PDyU9FpRQB7jkPgO+LcpCGb6kFUplUqaz+fGD7mJHhEVAUMiKcBoGgxXr9dTrVYzTwXPbjQaaTQaqd/vGw6KU+CpCnjDYFycRfYsGOFH47nwcMkKxeNxnZ2dGfh0dnamly9f6uLiQs+fPzeLVCwWVSwWVSqVVCqVTPPi5RD2+Nk/WHPP7QDLiCqYi/ghVB44Gw6HGo/HarfbGo1G6na7AUYlYSHpfrwXz9b1A8QTiYR5PBy699kQH1tQvtJrg+WnIQK49vt92xPdbleNRkPT6VSj0cju0Xu/GCY8ovAQNCy/H0gXZfFzl7huP0Dw6urKxtQAkDNdcTAYmLEajUYWJsIPw3P0s93ZV3hG75umf5Dn4mcOdTodC39Go5HOzs7029/+VhcXF3r27JkODg705MkTPX36VJ999pnq9bpqtZp9XrPZ1Hq91mg00mKxMAS83W5rPB6bRebwZbNZy79HVTzT1K8VFpPaEDwWUtW1Wk37+/uq1+va398395TZRRwUHjyHcTgcqtVqWZyM8vXxddQE4Ja50YREsVhMg8HAFAxFehgrvDO8u3K5rGKxaCxeDoo/IL4uKaxcoqxgwKBQBKTYCa8B9VHEjUZDJycnNnM8jONhfFDCrDmlKHg0xWIx4FXeVx5coAOw670NqjMLhYLS6bQODw91cHCgzz77TI8fP9b+/r5yuZxN1QuPKAWv8RqXn+OisRhR9lxQLhQskuUhPobGTS1MuVzW0dGRMXiZ3Qu/x7OYAexYf592BliPsuJFmNJJjYskW5fT01M7THgqnU5HnU4ncECwvN5D8VYe3Il18nO2/VCxqApKQLpmNFNCs1wu1Ww29f3336vZbOrly5dqNpt69eqVYS6SAnV5fPVD5MjozmazQNboIfIg5UJqCw4KNUTz+VyFQkFPnjzR/v6+Pv/8c+3v7+vJkyf2u4C6AL48cB56r9dTt9tVp9Ox2HGxWBh/gTKDqBcwEgKdn5+r0+mo3W5rOp2q1+uZIn38+LF++ctf6smTJ/rZz35mISMuPfVVPs0oyUBfYmribepqUEhRtspkQPBUFouF3S94iA/BSSBgbT1LFYCWQ0NLAQhz7Kter2cYF2FC1JULDFrWoN/vazQaqdVq6eLiQt9++63Oz8/1ww8/qNls6vz83JQr+GZ4ZCuK24fVZBwhYz5E8T6YoYtWxQUFla9Wq/rss89ULBZVq9WshgYqPxvfKxYPPII/kEpjMdhIpOGiLChfwsZ4PK5ms2kPHfbo4eGhpahxY6keD89M9iA2bi6Ff3gB0utn8ykcHM9xkq5JdYlEwr5SpwbWBgbFfkN8lXiYZEeGxFdcUzkc9aSAdN2Cg+ufTCbq9Xo6PT1Vo9HQ5eWlut2uptOpUfrhBMF0DrPdfckNeykWiymbzWp3d9dCeDzE+8qD+7kALqFUOCxHR0f6xS9+YcAjeEO73dbFxYV5KT4c8sQ6FMtkMrHcva9nQsFEfWPAqMVTmUwmKpVKevr0qQqFgr766itVKhUdHBxosVjo8vIyAFL6tDX9NzgQHhTudru2MQBDO52Opf+jKr6VAskAKshjsZjS6bSKxaIpIbAULK5P0/tskS+rQMn70AhvmMxI1NcI4BawttVqqdVq6dtvv1Wn09HLly8NRkgmk9rf3zfoAI8FQ8O5Y90IOT0DPp1OazKZmLF6H+/3QfR/SZYeS6fTKpVK1nYBVuV0OrVeEwBMuPC+/sFTmj3IBPcFIJeF5u+zeaIo4dQxpQ2UTBC+eEr/bDYLWGXPZQA/IDzikMBjuKmyPMqHJixhxjfPmn3g+TC8vNL1tWikqAk9w/gUyoXs5Pv0K/nYghfssz0UZO7v7xug6zkqnvnOWbspg4mxp36JTC3hNe+/zzo9OCza29szjOCzzz5TNps1IHJ3d1fdblevXr2y/hPhegeP4EuyAwi1ncPk06vh3hJRVjAcbpQhShNFDGWfcIgQkPUjDCRFiFdC6ETWqd/vv1HD9akoFk/u8qUAeCqA/7j2Hqil9ggFQwbIh6SEh9SkkaJlzQHBo65gYKiDrVGxjMHCgEsKFML6F4rWG3V6CFGfBu+nVqu9oVzu03bhwaCFT+PxQFEeHBrSyn7zhAsOWQisFWnmarUqSep2u3bwwBmoEeFzopg5YsP6vrcQ6Ki9Ii7m0GSzWSPc4cGQ+bmpGtgfRBQvtVdRXJOwYFVRBHgS3oh4jxbFS2jk++SGQyG4P3yWL4BEsRMuRL2vi+/PQutOPFiUBYkPT3/wHkrY2PF9j+WhlIfDoZE8aeLGOt5lnR7c5tL/Ie9i+VQr6L9nVPqmydwQblcymVSxWFQ2m7VUbSwWs1TlfD5Xr9cza4P7FzXxB4PNXCgUzGvZ3d21gkMQ/WKxaLgCsTIKxWd/wqQwNhzK+yYFHmXBgmKAfHc9AEWYqZAvYWlLekM5+ALX8XhsCYVUKmVNp3w1dtTF11BBxWDfg6fQKsHXD9F/mM/gjPF/HxX4fRaLxayfy2g0CmB9dz1rd1IuPh0oXVtKOCfe8qAwfM0I//asQt7jeS7T6TTAsITR66s+Y7GYkdCor1iv17dOFvjQwv1LMmyIdcBVx8KSIapUKgFKeqVSsZ/hybBGcDO8S0zhGhRtzxWi7D5KtH+/RtK1QvTYiM/kSLKwsV6vK5/PGz3dZ4u4vzDe5PEp7xHncjnF43GVy2WjwqPANik+S+rX6qYQT5IlM7xxxrCjAHK5nAqFQqB5lHRNAwkrZIw1HqHvcvA+++nOngtxmLcivqAJotfu7m6AXs0hI2XIwUFh0bEe5YK18r1i1uu1er2eubm8V5KlcO/ba+JDCGvg2wZ4xULbRQhzmUxGxWLRmLngMXz1Spt7Z41QXPF4XLlczmpJ2Iy+05r/ugm3n02McfJeFwfHF975Ju4oF9qDEtaQKcKaeuYpKWsyUDwTQs6dnR1VKhVJsg6HvrXAJoTrJnTxCQz2AVwdzpdXDIRB/ud4w2HaBmFTuKyGUJP3o1x8VfSPplx8psbXvXiFwh/01O1sNmufgYWiUXer1dJkMgm4bIRJhEM0n8LboaITr8Vbt01T29HmxLrz+VzJZFLlctniVLqq8QKExKvZ29szNnM2m7WsxmAw0GAwUKPRULvdtti5XC4rm81aqEUbyHg8bp3WKBOAESzdvx/HjyVY0/F4bHvFGwQOU5i05VtvFAoF1Wo1MzoobA5cv98PfAYKBeVcq9VsjVhvjCHtKjYphGd+vIrHg1arldWgoQC4F4wV5xLDXigUrB0soRN7azabqdPp2N+DPAeuEu6X4xtJ3RWbulW5oPl9Gitci+ERZB68PyAoF4Cny8tLnZ+fWwqVh3twcGAtIGu1mt2cbwcpXfNfsODexduUeCANkJmsF5wMUso+tey7sGezWQNyKTbrdDq6uLjQ+fm5ms2meUHUgABmw+Yllcg1eVCUZ7AJ8QcHLAg6ezhTFGaE+vvFu0Nh7+zsmLcGwB8OjzgYeDOEE9lsVrPZzJ4Ba7SpDJvvUwReiZLh/MEBItvImcP7I1JA8ZZKJev5AwMXDAoDT3JksViYMZeChbG+ROJH81ygZVerVbMCntDTbDYNVa5UKgZQVqtVc6UGg4FOT0/18uVLfffdd2o0Grq4uLCQ6tGjR3r06JEVNR4cHKhYLNpoEf6u35A+1brpWNmTACUZEF2v1w034iCAIfmO/2x2Zs/MZjO1Wi1dXl6q2Wyq3W6bMq/VaqrX6xYmYNXS6bSk1xjFavW6ATWT9yQFuBGbWC82L20TUa5+HcCdwNZQwngtTJbg/bPZzOqPTk9PdX5+bi0F1uu1KV9wGg4vFj+fz1s5iZ/zsykhRIFSwHBAPDI/YbFUKlk4nc/nVSwWdXh4aGTNMOYJuW4wGASafLdaLfsZhskrWKIVlJLPNt1FblUuHAy0H41qpNep4bOzs0CDo0qlor29PXOhIOI0m029ePFC//iP/6hms6mLiwvrRbK/v2+zfL744gtz+X2KTHpTk+ItbVq5eGYxcS7go29ZiSIhnPGkMIr2Op2OGo2GHZhut6tut6t6vW5e3eHhoWVLyAgw9CudTuvq6iqQbfLZgU2J7zwoBen5nigI2IpHgXeH55LP503Rwuk4Pz/Xd999p5OTE11cXNjn1ut1FQoF2z/wNzBSmUzG1up9+5X8mOIzrPTCxdAwjZIKcdaUrBfdDsfjcWAKgE+UtNtttVotdbtd8xL7/b6dXY/xeNjDdz+4b5HnrcqFmP7p06dmQWgSBWA2m8306tUr80gODw8Dnsnp6alevHhhGhXLS+uFr776Sl9++aU+++wz7e/vG4LP8DSPHRAiSQrgCJtUMLiRZHkeP35sCgOwjCbIOzs7GgwGpggGg4FhKVj2y8tLI8elUikdHBzo6dOnevToka0ROAIbA5cfdzgMmIbrbD62eOKWTyljWOjxg4LFe6F3y7Nnzwz4x5qy316+fKmXL1/apAnY4igRH376FD7Gyr/Ya5sIjTi8FF9SbIkiJZMG32u9Xqter2s2mxmUAEMXkiBEQZq24QigUH1myq+Hx7x869EwJPIuuVW5gBWQCiTc4QDwQDqdjlarlc7OztRqtbRarcx6djodc79gFJZKJf3qV7/SL3/5S1MsdKTr9/vWOIoqa3/jHuuJggCW4YkdHx9rtVqp2WyaO8sDAisCK5CuSVAo02azadgDvJj9/X09fvxYtVpN+Xzeho+Tmiam9kSyKAkhslcuPkQslUpKJBIql8tGKgQkR+HC1sYSX1xcWDh0fn4e8OKy2WygncBNzaI4MJ54t0nPhfXxrVB9g/tWqyVJgYZPKIpSqRTAZFDK7XZbzWbz/2vvzHbb2pLz/1HUxHmmJkueTvfpPh0gjQAN5AGC3OYdcpVHylXeIbdBbgMECNBBEvSQnG4f2xoozjNFUgP/F8avWHtbngdu/8ECBNmyLHEvrlWr6quvvrIPfgf0BV/N9U4ECzuWD7W3OpfhcKjb21tT/vIsUR/ReK5KJpMJtAXQc8SNg3PZ29szXRfyv2azaZ4XkSjfBYvDIS/1fJlVGWh8pVIxjOD6+trwIc/DmM/n5mS5lXzal06nrbwMtR1wHBGoer1uGAq9SRwmvwEHg0FkuqLZN3zGEbCpE4mEbm9vVS6XtVgslMvlAvSEs7Mzk+AgyuMQgecAoPM7WFsvhAQQTDrmD/Gqqf/sZSLhWOxVtzJE0k6nYxIbkFPRtqFFgkuKiIWeM5w7vBecOhc2+402HlIfii2ZTCYwTvm9n+lt/8iAavoVstmsEZ9Q94/FYhaqXV1dBQBM3lBuJl4oD8cDgs2gt0FTGVqfPqxlw1F1CLNVv7bRnZ3P5y0ElxTgFfhSqye78eZKyw5gbhTfnAfgyyQAUkF4M14aknSMRsgodPxCZvMVP09rIH3hgCACxrox6QGAGoyKioqPiHxDq//ZlG/D0YuPiKPgXMB/WLPNzU2LYhFzn8/n6vf7Nnge7A76Al/3JWZoD5w7P/sJhwtNgnSJfQU88qHn7K3OhQfAQQDY8iIfPXpkJDYIPDQdMjgcI7fDmdBw59m/lHQ5HNz00+lU5+fn6nQ6diOXy2VLRTzQ/LXNt67H469mEZHK8Fw4TzYNVRI29NbWllKpVAA/AlMKV8YAONHMIVK8ubnR5eWlnj9/rmazqeFwaOuyyhKrJHO4ni+1sbGhTqdjl046ndbBwYFNEERELJFIWBgP5gcLGZVCUkxo8clkUnt7eyoWi6pWqwE9Ien1ecg4qFWuEWxj7ywhFnpmPKlSIpEwx0DqRzrppSjgn5Fi0zaAcanx7+l02vYl/W9hoanPgrlQnoJqT7iPpy8Wi9bYhHMhYqHUyAOQFvi2b9+lycHyPRFUGCix9Xo9Ixdxy8MmXJVzublZjpT1bQv5fN7+/fr62lJMiFs+z2c9WRspSJfHgfFvOBiqKJDUBoOBpU83NzeBm3yVBoFLCvafEa0SQXAZIR/gq0g4bfYR60cVDqN6wkHBMRH9egfHheap8KsyIhSv3u87l33LjX9vPSCN+Uqk1xEmysZYQ6ITP3CONJ2vfcwZe6tzodQLcEQjU7FYtIfZ3Ny0kRbtdtv6fcJvOjfDfTcErE1u2e3tbUuPXrx4oXa7rdPTUw2HQ8sNDw8PjUK/SkU6zz4lf4UByhtM5EIp8U09HlJQmT6sJIZIVrFYtOhQks7OzmyEy8XFhR1Uv+FWeSt7yQScZDweN+GjXq9n5DhSu8lkYoUCLh3SPFohSBu4zBgDnEqljJS5t7dntICbmxsDw1Fu87jNKp0Lw+xGo5Gl/OBuknR4eKidnR27bNFk9qlNuIVA0r0XGAZssb+/b+LmXNY+XfIfny0tuo8VG+ZokApRlmZz0HDnzXd2eryE/+NfOGg4OjAAVNDkIRB9KMj0uY0Q34evvvmLTuXNzU1rDWA9vXaJdy5gLjgVohQ2Azc5v5+KCjOPcHCSVh7uS7KyJ7ezFIxmkUTIZDKSZKkea+qFtL3IkV8rH7GA6xG94LTozwGTgDzm1fpWtVZ+bfxZwCmTEoLt4aR9VIP5dfIXl29UZI/SLMtEStKs+0rQ0of1pr2zt0iS9fG8ePFCvV7PNj4sRwZVoysB8IoiPemSb2qEpUkqQYhMv8Pz58/VarV0cXGhwWCgra0ti1hyuZxFLbzGVTkYwvxOp2M3Tzwefw10pvxKDwjcF39YfBc5DsKLTHsxZd6PVqul3/72tzo/PzdFeCoOAKGkoKu6mX2bAljKxsaGJpOJdnd3bYAbAD8bnRnYcHl8GwrGWgLY0psGizmXy9msce9UiFq8FMgqIxcuaJp6iRyy2awWi4UKhYIGg4EymYxGo5Hq9bpVjbxMJZVEP6aWaIQ1Ans5ODhQoVBQpVIxAqM/R/wZB+7L9u9j75VPACyxefv9vpHDJAXkBPxNiVO6ryOXB+EwEeoOh0MrszEwjPGbcCOy2WxAmW2V5vs+GIEBKOmrJER7AIvIW/rGNNaR9SHUD98m4AWdTke1Wk31el3NZtM2FGvC9606cuHZcXr0hYFBUQkiCvQOFsq+dy5cYB5zYJ1Ii4hawAvAM/x4EZjDq+5NY40AY9kzfs24WMDvFotX0yfJAnCQntWO8bNIsRmRWy6XbYoGTi3MwA1zYD5b5IJR/mo0Gib8xEH3/THSUmaAyMY3VdHnMBwOA2zC2WxmYT1YS7fb1dXVlbUflMtlqwKAeK/asWA4l1gsZuS5QqEQ0GTxjZ0cKqIe8BRffua2ICKkFM370Ov19OOPP1pj43A4tMPmh9MR8q/y8LBBqToAyAKEj0YjSUsMwGNX4dtUWgLnHhjHEdNtDgcIKj0VNCI8hob5CQCrdMLsE/YHjtbDB6lUSkdHR5rP5zo6OjLYgGiQlI8ojXXnA6JquVy29BHcjj3s8RpAcN8D9tmciy9hwp8AjKOHCFyBB/ChvacS4z0h9bAQlKSbzaZNbfRyh2wWcAb+vkpOQti4MQBut7e3jcKOgyH64FbyG5r0KcwixTEjV0Hq2Gw21W631Wq11O12DVSWZFgFkUBUVNZwCERoPo8nosBBE/XxfVwk/gPuBj8HToavHnJgiIiZM82lxmHylblVmsdOiEi8kTbhPGkVAIJA0pM0yXN4qP6kUikVi0VrV6HNgOyE1yEFq06fvVrkH8rzUbrdruLxuCmN40h4ETgbSD/+jWu1Wmo0GsZPgKsAsEbYBjaBGhl8Fj9KIkqGxES9XrebmO7oRCKhQqFggK+n6rNGbBCcAikNfVZeVKvT6dhhgduxvb1t6whHaNUaJd5wAITwGKqCpD+7u7tWGGAvhfcADXs0uHrwkgY/z/ClJ4cmWq/6HxXHIgV5LPwd5bkwOMuFBU7lR4DwPUTBEDfDJWvWh0omKaLXvQlLnnyIvXcN1/f0+BvX37SAPvAySJf89wOkwbAkjOMggPz7nh3wHJ+LR8089iLJuk95c1FN82Vm375AW7vX2UB5jvlEOB2cDXm2tFR7v0/TJCpGJLuzsxMAZr0KnQfBwUqIYLi8fNTHmsBX8VM7+ZngFH6MaxRwlrD5VhYONI7PUxbC3wee4nvLPDuZZ+Z3eOIg6+cB/zB/5mOzhHc6F6IWnId/QClY3oIABGHME4H8TcyDeEo2f/aEHe+wVl0Veh9jc1Nm3traUrvdVjKZVLPZNKYzzpKqGxsC54JxULxzBv/C8aD9QerEGkft4EjL1IiIw2NBDMwj3WaDe6Id/59Dw/cidEQFzj87UZyvqngaQFSiFgzHAdtWCoo1gcUA7OMo/SXvmzWhhNDKwzn1rGTP/qbXC8zvY6MW6QPV/3lx/s3zNXQWAk/o2YbcqnBgwh8wBFkYzHvOKDsWjM1KJUKShaTczACN29vbBm5ubGwEdIQJiXE2OBawLz58hBOVRsW3mY9Iwpubg0OTLJGO77/yHCIqK75g4LGnWGwpV0DUEmXHIimw13mN/tyFzwDf41Ng1oT/5/Es+EW+qsTXPb7yoVT/++y9MRdpeXD8LwwTwDyQ69X+iWDCIR0hrt9M973p34JjkZaC05Q+6X7udDoWqfiyIMCax6l4o314j6Oh+oHzuo/GHnXzqa13mjwDvULspeFwaDepn/LAXgIo9mkQFxp/9xM+o+hUvHmwm3W672x4/CR8gXunHT5jnhvkUx8ilc/VyPlBkYs/4G97g/wLY1NAEvOlWWnpZT2vwVepvhWnEjYOOTcEtwu3LGGpv239RAWPJfBn72B8d/UqJSw/1cIbmMMgvS5j6jvCw+Am3886e8fr9Xmj7ljCFo5kvBHJsE6exRyuRobxUT575+Irlp+jGhv7Fjfk2ta2tujbtxkWrG1ta4u8rZ3L2ta2ti9ia+eytrWt7YvY2rmsbW1r+yK2di5rW9vavoitncva1ra2L2Jr57K2ta3ti9jauaxtbWv7IvZWhu6//Mu/vJVh5/sawk1W0lJLFkEaaTnFLWxQj/1gK2k5ChQdVSju76K6/+3f/u1XEXz5h3/4hze+CN93BYsWGjq9QQg70RnuZSnD5ps5kSOgi5xmSNbba3C8iW35j//4j19ljSqVyjuZml7p7L794Rth72tDwXyfUlgvx0tdvC8LtdlsfpU1+ud//ud3rpFnJSOF6dsCaOi8j8Xszeu1wPxF9sOv8/uydf/u7/7u3m/4aNl8Xgy6sZ56zUFhIFM6nX6tUzP8sGhHFItFpVIp5XI5G7aO5ge0dy95GHWGMQ6Rpjk6l+kb8oJFUrCH5L431bdNeIlG1obOdZwNPy9K4lrecCZIB7BPvIIh/UT+0vGSoNLr7Sis93A4DEgshC+3b2EPYXd3d/aeX19f28GnI57eq7AcBeZbclCmY+SIbxYOd5p/rOzCJzkXbtrxeBzoafCCyvQV+ebH8EbwfQ38m/fC/DsbMYpaJW8y3/PhpSuI0hgdIS1vEm6Pt/3MsM4pH/xf5AfCamZRtLDgGJMLJJlYFBGZd0J+uJe3xWJhzrvdbpsCnZei+FYcirR8v1E6DDcB0/Xsxyr7hk1vXsaEgYfIf/B7mFeG0t0XF4sK2/X1tXq9nobDoc7OzgKhaiwWM+0SXjx6LUgE8L3+oX1n7Pb2tnUMX19fWyew/x1eUyaK5p+HWzifzyuZTCqfz9vB8ZqvXroxbNxaTMLsdDqBERlehc13pUe5+dPrj6A0l81mrYO8WCzq+PhY1WpVjx49stEqXmzKG5cQ3eToDP/2t7/VxcWFTUSQ9M1Ev0Smo9FIZ2dnAQexublpkhI0arbbbTUaDY1GIxvFLC3PG1Ef651OpwN7Ecd1cHCgdDpt89C/iMzlfXZ3d6erqysT3EZcGHHuVCplw6gQhPKzkv3DYqis3dzc2FxqnE54Ah3/P+obg42AIFY6nVYymVSlUjHJRqI0Pr/NudBljcIdHdccKK9c9i0cHIxxM/5SyufzKpVKOjk5UalUUrVafadz8YJIiJRvbW3p9PTUOsrD8hRR30d3d3fWET8YDEyuw8uZhC/asF4ShpyH14vx86O8DlMmkzFh9S+moXufzedzNZtNdTodnZ+fK5PJ6PHjxyoWizo5OVEmk1G5XDYh7uFwqG63a0Dmfa320lLOr9ls2qzfZDJpimSo2ROuRbmFHodLFJbP5/Xo0SNls1nt7+9bZONFot4WZXidEsSl2Ch+nAlaLyi6faroz5c23s9KpaJCoaCnT5+qVCrpyZMnFrnwDGAEb5JPCMsPpFIp2zv5fF53d3d2q3NDR9mxSDLd6na7rfPzc5VKJX333XcG7I/HY5NQlaRisajd3V2DLcIZAtGi9Orc4TjAS5vNpmazmeLxuGazmQ0f/FAphk8GdHmDGCaOQv/GxobG47Gurq40HA41GAzU6XQsbL/PuXhhm3Q6bcAk0xe5jdBB8ZFNFA25QmZH5/N5G9rFmx8W9LlPtwPzuiRh4z3wCmNhKckomr9pEeFmrk42m7XIzn//2yohGAeI1DCfz9sYXNT/v6XIBQyF4kcul7OodbFYWFQGkI2UJRcMBk7jQX6vdy0tpzGQmbDvPvSS+mTnslgslEwmVSgU9PDhQ6VSKWUyGXW7Xf34448ajUbqdruWPnlg876fyb9Pp1Pl83ldXV0pm80GbiRusCjfxpIMY8nlcnr8+LEymYyq1ardOOTRaLti/uCEJUUXi4VFb14U6ebmxg4hs3iQjfQAaRSNy4Lpf0dHR8rn8yoUCiYJ6h3mmwbH+/0A1pROpyVJjx490s7Ojs7OzjSbzQy3AqPxYudRs5ubG5sXlkwmLaqbzWZqtVqaz+c6Pz+3iZJEt/5iuW9t/HxtJjtClxiNRjaD6+Tk5LWK0vvYRzsXcjcqHmAFyBQOBgO1222bTTSZTGzcq1eeC4daPj1i3IavePiKyttu+SiYj1wQHvf/5mUbuUm4RcPKYh53um/teD8kmUI+P/dbMF865bWHb0pfVcI878M7ZL9+kgzfYi6yn74Q9TXivZdkaTbRCPiRjza8Y6FiyF7h7PAziX6k5cgb1plKMHAEOM/72kc7Fz9BD6SZShBo9bNnzwIK9n4KANWj+8S6wVIYCcENHIvFrGwG6BTlSgg3J+kiurjScrog6Dx4CRwPHwoT6vKZTcLhwOEmEgktFovAtL2oR3fS8vAMBgPDSAATed/9OBb2DUPmvBSoJ1lKSwwvkUioVCrp4OBANzc3FsEQ8kfZyBJisZhVGz0Iy8U9Ho/V6/Ve+79+gqc/W6w7UAO8od3dXQOPmepIVelN5f/77KOdC5uZNxsWLm8sjgBPxwPwYOEHD4NOXihYknlunAqfvRePmvnX5W9TSQaQAcx6pqSPVt6ErxD5JBKJ14iFzFbme78FBwNRMplMajQaWSoMb4rbmKiMS8hzQPywNYx/Zz8xTC2VSlnpPspFAUmBiwIHCzwAjYHRyVzkHsfzzGQfxfi9BlWCS0taYnyevPoh9tHOZWtrS4VC4bWcjodKJBI6OjoKjGyFqUqYtbGxYTe4pMAgJj+ak7EcXuAax8QcoCgam9aHoDwfERhDq4bDoYW1ngzF5vfrjNMiGmKTsQm4YXBQUXYuHP6rqyudnp7q9vZWh4eHSiQSgVlX/ub1aSHr6feGHwxGu4X0SgA9n89rNpupWCxaseE+pxQlA3BlSCDR7sbGhgqFgubzuZ48eaJ2u22XFXthOp3aBe6H2Yenek6nUxWLRUvjNzc3NRgMdHd3Z4MLP9QJf1LkAqjqZ0N7zwkGM5/PNZlMbNIiDwwOwUPjYemXYXD25uam+v2+HTw/FiHq5kdm8sZ6QBvWpR/aJS0xFD5IATzHJ0yV96V674Si7FwweFODwUCXl5dKJBKq1WqGCZACsAYQDtPptNHYwf18mdW3pHDg/Bp6DDCq6wQplYogZ4coZD6fK5PJaDabKZ1OvzZihAFyGNUyaUkDoMRP1Osvw4+dCPBZMBd6h7a3t21+cSwWM7xkOByq1Wrp9PTUbiq/ITKZjL1wmvFKpZIqlYptlLOzM/V6Pavnc/NH2fwMnl6vFwAjX7x4YYh8r9dTo9GwvBlWc6VSUaVSUTabVSaTuXdcp7QkWVFR8LNpom7eyVLpkKTxeKxsNmvOtVar6fnz51YRyWazyufzevjwoY6Pj3VycqLDw0OLev0cHz8rGoDSl11ns1mkq49bW1tWet7d3TWHCtFwa2tLzWZTkox9LMmcCvtnPB5rMpno5cuXtmdubm6s/M95BnZgLfk5X825cDMSnXj0+urqSu12Wz/++KMGg4FarZaurq4sBYL+DgWeyYOQwgjNEolEgGnJcKx39ZZExXgDJVmezGZm3nOn07HIjmdLJBLKZrPWEkCfkP+53PSUVTudjvXPENZ+ruFWX9LCfIvb21sNh0PVajX993//t2EKg8FAg8EgAD7mcjllMhnjDXnuEx9Ezr1eT/1+X61WS/1+35xM1IsC0rLqKC2jLVphtra2rAWEedgeG2FGFgAwDoVLmp9Nm42P7sgccGQfuk6f5FzYvL4szBv5/Plz/du//ZtVi/L5vPb393VwcKAHDx6oXC6rVCoZ0s2b7icRZrNZNZtNXV1dqdvtqtVqKZfLBfCKKN/ORC6kO1dXV2o0Gup0Onr+/LlhLYlEQrlcTtVqVQ8ePFCxWNT+/r45Y7AGKcgFAsu6uLhQp9OxYfX5fD5AEY+yc5GW5WTA2larZZdTOp1WuVw2oJHLJ5vN6ujoSAcHB9rb21M2mw3MGff7cTKZ6OLiQvV6XbVazQido9HInEuU1whs0nc7e4ihXq/r8vJS/X5f3W430ALipyz6PjT69zY3N5VMJpVMJtXr9WyfLhYLVatVFYtFlctl5XK5r8dzYUPM53P1+33L+wnxJ5OJ0um09Yns7+/r5OTEmJKUomHe4k0BIfGi9FTgYWG4em5NVI3clfDcc30IbQ8ODpTNZlUqlVQul3VwcGCtAmE+i48UaeYkFWINWTtJBnRH/WaWgpGwj7joJcrn86pWq8pkMsrlctZrRPT7pt4X3gNu7slkYgAltICor4/vJSNSHY1GhpV0u11rVK1UKvZ9i8XC9ILYe3xABwGzg/ZBYyfOhxnuX7VxUZKh/BcXFxqPx2o2m9ZHNJ1OVS6XVa1W9fjxYz18+FDff/+9gZCEub1eLzCaFO8MgIcnxXtXKhXl83kL1aJsvGmU36+urjQajSRJ5XJZhUJBDx48UKFQ0P7+vvL5vOFMgOM0k+E4+JnwD9rttkVH/DubY3d39zWqdxSN10eqm0ql7N9oXnzy5Il+/etfa29vTycnJ681LtIcy8+TZI6YHhsiyOFwaF3230paBI40nU41GAxUr9clLccl0zqxt7en0WikXq+n0WikdrsdAH59awBOi/PWbrfVarVM1gHHDnTx1Z0LHo9Qc2NjQ5lMxpxAPp/X3t6eksmkRSKAj8PhUO12W8PhMMBfmU6n6vf7kmR6FPl8XqlUSoVCwcBgX26NKsuSg0PXMx3j5XJZ+XxeBwcHgQqAr2C8aQZ0mJAoLXNradngyNpReYoq/uIxkp2dHRUKBWWzWWtkPDo6UrVa1f7+vnK5nLa3twONev75wgYACrfFkxNZr6iuizd4KUSp19fXAWIbTG/OF07E7yNPNmRf+dSRKBt8xzOYP+Z8fVJahMfrdrvq9Xq6vLy08PXRo0f69a9/bYep3W6rXq+bd8QhEZ6SEhHCdTodzWYzSw3K5bLi8bgqlYphEeSAUaW683ri8XiggXFvb0+//OUvlc1mVa1Wrf+KcNb3DN3XqOip8jgQz1gF7OMGorwY9QMEveHg4EBHR0f69a9/rVKppOPjYys944gBuX3K6BtZ+VoqlVI8HlculzOu1WQysdv7W+iul4JALlBCIpFQoVAwOsfV1ZX6/X6g7M5zeofiG1pxWLFYLBDxenqItJTV/CqNi/xCHnx7e1vFYlGlUkmHh4dKp9MWhtZqNXU6HV1eXhqq7Ts4PfsPb0vpMJ1Oa3t7W6VSSYlEwijz4U7PqDkWKSjOk8lklEqlDCcgzGQdyIO9VrBfXz5DHvTMTPAoUieIVL4fyVf0oma8ZsDaVCplmNqbIpIwhyfMxeAAUW07ODhQPB7Xjz/+aHwaL1nh084oGk6BJk4upidPnli6xHONx2P1+32rDoUZuxiXE9iTVx2gjYRqFI5aen+M6pOcCw8tyRxApVLR4eGhtra2DBeo1+vqdDpqNBoB1qk/RGwOoiH0TRaLhQG4gMOeqRn1kJZ8mc7Tvb29QI8GNxGt7UQi/vaVgoeIMDiZTBqp0Msx0IvlI5woa+nyuriBSWM8Me6+C+Q+KYGw0R5ydHSkTCaj3//+93bhXV1dvSacFFWJCs4KrSJUX3/+859rOBzq8vLSOFMA/X4vSEuRczAY358UpvkDBkvScDi0lPVDMLy3OhffZSotJQQJSf0tivdnoxOVDAYDNZtNE7SRFHgzyQ99PxEHgQWRgkLOkgxrWHU4G+7KDb8eqNu8YbyJw+HQ1g6HQvoESBfm8rDBdnd3LbcmXaA6wPcQOkfloISjCr9WRC6+CjYajTQej02PhQ9J1rvmfybpEn8nveR7SI8ePnyou7s7NZtNbW5u2sHCorReUvCsSMsu6Hg8HtBs8R++8sr/kWQ4jBcY8+lRKpUyiQv4RqPRSJlMxnhqZCmfzbng/XxjE14+3KXKQ8NN8bV339oNEIsivmfo0jtBWM9i+w9Po191WoTT4PWG+4D4mm/Eo+mSAyXJnIlP/7wkAxvHCwLhhMBZiPw2NzcDHeWrNI+LcOuxZtLSuVBVG4/H5lyQO/WSjjwTX2dPepYt+43oFhGzhw8fanNzU2dnZ5KkWq1maUEULEwqpJqDQ8UZhJ2J7xPyYLW0XAsyAvhCrD8/P5lMamNjwy4muFjpdFqz2Ux7e3uGxbyPvdW5UN4kRcGzpVKpQA6WyWSsf4bvJ4qBaevHRsTjcfX7ffX7fWMVek1TSdZsBcpPGOdJQvAgcHqrMFIRP3fI5+7gH5lMRnd3d8rlcuY8WEtuDTYVkYtfSw/u+v4auoh9idE73SikQeAmAPA4YfA13/EMBhVW3Qt38IbxkeFwaPuLg4Nz8X1wYFXZbFa5XE7tdjtQxl+V4TSIYkkHmYBBBOwvMU9RIOpFicCfN8h2/kyCbRHRSbKyM3vp7OxMNzc3Go/HJq7/tqkUYXurcwEMAtDhliAE3d3d1e3trR0MwnAOWFhMChSahePnw+W4vb0NdHBub2+bc5JepUL9fl/X19cBjQpJgZD4axobngMBSS5M1+/3+3Z4/GgRlNIwv8FJfUD/vcOSZFEO0YuPgsI0+FUa6bR/j4hioOfjLEmTw93gHhPxB8vf5IvFwtTVfCrl9yz7Fp0dfzGt2rlwSQE9eHF7nsF3unstFs6CH4gHrR9aB5VZnBCXHOtIjxEREN3mV1dX1qITDgLeZm91LmjeXl5eKh6Pq1qtKpfLqVwua3d3V6VSSZubmzo8PNRoNDI6Prnczc2N6WewQLPZTN1uVxcXF3rx4oXOzs5Ur9cDokq+r4byIyEz5WuiKJomV2lUa3g9HBq8PmkcYuWbm5u2Kfxrp0eo0+mo2WxaasDvgDjIZzZkOOwNyxyu+laWXjkY5g8lk0ktFgsdHBxoOp0GyvBcZjw7XfbeyfJ/Op2O2u22Op2OpFeRLFT1o6MjFYtFKwJwiXE4PFYYBecCk7ZWqykej+vo6EiSzGnSdEkWgdNhDyE1e3h4+JrcJc6IXqGHDx9qf3/fRovQFEy66nFPzrEHet/X3upcYNriXHwvUTweVzabVSwWU6lUujd84+F4c/GCkOfOz8/VaDTUarUMyAyzArmFCAXH47HG43EA6FrlpgAr4M0myiJFgqtDVMHN7NMXacl2bjQaev78uf70pz9Z6kg7fLVaNZ1ZpBr5/b4p0jOdV+1UMNJouBnsp8lkou3tbY3HYzUaDevHgq7uR4GA5Y1GI11eXurly5d6+fKlXrx4YRHKw4cPtbe3Z7cz+9FHQ9IS31r1xYSRujQaDVMF8Ep7rIcvn0tLvAo1yHK5rMFgYPsJrhN7c2trSw8fPtTjx48tGBgMBqZmIL0OvnNR+cvqfeydaRH1c3oQuJWn06kuLi4sRaG8CrA2nU41Go1MLKrdbqtWq+n8/Fynp6fq9XrqdDrK5XLa29vTDz/8oJOTE2WzWSUSCXU6HfPSHsjyIXH4dl5F+M8Ar0KhYOxSUsPxeKwXL17Ya53NZjo9PVWtVtOzZ8+MVAcfqF6v6+LiwoiGOJUHDx6oVCqpVCpZLw3m0wzfm0Na6ismqzJC7HQ6rWKxqF/+8pf25/l8rnq9rkajof/8z/806nqtVtPvfvc7m4nFvms2m6rVamq329bUenV1ZbdyoVAwLpGft8MhYa+Q5r+phP21DV4K54mUBbHsP/7xj2o2mzo/PzeYAayOGWGXl5d2yUwmE/X7fWtwnEwmJs+AYyWVpxUinPKE0/sPtbc6F990JwXHOqCVKy1nPeMZE4mEzY8mn2Yy4/Pnz/Xs2TOLbKrVqo6Pj/WLX/xCP/vZz2wzkPeFnYsHAKMQ8kPye/DggVKplCqViiQZW5JUqNPp6Pb2Vu12O5DTbm1tmfQCDhiHXq1Wtbe3ZymRxwlYd5wJ6xRudPQyBKsyqOq7u7vKZrM6OTlRoVDQycmJbm5uTOun2WyqXq+rXq9rMBjo9PRU0isHLr3af+fn5/rzn/9sfWmkzR7DornVs7jDUS7OeNVrg00mk0BkxXMhJnZxcaFWq2VQRblctubCTCaj/f19W0uwPRwMWAsQAo6VSi3ppq/e8fHFnEulUglwUHK5nDY2NnR6eqrr62tzLjiUUqlkYNlkMlG321W9XtdPP/2kRqOhi4sLSdLx8bGKxaKKxaK+//57PXnyRHt7eyoUCqrX6+r1eoHcD+6Mr+HjeD4EYPoSls1mDX/yal9EcNVq1XCTyWRir53bAgyJJjqaM1Op1GtCSAB1vOEcqmQyaaVEaVnql4Jt96s6REQGlJgB/qk60nQXi8V0dnZmFQsi55cvXxoYu1gstLe3p1KpZNU3+pC8PACDvDzYTjkX8DOcXvgq2yrWaHd3VycnJ0qn0yqVSpb2otEjyTC5Wq2mer2ueDyu/f19PX361KAGmPA3Nzd2HuPxuMrlsorFom5vby36Gw6H6vV6ds657Lyz8474Q5zxW50LknmSDImXZMO9nz9/blFLNpu1DU5ujDjP+fm5er2eqbvncjkdHx/r+PhYP/zwg7777jtrROx0Olbu8g9EBOVToig0LXphKz9YitIg9HzvYKisjcdjtVotw2KSyaTpl1SrVT19+lSPHz/W3t6eyuXya30i/sZm/UhfqS5EgcVMWuMHpnPreqUz6RXPqdFoqNfrmbpaeIQpY4I3NzctXcS5+J/p8Sj2jKRAVCcFb+pVOmBwTFI8qmt+nAiUD4a6HRwcmOI/io9IppIao9fCz769vbXiAVQQuENcTF4UHYf8WXuLCoWCbm9vbSRGs9nUdDo1T4e48WQyUTab1Xg8tk5fUhiEugnbcrmcCoWCiUXBkUEDplarqdfraXt726IZlNrpmRiPx4E5QKvMmalmNJtNYy7H43EDdungpmz/5MkTcyyA2zhHXyJl4iDOyVO4SYUIcVlrqgTSEuhbNcFQWkZSL1++1GAwUKFQ0PHxsQ4PD03LhmgCiQUkQH3TJ5GOB2FxJmG9YZwLjowIkd6b4XBoH4x2XaXBfuV9REe4XC4rmUzq8ePHkqSjoyP1ej3d3LwaxfIf//Efyufz+v3vf28XMKA368FlJMlgisXiFb1/NpsZ54WI7+LiwiRlETVnROyHUD7e6lwSiUQgQgCkbbfbdhP4UuLu7q4Wi+UAa27PbDZrh4HQFc1O8Jtut2sfo9FI1Wo1IAqFNwXA4/ev+lYmyhqNRuZc/Eb33B5pedBI8/yMaCRAfaUDXRevjSoteSye2UoqEea4rLpqxGvgUJyfn2tra0uDwcCiKygMUBHG47FpkfgqIt/rqxe+XOrF4qVlBzmguReK8pMoVr1GXhibi/vu7s4cb6FQUL/fV6FQ0M3NjZHa6vW6RSFcQDyzj2i3t7etckS6A28FUJjvxUFxfol8qNx+FufijfDs9vZW6XTawnPYkKDX6LtwE93d3Vm+GIvFAlKP8XjcSqdUliD/oD4GGHx+fq5araZWq2URERWaVeIJ/X4/oJ4OlwMWZD6fDxxyblLeWD/WAodEnu2b1bh9AIFpK5hOp1Y5abVaJvDtNXRZ+1UZFxQdvfV6XTs7O6rVapJe3cY4EKK32WxmQK60XBtJrxHICP/D+4D9iBBSp9PR2dmZ+v2+fvrpJ6ukkA6tco2AE7rdrr12VP2JQpPJpI6Pjy26I7XzZFKiHy49/i/Bgdd4IerL5XLK5/N2Bi8vL9Vut60J+cmTJyoWi8Yt+yzOxS84YA8hqqe6exFpvJ4HHimNsmCeWcnNQY7HA3gZTPCbXq9nGxQJvlWDlRC/uDX9B07HU9I9DRtGskfkPU7AJiBK8z02oP2UHdFGhQEcLlGv0jzRj9InHJ5MJmMb3WNsvls5bL6S8SbH4L8HfhT8Kgh4yC7437GqfeSJojB1uXyJUKkMsQfClUJeu8ck/VnD4fh182ca50PKDp+qUCiYSNeH7KV3Ni4SsRBaXV9fm8DR2dmZ5au8gZubm2o0Gna4ePFsLpxGeIOg4QHGQk745z//Wa1WSxcXF+r3+3rw4IExMBlUvsrDg4OdzWbmMHwPDAeFSg6VHRwqzMywahhRCpsKHMevHdFKt9u1FIPSeBScijdIdLFYzG7nf//3f9ejR49UKpVM8tM7FPafT4PCLQDeudzHh0JvljL35eWlyX9QEYlCKRp1/VwuZ5cOvWfQ8Kk2gntKy4ubqmBYndETNdmT7KNyuWxVtel0qmfPnpnMpfSqWsx7A4/ts1WLpKXKv+9KJZxqtVp2qDgsXouF3ghmGPtmu/BtDYWeD9IBFOxoisxkMhaiEf6vcnPcF2F4djKpo+ekUN6TlnNmfDe1pMDNwqYh1SL6Q7qBaMj3f2GrDvf966CSxeuu1Wra3t5Wo9HQYrEwUXLp9aHovrcozHPyv0NaphR0WCOpChMVbgcVlfD/X4VxthD98hUtzp/vmfINneA0RMdhfkoYp+LCAsfiZyDQDdaSSqWsIEE69NmqRRhpBwg8IsCLxUKNRkPj8Vjz+dxo/zgGsBcPWhL2c1BYLDRyKbdRvn727Jm63a4qlYrS6bS+++474zKEFelWZbe3t8YrwPH2ej1tbm6q2WwqmUwaJT2XywUcRSaTkRRUm8Mp4Si81ALM6Ha7rT/84Q9GuoPT4Csp4Vtm1U6GytXd3asRoc+ePTM502q1qr/4i78wQS3AXV8l8pUvDhotAkQtCB6dn5+r1Wrp2bNnxupFWwiOEYeY17ZKw/mmUim7OHhmeGSxWEy/+c1vNBgMVKvVjJzZ7/eNnevX2Ee+qVTKnDIaxRQNqKQlEgkdHBxYherhw4fWxxa+tN7H3su54EW5QQnxs9ms8TNwQIT44RDWA06+49NjCByku7s748ig0MZ0OUq0fpZPFAyFMHgcPA8OgYiNqgiv33NVPM/At7aj1RJW/Scdkl6Xwgx/LUpGGI+m7fPnzzUej5VKpVQsFq0PZj6fW5MjabZ/Jo/z+a7i6XSqVqulRqOhy8tLKyAwVoQUlAMYhfUhKuCceDE0QNmdnZ0AWbPf79tz93o9O59Ed+GMgKiGqiTrSTBA8y0i+LCcP8axSB8oc8mGJ08tFotWR/dlYvokEKbhRUNp9k4F1L/ZbBpnZjqd2mYoFos6OjrS999/b7N9wjf0qs2X9aRl6ZVDvrW1ZeVXBMuLxaL119DpDB7FetKkiQA6FHBIZty+rCuOJQzER8VYD8J3CFxEqhcXF0qlUiqVSsZSZZIEfBjKojwXrN92u23EO5wv42vAVsAtAHGjdDlJS5Yu6WMsFlOr1bKyPfhnKpXSgwcP7JzQPoIzGQ6H6nQ6AUwTh353d6d8Pq9cLmdOGQcCDaJQKJiawadoJb23c7kv54XHgQaE74cg7CdCAdz1FGJuHbqlATgJbSVZSdoj1qvSbnmbkeOCofjcF0COWwTtW3CkRCJhZCZPfQcj4ODUajU7PH4md9TA23dZuDRPdEq6PBqNTAQrk8loMpkol8tZlEx5drFYmANuNBpqt9vmeCHLhQfusXf964iScWkSUXkNFS5qmkBph0CbhcgXEqrvJvfCXKgcSq8CBaJm1AiQYrivvP8h9kGRi6+EAALF43Hr84DLUCwWA4ATOTbziYhyIPWAjsOs3NnZ0dHRkXZ3d43FyYjSVQO4bzJuCCQTfcXD81kAeC8uLmyzkAZ50pyfrYNTB4j0Gqg+XWAjRDFq8YYjhHQJH6Xb7WpjY0OdTkfb29s6OzsL0P4BNcElPGjOWjEDmuiY9fOpkK+gRMn8e0g6yNp4Kc6dnR1Lh2HRHh4e2lqRNfD/mZPtqQpUH32ETeMxDuxTmzo/KHIh7PalLjgnOBMA2zDPAEdEVcSXEqVlL04Y4KV13of+UbVwtSdcUmWTw84l6vDjHbxzIc0iMuEA8cZTTfiWHEsYM/H7yVfeIBsyGQGn4HlW91XY/Kgabm+P/626FeJ9jPfVYyc8E46R5s/FYhF47wGGcdzSUpg7vE/ClUsvhv6pjkX6wMjlvlIpB8QvDI2G/sb2cpekOtzi8EC4iePxuJHpiFY+x8N+DbuvtOk/+x4hH9UgbygtSVBEMuBT6MAy4/c+p+J/X1TNk8K8A/YFAMDZMLhNmu2pDczE9ocNx+Odi//9UTVem3/fcZQ4G0nmfCnrt1ot+/98n6eP4JS80gB70beSeCf1qfZRc4vexA3wVSVJr72hHmvh+8BQ/BRFQrZwW/y3bqxBmKMQxgH8DesxFX/jhG8r/zu+BQs7Xh8J83fP6uV7wxecFzELm49Y+P5vcX38RR52xv6S8tkCmGiYY+UpId6pSK8PmvvkZ/gWwsS1rW1t3579/xESrG1ta4ucrZ3L2ta2ti9ia+eytrWt7YvY2rmsbW1r+yK2di5rW9vavoitncva1ra2L2Jr57K2ta3ti9jauaxtbWv7IrZ2Lmtb29q+iL2V/v83f/M3b6XvQh1G4Iavea0V3xsCrdi3Bfjv80r3UJn9qIhwD9Pb7F//9V+/Cs/78PDwnRTnMAvat0JA4/eSju8yP0JCUmB+tqe7v4vGfXFx8VXW6Fe/+tU795FfC/bU7u6u/dlPPXjbPvK0eOQGUPHz6+TtbWv+u9/97qus0d///d+/dY38fvE9U16lkBGu4X6ssPk2AWk5WSGTyQTGMr+vZtI//dM/3btGH9VbJC07N2mo40Uj3YggUjabVblcDjRR+QFU4WY9xH1OT09NpR3dk3D/TdR7jnwfiLTUMuWg+LXb2FjOLaI5776f5z94fg5Qt9u12VLhjuGomu95YT/RsIriIB3xCL17R+p/jrRca/YTUgPz+dy+7rVko76HpOVZwwkzQYGJnV6gjYHy/oKWghecd1SoETAbiYmfNBJ/iijbRzsXbls0STg46XTa9G4ZfsZgJ7pV7/Oq3DTJZNLGVaJNy8xg33jmo6Womm864+/eITOjh68lEglls9nAEDVvvinNN+Oh/XJ392rEK+34NLNF3cIRFhufA0X3vG/6vO//ScsoBOeM/Kgf8u4/fwvOxTenEo1xRhgpQ2Tmm17DQt1YeE9w2eOc5vO5KUZ+ihrBJ0UuKFbl83kboL6/v68ffvhByWRS+XzeBoTRAcys27AR0SCIxKA1IhkW0I/s5HVE2XhjCO/RpkHO8cmTJ4FRmoVCQbu7u6ZTfN/Pw5lMJpPABIDT01P1ej393//9n3q9nsbj8VvD4ygYDhfz3eAMRe90OhYBkzq/Sf6DvYEANQp2jUbD1skLL4U1TaJqPmIZDoc6Ozuz8cbSUiIzm81aRIYuTthwGKy7V4Ocz+fa2dkxp/yhI1y9fXLk4kcUFItFUxZHJxWNVEK1sHMJt9x7iUtuL9Ipvs6tHPUNgfkxK8gIFgoF7e3t6fDwUOl0WrlczsSxyHu9+QgI5zIcDk0lHmFl5Aqn06kJe0fduDnRA2ZvsQbT6VTpdNqGqN+X8nknjoo+2EEikTC9nHq9HnBgXsgryvuJyxehMdbMp5Re/wi52fsuF5yPl9IkjWRt/Iicj1V//Gjngoo9Qsrlclnff/+90um0UqmUer2e/vSnP2k4HKrb7Vpqcx/Ay8/znpLRJfwO5uf6FOJbiF78Zie6q1Qqevr0qY6Pj/WXf/mXFqmEJR3DOILX6b2+vtZgMNDV1ZWSyaTG47F2d3ftpo/FYjZwLcqpEaJQSFRmMhlTHzw+Ptbd3Z2NuEBO9T4cIXxJeTW1xeLVvKvBYGBD1vleRJSirPPCQWfeNZEXqXUYq2I/3ZcSScvIBdAWp+LxKSIZUu0vrv4ftjDq7Ac24WW9VON9FRF/4/A9fkQCmro4En/L3Qd6Rsl8qA5Yu7u7a5FeOp22Ad9ovobXNGz+1g2rijHbBkfmcYqoOhivPkcoXi6XVa1WVa1WJclEqHO53Bufw1eQ7sOlGLfLSA2PT3wLIlJ+EBwXux/PA0bincubLKx2J8l+7mQyCYy5/RTc7qOdCxEIEQkgolfyBzepVCqBN+++Gwenwb/t7OxoOp2q3+8HRlBeXV1ZaDifz19LH6JmRGPVatUG1edyOZVKJbuhJdlALz+tErsPgGOj+VJ9LpdTOp3W3t6ebm9vdX5+boLoUXUuHpBm+Pxf//Vfa39/X999952lkmH1tDcp8BHeM7YEYe5isajZbKb/+q//slEtRARStKNfojt0l0mtmeXlx/cQ+TN+5b7nCkfC7KFut6vxeKyXL1+agyFA+Ji08ZMiF6Ys8gL8G+8PSFj+kgfzoskbG8GRneAtpArc/LPZLMBXiOqhwYgweNM9N4GDEJZ2DD/TmxB7v3m4fXFmpFlUj6JsOA10lqEw+JBfCqYxfq/5nyEtU3ZpCYTyb6lUSul02gaKES1G2cI0DNYJ7M4PGeQSRm+ZyNVzf/iZnDnOJmvGdEYqtvz+r+Zc+OWSDBPhAb1T8OmOT4vC4zPC+qiEesyyrVarWiwWNovG551RGpDmjc3PczAeJJVKqd/v20weADjvcL1At/T6zRrW4r29vTXgLZ/P6/b2VtlsNgCoR9F8hWd3d1eVSkWPHj2yMaJEw6RNHlMJz1GGOyTJRsEOBgO7kDY2NlStVjUcDlWv1wNOJ+rGId/e3lYmk9HR0ZHhnZLs30izAcalVxgdUTEZRVjDWJJFQ/P5XOl0WrVazXAXpk58SPr40c6FDe1r7oPBwFTHr66uNBqNAoPpffUnfOP4vJfbG2/J4WMIlB9w9a2YdxrT6dSGow+HQ3PEYWYlUQ+HxufI0tLBcIuDI1D25iPqeIJPAWOxmMbjseEjRHqkTdKyrO95Q3y8SdjcTwtgRrKP9qJuvNfsCegM5XLZntFPNQ0TVjmXPKuPYGKxmEEYGxsbVu0luqOa9qH20c4FjODu7k6tVkvb29t68eKF8vm87u7u1O12dX5+rnq9rufPn9v/A9Asl8uGO2SzWbvlSXXYUCwkyDizf+bzuW2QqBtOA2c8GAx0fn5u4CX8Fm4XPwrCbxpm9/oo0IfEjCplegJTL6NsOE9JdiDOzs5sMDrzsaEwwNQlhCddJmL2M7II+eF8MHs6n8/b174F54ITZV9sbW0plUqpUqno5z//uaVBnni4sbGh0Wik0WikjY2NwIVMBOjZyrFYLAA/QKjb3Ny0md4fCnx/crUI7kq/39fZ2Zn6/b7RkS8uLnR5eakXL15Y6MpsZECnVCplE/S8l+XhAOaY++vDuk9hD35Nu7u7sxYGQLnBYKBWq6WXL18qk8kEmMh+E21tbSmTydxbVcI8LZx0E+A76tGdr3rBCB0MBjYN0Edgi8UiMBHQ4wg+RfS0d9+Xxg1Mykk0+C04F5wkOBHRG9gaY4CZOElGwWRT5orzs7yj8G0ArK1nkV9dXX1URe2TIhdfHeJmAcEejUZqtVqq1Wr66aefjCi2ubmpo6MjZbNZVatV5XI5ZbNZjcdjG2TvmYjNZlODwUBnZ2dqNBo2VzhcUYmiscnn87m63a59nQYzOEBwOEghWVt6tKrVqkqlkkUwbAZfRoW5yxByWM33MTSjZBwSIrN4PK7Ly0vt7OxYCbpYLGo4HJqT2d3dDThQ8D7p9UKBTyuJVmazmR2eD2nQW5URveIwccIMjce51Ot11et1tdttXV5eBiZ0+pntvv+Kn82Y3EwmY9FfLpczJ/VVnYvPAbkdCD0hwME6ffjwobLZrCqVig4ODnR0dGRpESGxx1I872EymWg0GtlhoarAgkZ9Y/AsANBgKpRgb25uNBwObbA81R7SGlKh+waDh+kAOGewHEqxH5Mvf22Dp8NtubGxoclkYpENlw8OlP3hDw0HEKcivUrDoTCA5/R6PYsAcMxRT699JYyzNhgM9OLFC8Mkm82mXr58qU6no3q9LunV/mNNGTzoozrWikgQOIJeLJ96f6h90tXvwzOPl9zc3Cibzero6EgnJyf64YcfVK1WdXR0ZC+W8HQ4HFqbuJ8TzC0+HA7V6/U0GAw0Go2sEgUTOOohLanQYDAwR0PaRyrARzabVaFQ0MHBgXK5nAqFggqFguFSlGZx7DgtwHRA4nq9rn6/b31ZUXYuPsLIZDLKZrPKZDLGu6BYwH5gdrQnD+JcqLhJwZaLeDyu4XCojY0Nu9UhaOJgvgXn4qOt2Wymer1uLR63t7fqdDq6vLzUYDBQt9u19cnn88rlclZF9OX9cIvNbDYL8IMkWbTjiYnvY5/kXLhVmOsMMxSewt7eniqVig4PD40x6Mth3LrkiNwmHDbQa29sIt+8GFXzbFsfsfD37e1tpdNpZbNZFYtFlUol7e/vq1qt6uDgwNaTmydc+YEgB7ZC5MPXoXFH/eAQ8U4mE3MEPI8H9v0mp9SMsw2zSf3oUulVKopMAQ433F4RVfPYIgWNfr9vRRM/bxzMZTqd2hpMp1Pt7u7aXvHMcY9JsWe4lOi4/tjL6ZMaF8nXwE0ODw8Nwc7lctrf3zdvy83T7/fVarVea6ry5LmdnR0lk0nLF/l3SQY6fQvORVrezP52JDqLx+OqVCra29vT48ePdXh4qCdPnqhYLFqJEZzg7u4ucJBIq9hM4FDxeNzSyfl8HvneonBR4Pb2VltbW3b7djodnZ6eKp/Pa39/34oAVEh8CZqDw2H04lOj0UidTkf9fl/9fv9eCkTUDcdKsaPf7+v8/NxwTw92s1e2trZ0dXVlMAINnDw/mNPGxoZFLM1m06JhL1XxofbJiKjHXHjDff/G1dWVWq2WgZiEtf7/c/B4oyFU+RvKA5jhGv23YDgH+AmkP7/61a9UrVb18OFDFYtFO0DIVEDx91UC37jn6evJZNIiyeFwGFjXKK8VaWK329VgMFCj0VAul9PR0ZFisZiq1aoKhYL29/ct0svn88rn80qlUq9hUdKyixgOB5dZuKwbtqim2R6T9PIIvF7vVGkP8EC5rzj6VBCHdXt7a+ez2WyaPMXt7a1hYB/qZD5LuQUPSJ08k8koHo9rPp9bHgipzhN+PF/D/6yNjQ3bNJlMRplMxjYJOaOPCKLeLu8JgZlMRru7u8rn83r8+LF+85vfqFKp6MGDB0omk/asRCy+wc7Tuf2mkGSYBYg/AHGUmxalZUoCsfDq6kqdTsfWI5vN6sGDByqVSjo6OlImk1GxWDTQ26fI/qD5pkSiGpyLL+V7jCaqxt65vb01tjEEVQ+4gssATSSTSQNxKQp458LPJp0cjUYaDAaq1WrqdrvGE6Ii9dWqRRhv3GQyUbfb1c7OjjqdjmEm9LX43gX+zoYA4OVhJNnGqVQqWiwWOj4+NpIUXnw2m92Ly0TNfO9GLpezm4UIBUAyFotpNpsF0Puw+a9RlmVdOTTFYlE3NzfK5/NaLBaaTCaRFo3y/Ss+1X706JHK5bKOj4+VSqUM1Kbjm5A+fFi8cfvG43Elk0kVCgX1+3212+2AhokHyqO4n7yz5Axw+fr2GnC8XC5nEIKXCuU5PWuZC4rf4QsrdNn7ws372ieT6MgBJ5OJOp2Otra21G63rYxKRy63Cw/jHQtOw5e2kVOoVCra2trSo0ePtLu7a/0OHJaobwppybBEKcw7F24TL8voiV5vejM5hAghSUsafblcViwWU6FQMND8Q5H+r2W8Js8Axbk8ffpUDx480C9+8YtAkyJ/9i0k3nwKAReGyK5YLNoNjdPlUvM/M2rmHQF/jsfjJtvhGxjZXz6i8Q4onA6xfvwdxwKY6x3YZ3Mu/k30HpODHdbFwCnwwIT4vInz+dych5er9A/nD4HvrclkMppOp2q32wZkeiLdKjV172PL+nXzG5c3lzSx2+1qd3dXNzc3gcY7KSitEGadcpNIeo1EVigUFIvFVC6XdXd3Z7yjVYG79zXJYbz3XDSx2KvGy8PDQx0cHKhUKlmPEXuNfejJcr5K5IF+8BycNmtSr9etxB0FDM+vA87RVxkl2eWLAwFPQafaM259KZ7olgg5LGvpL2pSqo2NDfV6vUCrABjWA2HfZwAAEhtJREFU+67RezkXwm1elAeStra2Ank/zoUwFlV6CDqoWhHW+lsHnMZT1r1zub6+NnYiXhXdilU5Fw+uSkEhHr7unQ1riUo/DF2qJNvb2wEA0pewOVA4IZwzjonvLxaL2t7eVqlU0vX1tXUAr7IszbOH14S/exYplcaDgwPTZ/biRV6uglI1PwfJAc/I9c4FpvMf//hHc/RRwKV8FMZhJivg63wGtKVPKpfLmb4wkQ3YpAd0PUcG6oePbEndC4WCcYboivayKu9bpX2rc0ETgl8Uj8cN62ABCJ/QdVksFpYL+9CLvofJZGKRDG3wGxsbxkQtFosmoOR/h283gMdx3821CvOOhcX3Xbu+d4PXzBvstV1YS88B8vqmfMaxUJKFCs73crMB5H2sBurnMn4/B57X7B0tGx2HQMPizs6O3bCsD3+nS9ofRg4Iz85+5GCUy2W7+JLJpIbDYeA9XJX5yBN9ZKRd+XdfOuYyBUvhwuF5GfGTzWatf49Lu9fr2T7xl5kkA3CTyaQWi1cSJ+Px+LVL9L2e6W3/CEK8vb0dcC6EVqPRSHd3r1SydnZ2Aow+z8fAA4/HYzWbTdVqNesd4uDt7+9bFQA5Qw6iD+dxLnjeVUsvhFMiH5IibyktK2rcKp5mHU43fbkRvgpclnD6xKGlQuI7yQGM7yvVfk3zzgWn6Dc2ZDnCftaH56ZNAtkFWkEmk4mtNc6HVJy0HIfFDQxPJp1O2+W5asciLacQSMtG16urK+tJY+2geRA9EO3ibABgYXiXSiVrq0AhkjYRf/GxTpKUzWZtL3GuP0Zw7J3OZXNz0w59oVCwFzQajfTs2TMNh0OjUxOGPXv2TOl0Wvl8XvV6XRcXFzo7O9Pz58/VarUsTJ/NZia7kEqldHR0ZAxVDhOemdvIU5dXHbFIy/THV4T8CBHa+31zGCLL19fXlhY1m02rsA2HQyN7MeyKW1qScrmclRh5jzY2NuxmZ6Pi0N4GDH8N89UISaapUq1WrQ0Ch0EUNxwO1Wq1JEmZTEaSrDCAY6BgAJYnyYoLOC3S70wmYxE4kXa4erJKIyplb6AAwPvOZ4TYJZmTTqfT2t/fN0fjuWbSq0hlNpup0+mo1+sF+vT8vvWtGNIrJwNjWtJ7p0PYW50LB4JxIU+fPjVwCLCnVqvp+fPn5tlSqZTOz8+Vz+cVi8XU7XbVbDZ1fn6u//3f/1Wn01Gz2TSnwOKk02lVq1UbTwL12IeBPpeW3jz06WsarwWniyME1KbaBRGJVIAoEEfS6/Xs0OGA2+222u22OVpumdlsplwuZ1q5kqy3azabWaTpI6lVVtJ81EmkkUgkVKlU7Cb1VSOitn6/b+zu+yoekqyJkQjEp5a+IAC1gZTeO5YoOBf/3Hz49g2PpVBpZD0TiYRKpZJViXCoOGrYyRcXF4HRKqj9sS/5XaSttOzs7OxYVP3ZqkXkZYjTnJycWKjd7XbVbrcVj8d1cXGh0Wiky8tLczKU/QaDgZrNplqtlubzuTXnEaGcnJzo4OBAT58+tR6k7e1t3dzcBIZ68TrYFKveDBgbGKFk3uCTkxO7UXjtbI7t7W0NBgNJsnVjw29tbanb7arValkzIg4dgmIul7OOc9jLpEW+qhKO6la1ZiiaTSYTpdNpPXz4MBCNJpNJo+STElFGJ8zne0ejkd2+AOLdbtdSRt4D0iefbnkWq7+YwmD8Kmw8Htv7t7m5qUqlYq8JDIYWCZoWNzZeyXamUimbBba1tWXfS9Nvo9FQo9FQu91Wr9ezyhGSJ55Y56u57McwM/x97Z3OhfAaoSd0NumLmU6nyufz1kwFcEbe5ycC3t3dWZPe999/r5///OfmXGh29AAnoK0UnLPsS7tR4LbAN/B062q1qkwmo4ODA6uEeIZlKpWyA9DpdAK9QUQygGleywVwjgPnK24epwrzWlZ5cHwK6Mup3sGkUinrZUHDlegDUST2B1T1TqejVqulVqtlKXK5XJYk+zleSCq8BlGIfDG4SkS18FS2t7cDkRtSsv1+X4lEQr1eLzAcjymlvV5P7XZb9Xpdl5eX1i3NbCjOMTIVnpRIdOgZ9Dj+D7G3OhduCBrJisWicrmcDg4ObKxHtVrVX/3VX2kwGOjJkydGFMtkMiqXywFWIRgKWARYy93dnfr9vjXhzWYz62/w3BhwBT8qY9URDJFLJpOxcJLqBKVRnALAdjwet1GclFER6oYYB7kOlnKpVLL+EI85EMURVgN0+48wT+ZrG7k6Ud3+/r4qlYoKhYJhBJAIAf4R0NrZ2TFws9Fo6OLiQj/99JNheTgbSrLgDXCJuCA9qY6qJQfaX1irvqyAIiqViolx39zcqFQqqdfr2eVzcXGhZrOp3/72tzaqloFyg8HAWMicI8a+QgPY3Ny0goGvMFFQ8amQJ6p+tmoR7Ea6mE9PTzUej2341tbWlrLZrI6Pj3V1daVKpWL5NJILeEI2N07C4wCEzGwIBKeurq7skPJQHCJPeFr1hojFYuYMuAU8rZxoxnOFCoWC7u7u1Ol0AkzJxWIReJNR7CuXywZugz2REnnaNre9P1CrvqF9ZYMSKY6Sr+EkZ7OZBoOB9cVQ6QFLubi40OnpqX3GAXPTe9GosKP1NAbwjKhgLlJQgwZeSqlUMnmOra0tm7dEmRgHSitAPp/XZDJRv99Xp9NRo9GwZ2ZtcMhecAvHSpSYSCQsBfMp04fYO9MiHmJra8tyZjZ3IpGwcAzvyqFA7lJ65S0B7tDV8LV1QsFYLGYD1vr9vk1/m06nevHihXq9nmq1mnq9nh3oVWMw5O9EXZICz4niXrVaNZV/IsDhcKhHjx7Zxgd/IE3wvUPIPnr8gOf2nBjSUFrniT5XSaCjHWFzc1O5XM6+ns/njThJdMvrJ32Mx+N6+fKlpZBEfPF4XNVq1YS0Hjx4YDgeEaSPbkm1wR1IueEjrVq+gwqQT+EopiwWCyuh397eGtWfM0ULCJeXF3zylUoKJ+VyWfl8XplMxvbkzs6OisWiRYD3pUmfNS2SFAi3vbf34KrHRhKJhHVRplKpAJUZFJ+DAPjre4/4N4Smx+OxRqORAVKU6Ghe8+0DqzBfAvUbg/RnNBrZrbqzs2OOd3t728J/IgzGvHJrh8ukiGz5KJCD47EqypmII5EarMqQgoC5zX6AZ0JqSLTlQVjPImVIF0TCfD5v2jd7e3va29uzaglr5C8dsBqvUxJ2KqtywFT4cLTSclqnb40olUq2r6bTqbrdbgDPY728TCUO1Cv9kVr7FgEcmNdplpZMfVKj97V38lw8o5LKBXXvYrFoRJ/JZKJGo2EvxtOEuXE9Tdt7xHg8HhjRSs3/9vZW7XZbrVZLZ2dn6nQ6ur29NS/rB3GvyrwWyfb2tpXgYZg2m01tbm5a9McHEw/29vYCVQL/4cN532DGwWGtqCw1m011u13rm6GKB39kVQfHH/T5fK5nz55pPB7rZz/7md2k4Gi8p0QTOBw+s9FJHbmNveypPwAA3lxsp6enevnypUajkaSgntAqzb/fkgLVRC7s3d1dK0VLryJkCKe+pcJjbH5fheUXcCR7e3tGuksmk5ZqAQBLyz5C6fXO8zc+07se2INcUK8nk4nl+/wb9XS8pn+z/IPjVJingxPyB8nfxKjkM4OFB/U1/VUCcSx0WA6QMB/wkDCf1w+hy4tChZsevZP1VTPPCQEARcScmxmR7lWnRKwFr5k12dnZsc8wwbkxvUK9TwWlZWOiF5z25EoPOob7uebzuVVRiJr9jbzKNfK9Q9JySuJ8PjdHurGxYTO+OIOe0Rz+eR5HAdsC3wo3P4LxbG5uGoAORCEt1/JD7K3OxW8IbslOpyNJKpVKGg6HdriTyaROTk7sYXxPkX+Bvj+GjUQ6dHt7a6F8rVazWUjdbtdALXgwMGBXDeb69gaqWZ7qDxfoxYsXFo5DzQ+3xIedpK+0kdr46k+v19NoNNLZ2Zna7bbOz881GAxUr9fNGfty7KoOD9q4ni08mUz0u9/9Tnt7e8YEzeVyr7VQsA6eTYqzkJYd4kTGvssXDG9jY0PPnz83dfyLiwvDKLwTW6Vz4b3lADP5gAoRKXOhUNB4PFahUDAdJd9OQgTIe05qRFruq3PFYtGqcjQG02jc6/XUarU+KfJ9q3PxoRU36O3trZHlCoWCEbukZROVn/ELyi8tsQhAYB/mszDj8djIP91u125f35TGIq2aecpr57MP3aVlFYznkWQaLnSIcyPhZMKdw9ISWAf/wlHws+HFEK1QaeOgrfpW5vXDMqanqNPpaGdnx3rTfFQLoIixNqxVOELGufB1blowKg4LoT5f5/9GxUhruFDgCFGdBa8LO1MucSqsvtmVc0LkQloEGJ7NZpVIJDQcDg0b9XPEPnYPvdO5SEuFM0Lan376SZeXl2o0GvZi6ZuhGsJGoYGMUAz1fzZ/u902liX9JIh4k07AaoWSDKNw1Y7FG9EL0RwHgz4Z+lva7bZpZlBR813Tno4dNt5wOEEvXrwwAWvGr1BVIaQOl/JXYT4CBtilOnJ1daXT01OLUpAAxcngYHHEngbvPzhcHBz2B9XF//mf/9HFxYVFwffxgFZpPu3l88bGK2HxnZ0dtVot5XI5S2HA7LiU/cROX4mVlmm4x1q4pMFSGEA4HA7VaDRsNI2fffWhlIb3QkJ5g72D4dZgE+zs7Fgbu29iDOMKfiYRLMt6vW43b7vd1nA4tBI0vx/PSzoVJfMAGutCCkhYjsgV7OW7uzubMOibx1hTKh6o9sViMWPsUjWr1+umkM9GCDNTo3BwMN8YxyGiWc9jd34yIB+kwD568x9gFJDx2KvsqU6no06nE5DqiNr6SEvn4h0yFwYRH2fL0/Mpq0Pg9D1nflSP75YnRWXtwO7AW4gEP8axSO/pXFj87e3tQH8GXpOQnweBZMeBSafT5qCQWsDLoviO+QFhCH37UtyqAdw3GQsP05ZwFZAVAO78/NywIxTXIDchrg1ISViKXV5eBgZfdTqdgCyDryL5wxMF43VJMn2QbDar29tbwxU2NzcDKQAyAb1eT/l8XtVq1ZyLr7olEgl1u90AtaFWq2kwGOgPf/iDarWa4VG+7M33RtF8p/toNNKLFy/Ubrc1Go0s8vXRiNcVDkMO4aojlyAXEBGLH6gGb+tTSJgfVMMN9/TwS/2Lhp/Cg+ExpVdvKPNQfMemL9OS/uBEPDU76ubXg6iDN5FIzEd7sB957mQyafO2E4mEpUE4jcvLS7XbbQtb/WwiX6oO9xVFxTyPhdeMRIKvIPquZpwnOBV7ze9BPsP1kF6VcukqZ3a2TxOl6DoWSQHczmsnIQqG82X/ENF7vIp948XHfCrJenLhcxaJWj41qnvvyMVzAcAIiGRozZaWA7/QpAiXlzEWp1QqqVwum6CNj4ru85pRi1gw/0b40J11AwOhe1WS0ul0YKYvSD4OldI/NxFsWw5JuIkTpxbFcN9XMHgO0hgwGPqzhsOhEomEcTgI4313Lrdvt9tVv9+3FBF5ivPzc3Mu/hb2rPAorQ9GdYf30A8lIwIGg6GPj7Pk8RSyB5wUjho4AtB2Pp9bVajb7Zoz92n1x15UHxS5+AqGf2M8k48FYiOEXxipjSfA+Zq775ORlkJDUnQdi7c3lTZ9VOPTFdbIy1t41u90OrU18pGMpNeYlFF1LJjHoog4SGW4eGhz8NovpEtEftIyCvIOi07y6XRqrQ/esfhUKIrrEzZ/sH3URVQBt8o3HV5fXwc0cLyDIaugvIyTGQ6Htrf8Wn2KY5E+wLl4cI2wlI5lXyKMx+NWIaJx0aP+Pgf0coY8EFGOf8Bvwalg4U0bBp+JZjgshKbwEzwvgTWg9C4F5wbzZxzwt3BoCPPB6+jy5YLxwk7cwr7aSBRHJMxNC+GLPwNus5ekb2N9pGDlCE0bPlMU6Pf7FrEQ8foIBoIdP49qJqRLQFy/D4mSPlda/cG8+fDN7B/Ak5zoWeDBPdmJ8BTA06c/3ol9Cxvhfc2Hp5gPfT3AxlqxBmEdm/DPkb6dgyPdzw3iwsIR+PQFkNKvAc4FBrQH0DksvmryLa0Pdh8cwGXL3vFrxucwS5l9hTPx1SB/uX/uDvrYt7bga1vb2r4N+3byjbWtbW3flK2dy9rWtrYvYmvnsra1re2L2Nq5rG1ta/sitnYua1vb2r6IrZ3L2ta2ti9i/w/JVE3XIir//gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x576 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 8))\n",
    "layer_visualisation = per_layer_results[1][0, :, :, :]\n",
    "layer_visualisation = layer_visualisation.data\n",
    "print(layer_visualisation.size())\n",
    "for i, flt in enumerate(layer_visualisation):\n",
    "    plt.subplot(8, 4, i + 1)\n",
    "    plt.imshow(flt, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
